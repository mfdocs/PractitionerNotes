<html><p>Server Automation has many tuning parameters this document is a best practice minimum recommended baseline as a delta from what is shipped.&nbsp;&nbsp;</p>

<p>Tuning parameters can be examined and set using the&nbsp;<strong>sa_config_checkfix</strong>&nbsp;tool.&nbsp; &nbsp;This tool is supplied as part of the OPSWtools RPM bundle installed with each patch rollup distribution.&nbsp; If your core is unpatched this tooling will not be available.&nbsp; These configuration parameters can also be adjusted using the java desktop GUI interface via the System Configuration administration panel.</p>

<p><strong>Basic Operation</strong></p>

<p>Examine a parameter value&nbsp;</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>/opt/opsware/support/bin/sa_config_checkfix -k way.agent_reach.check_reachability.concurrency</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Set a value&nbsp;&nbsp;&nbsp;</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>/opt/opsware/support/bin/sa_config_checkfix -k way.agent_reach.check_reachability.concurrency -v 50 --set</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The tuning falls into two categories. Functional based and Component-based. As tuning should be applied holistically cherry-picking items from this guide is&nbsp;<strong>discouraged</strong>. If additional tuning is&nbsp;required an understanding of the components affects and its impact should be made before putting it in place.&nbsp;&nbsp;</p>

<p>The tuning of timeout values with some exceptions that are not performance-related have been omitted.</p>

<h2>Disable importing Windows Superseded patches</h2>

<p><br>
To prevent superseded patches from being imported.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>/opt/opsware/support/bin/sa_config_checkfix -k patchman.ms_mbsa20_skip_import_superseded -v 1 --set&nbsp;&nbsp;&nbsp;&nbsp; (default 0)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Purging existing unused patches will shrink the amount of data that needs to be compared leading to&nbsp;improved&nbsp;performance:&nbsp;&nbsp;</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code># /opt/opsware/bin/python /opt/opsware/mm_wordbot/util/win_purge_unused_superseded_patches.pyc</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The utility above replaces that documented in our user manuals.</p>

<ul>
	<li>QCCR1D244356 - win_remove_patches deleteSP option takes too long&nbsp;</li>
</ul>

<p>This utility&nbsp;was shipped in various rollups <a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00246">10.60.006,</a> 10.50.008, <a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00248">10.51.006</a>, <a href="https://softwaresupport.softwaregrp.com/doc/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00274">10.23.015</a> for releases &lt; 2018.08</p>

<h2>Agent upgrade concurrency</h2>

<p>This is tune-able from &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00247">10.23.012&nbsp;&nbsp;</a></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.agent_upgrade.concurrency -v 50 --set&nbsp;&nbsp;&nbsp;&nbsp; (default 30)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>This tuning parameter was added as part of this performance change.&nbsp;</p>

<ul>
	<li>QCCR1D224403 - Agent upgrade does not use Tsunami&nbsp;&nbsp;</li>
</ul>

<h2>Communications test</h2>

<p>&nbsp;This is tune-able from &gt; 10.22.011, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00247">10.23.012</a>, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00271">10.51.011</a>, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00249">10.60.007&nbsp;</a>&nbsp;</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.agent_reach.check_reachability.concurrency -v 50 --set&nbsp;&nbsp;&nbsp;&nbsp; (default 20)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The tuning parameter was added as part of this enhancement.&nbsp;</p>

<ul>
	<li>QCCR1D245662 - Restrict automated communications test from executing in certain realms&nbsp;&nbsp;</li>
</ul>

<p>When a server can't be contacted we will retry every 10s until the default timeout of 120s is reached.&nbsp; Reduction of this tunable will increase reachability testing throughput when unreachable servers are encountered. 30s&nbsp;will give you a 4 fold increase in throughput from 120s. As concurrency slots are freed up quicker.</p>

<p>Tunable from &gt;<a href="https://softwaresupport.softwaregrp.com/doc/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00285">10.23.015</a>, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00271">10.51.011</a>, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00264">10.60.013</a>, &gt;<a href="https://softwaresupport.softwaregrp.com/doc/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00277">2018.03.003</a></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.agent_reach.check_reachability.launch_timeout -v 30 --set&nbsp;&nbsp;&nbsp; (default 120)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The tuning parameter was added as part of this enhancement</p>

<ul>
	<li>QCCR1D252426 - Communication Test execution makes too many Spin calls</li>
</ul>

<p></p>

<h2>Compliance Scan</h2>

<p>This is tune-able from &gt;10.50.008, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00248">10.51.006</a>, &gt;<a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00246">10.60.006&nbsp;&nbsp;</a></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.compliance.concurrency -v 50 --set&nbsp;&nbsp;&nbsp;&nbsp; (default 20)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The tuning parameter was added as part of this enhancement.</p>

<ul>
	<li>QCCR1D241724 - Configuration parameter required for patch_compliance to increase concurrency&nbsp;&nbsp;</li>
</ul>

<p>This tuning parameter is used for tables related to Audit/Compliance.&nbsp; It reduces the number of round trips to the database.</p>

<p><strong>/etc/opt/opsware/twist/twist_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>twist.maxBatchCount=100</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 40)</code></p>

			<p><code>twist.statement.fetchSize=100</code>&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 30)</code></p>

			<p><code>&nbsp;</code></p>

			<p><code>Not audit related but while we are tuning the batch fetch capability:</code></p>

			<p><code>&nbsp;</code></p>

			<p><code>twist.recalc.maxBatchCount=100</code>&nbsp;&nbsp;&nbsp;<code>(default 40)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<h2>Server Script execution</h2>

<p>The timeout is reduced to increase throughput for agents that are unreachable. Reduce the time we wait for a response.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.max_serverscript_waiter -v 50 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 20)</code></p>

			<p><code>sa_config_checkfix -k way.script_launch_timeout -v 60 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 120)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<h2>Audits / Server Modules</h2>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.max_ssct_waiter -v 50 --set&nbsp;&nbsp;&nbsp;&nbsp; (default 20)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The spoke pools were increased from their defaults of 10/11 to 50/51 in the following releases; 10.10.006, 10.20.006, 10.21.004. However, for high audit load, this increase may still not be sufficient.&nbsp;&nbsp;</p>

<ul>
	<li>
	<p>QCCR1D212189 - Increase default size for spoke's Server Module ThreadPools, expose pool size parameters in spoke.conf&nbsp;&nbsp;</p>
	</li>
</ul>

<p><strong>/etc/opt/opsware/spoke/spoke_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>spoke.poolsize=100</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 50)</code></p>

			<p><code>sm.num.threads=101</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 51)</code></p>

			<p><code>sm.num.err.threads=100</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 50)</code></p>

			<p><code>spoke.auditObjectCacheSize=100000</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 15000)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>100k objects in testing used about 200Mb of memory, no additional JVM memory increase should be necessary. If Out of memory errors are noticed increase the spoke JVM&nbsp;&nbsp;</p>

<p><strong>/etc/opt/opsware/spoke/spoke_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>spoke.mxMem=2560m&nbsp;&nbsp;&nbsp;&nbsp; (default: 1280m)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<h3>Timeout Failures</h3>

<p>An audit compliance scan will run on a server for a maximum of&nbsp;<strong>way.ssct_snapshotcompliance_timeout</strong>&nbsp;1800s (30min) before it reaches its MAXIMUM limit where the Way stops waiting for the results. The SPOKE will continue waiting a little longer&nbsp;<strong>spoke.handler.cmdoutput.timeout_atcore</strong>&nbsp;&nbsp;</p>

<p>If audits take longer than 30 min to complete this tuning parameter will need to be adjusted. This should be tuned in concert with the _atcore parameter. The rule&nbsp;&nbsp;</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>way.ssct_snapshotcompliance_timeout &lt;= spoke.handler.cmdoutput.timeout_atcore</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Increase from the 30min default to 90m to avoid a timeout&nbsp;&nbsp;</p>

<p><strong>/etc/opt/opsware/spoke/spoke_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>spoke.handler.cmdoutput.timeout_atcore=5400</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 1800)</code></p>

			<p><code>spoke.handler.cmdoutput.timeout=5400</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 1800)</code></p>

			<p><code>sm.method.overall.timeout=3600</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 1800)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>SSCT WayScript has hardcoded limit of 3600 no point pushing the&nbsp;<strong>sm.method.overall.timeout</strong>&nbsp;higher.</p>

<ul>
	<li>QCIM1D133182 -&nbsp;Discovered Software snapshot creation failure</li>
</ul>

<h2>Twist</h2>

<p>The amount of memory for production loads varies greatly, know this, it will have to be increased above the default setting.</p>

<p><strong>/etc/opt/opsware/twist/twist_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>twist.mxMem=8192m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 2560)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Exclude login event sync for automated service accounts.&nbsp; This parameter should be adjusted in line with the user(s) used for OO integration and CSA automation. It minimizes mesh conflicts in exchange for a loss of an update to the last user login timestamp for these accounts.</p>

<p><strong>/etc/opt/opsware/twist/twist_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>twist.ignoreRecentLoginEventUsers=detuser,ooadmin,csaadmin</code></p>
			</td>
		</tr>
	</tbody>
</table>

<ul>
	<li>QCCR1D238526 - Ignore OO users aaa_user.most_recent_login updates to avoid conflicts from automation's&nbsp;&nbsp;</li>
</ul>

<h3>Database connections</h3>

<p>Increase the database connection pool.&nbsp; &nbsp;The Twist is JAVA EJB application and as such need to run on an EJB container platform.&nbsp; The version being used by SA changed from Weblogic to Wildfly.&nbsp; This means the instruction for tuning SA are different depending on the version you are using.</p>

<h4>Wildfly</h4>

<p>For all modern supported versions of SA, that is &gt;= version 10.50</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>&lt;min-pool-size&gt;20&lt;/<!--</code--><code>min-pool-size&gt;</code></code></p>

			<p><code><code>&lt;min-pool-size&gt;100&lt;/<!--</code--><code>min-pool-size&gt;</code></code></code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Recommended tuning:&nbsp;</p>

<p><strong>/opt/opsware/twist/wildfly/standalone/configuration/standalone-full.xml</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>&lt;min-pool-size&gt;100&lt;/<!--</code--><code>min-pool-size&gt;</code></code></p>

			<p><code><code>&lt;max-pool-size&gt;200&lt;/<!--</code--><code>max-pool-size&gt;</code></code></code></p>
			</td>
		</tr>
	</tbody>
</table>

<h4>Weblogic</h4>

<p>The versions of SA that still use Weblogic are out of support.&nbsp; Tuning appears for completeness sake.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>&lt;initial-capacity&gt;20&lt;/<!--</code--><code>initial-capacity&gt;</code></code></p>

			<p><code><code>&lt;max-capacity&gt;100</code>&lt;/<code><!--</code--><code>max-capacity&gt;</code></code></code></p>

			<p><code><code><code>&lt;capacity-increment&gt;2</code></code>&lt;/<code><code><!--</code--><code>capacity-increment&gt;</code></code></code></code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Recommended tuning:&nbsp;</p>

<p><strong>/var/opt/opsware/twist/config/jdbc/CP-TruthPool-jdbc.xml</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>&lt;initial-capacity&gt;100&lt;/<!--</code--><code>initial-capacity&gt;</code></code></p>

			<p><code><code>&lt;max-capacity&gt;200</code>&lt;/<code><!--</code--><code>max-capacity&gt;</code></code></code></p>

			<p><code><code><code>&lt;capacity-increment&gt;10</code></code>&lt;/<code><code><!--</code--><code>capacity-increment&gt;</code></code></code></code></p>
			</td>
		</tr>
	</tbody>
</table>

<h3>Entropy</h3>

<p>Low entropy on the server can lead to SSL connections into the Twist stalling due to Java using /dev/random, which is a blocking device.&nbsp; This problem is most noticeable when integrating&nbsp;OO with SA as it can make many thousands of connection quickly&nbsp;each requiring an SSL handshake.</p>

<p>The corrective action requires the startup file <strong>/opt/opsware/twist/twist.sh</strong>&nbsp;edited.<br>
Locate the following line in the<strong> twist.sh</strong> file.</p>

<p><tt>JAVA_OPTS="$JAVA_OPTS -Dtwist.apps.path=$TWIST_APPS_HOME -Dorg.jboss.server.bootstrap.maxThreads=8<br>
-Dquartz.enabled=true ${JRMICOOPT} -Dorg.quartz.properties=$TWIST_CONFIG_HOME/twist.conf<br>
-Dsun.lang.ClassLoader.allowArraySyntax=true -Dorg.apache.commons.logging.Log=org.apache.commons.logging.impl.Jdk14Logger -Dpython.options.internalTablesImpl=weak -Duser.timezone=UTC -Dtwist.port=$ADMIN_PORT -Dtwist.ssl_port=$SSL_PORT -Dtwist.hostname=`uname -n` -Dtwist.mid=$TWIST_MID -Dtwist.install.home=$TWIST_INSTALL_HOME -Dtwist.runtime.home=$TWIST_RUNTIME_HOME -Dtwist.config.home=$TWIST_CONFIG_HOME -Dtwist.log.home=$TWIST_LOG_HOME -Dtwist.crypto.home=$TWIST_CRYPTO_HOME -Dspoke.stream.dir=$TWIST_RUNTIME_HOME/spoke_stream_cache -Djava.protocol.handler.pkgs=com.opsware.rmi -Djava.rmi.server.RMIClassLoaderSpi=com.opsware.rmi.SpokeRMIClassLoaderSpi -Djava.util.logging.config.file=$TWIST_CONFIG_HOME/twist.conf"</tt></p>

<p>After the above line append the following&nbsp;(and yes that "." in between is correct and required).</p>

<pre># Stop using /dev/random as it can block SSL connections.
JAVA_OPTS="${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom"</pre>

<h2>Hub</h2>

<p>APX / OGFS Script concurrency</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.max_ogfsscript_waiter -v 100 --set&nbsp;&nbsp;&nbsp;&nbsp; (default 50)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>This tuning parameter has dependencies elsewhere:&nbsp;</p>

<p><strong>/etc/opt/opsware/hub/hub_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>hub.task.concurrency=300</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 150)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>This is tune-able in 10.50.011, <a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00283">10.51.009</a>, <a href="https://softwaresupport.softwaregrp.com/doc/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00261">10.60.012</a>, <a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00265">2018.08.002</a>&nbsp;&nbsp;</p>

<p><strong>/etc/opt/opsware/hub/hub_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>hub.cache.server_name=200</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 10)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<ul>
	<li>QCCR1D205112 - rosh "-n servername" performance is lower than "-i dvcid" with non-super administrator user&nbsp;&nbsp;</li>
</ul>

<h2>Twist/Way command execution</h2>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k twist.way.cmdpoolsize -v 200 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 50)</code></p>

			<p><code>sa_config_checkfix -k way.max_asynctwist_waiter -v 50 --set&nbsp;&nbsp; (default 25)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Increasing the Twist command pool increases the number of threads that it can execute this also requires an OS change.&nbsp;&nbsp;</p>

<p><strong>/etc/security/limits.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>twist soft nproc 4096</code></p>

			<p><code>twist hard nproc 4096</code></p>
			</td>
		</tr>
	</tbody>
</table>

<h2>Way</h2>

<h3>Excessive Twist cache resyncing</h3>

<p>This tuning param was introduced to work around an edge case with the ADM product.</p>

<ul>
	<li>QCCR1D135496: Job status via ADM slow; Fixed in SA 9.10</li>
</ul>

<p><strong>/etc/opt/opsware/waybot/waybot_custom.args</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>twist.session.cache.update: off</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>The consequence of this defect fix is that it ends up causing Twist to excessively flush its cache inducing a performance issue and sometimes crashing out Way Jobs.&nbsp; Turning it off will be of benefit if the ADM sub-system is not being used.&nbsp; If you don't know what ADM is, you're not using it.</p>

<h3>State transition blocking</h3>

<p>The Way has a 10s timeout when attempting a poke request with a 10s retry.&nbsp; For estates with a large number of unreachable servers, this can set up a mini denial of service inside the Way moving jobs forward.&nbsp; A small reduction (1s) in the poke timeout will help alleviate this issue.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code><font face="monospace">sa_config_checkfix -k poke.conn_timeout -v 9 --set&nbsp; &nbsp; (default 10)</font></code></p>
			</td>
		</tr>
	</tbody>
</table>

<h3>Thread pool exhaustion</h3>

<p>The Way thread pool when under extreme load can suffer from not having enough threads to drive the work forward this helps to avoid timeouts due to the congestion in the Way itself.</p>

<table>
	<tbody>
		<tr>
			<td><code><font face="monospace">sa_config_checkfix -k thread_pool_size -v 24 --set&nbsp; &nbsp;(default 16)</font></code></td>
		</tr>
	</tbody>
</table>

<h3>Way refreshing its cache excessively</h3>

<p>If SA is not being used to provision ESX servers a workaround that causes&nbsp;the Way to force a cache refresh with a Spin call on every command sent to an agent can be disabled.</p>

<ul>
	<li>OCTCR19L838880 - Allow cache refreshing for pokes to be disabled</li>
</ul>

<p>This is available from <a href="https://softwaresupport.softwaregrp.com/doc/KM03654109">10.60.018</a>, <a href="https://softwaresupport.softwaregrp.com/doc/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00312">10.51.014</a>, <a href="https://softwaresupport.softwaregrp.com/doc/KM03654074">2018.08.008</a> a restart of the Way is required.<br>
<br>
<strong>/etc/opt/opsware/waybot/waybot_custom.args</strong></p>

<table>
	<tbody>
		<tr>
			<td><code><font face="monospace">way.si_refresh_on_poke: 0</font></code></td>
		</tr>
	</tbody>
</table>

<p></p>

<h3>Split-Brain</h3>

<p>Due to the new way session job execution enhancement, introduced in 10.23 and 10.51, designed to prevent jobs&nbsp;from completing until all commands started by that job have completed, many&nbsp;customers have experienced jobs that stay ACTIVE (aka in-progress) for a long period of time.</p>

<p>A tuning parameter was introduced in <a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00242">10.60.004</a>, <a href="https://softwaresupport.softwaregrp.com/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00254">10.51.008</a> and <a href="https://softwaresupport.softwaregrp.com/doc/group/softwaresupport/search-result/-/facetsearch/document/LID/SRVA_00285">10.23.008</a> to disable this change in functionality.</p>

<ul>
	<li>QCCR1D240570 -&nbsp;Add option to deactivate the split-brain functionality and allow jobs to time out</li>
	<li><a href="https://softwaresupport.softwaregrp.com/doc/KM02895479">KM2895479</a></li>
	<li><a href="https://docs.microfocus.com/itom/Server_Automation:2018.08/PN/pn5e986537dd24c4.98859328">Split Brain Job Timeout</a>&nbsp;best practice guide.</li>
</ul>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code># sa_config_checkfix -k way.command_shutdown_grace_period -v 0&nbsp;<value>--set</value></code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>There are 2 possible timeout values that can affect a job. These are given by the following parameters:</p>

<ul>
	<li><code>way.twist_launch_timeout_default</code>: the launch (job start) timeout. This is used as-is for the command startup (i.e. if it says 3600 seconds, then the actual timeout is 3600 seconds)</li>
	<li><code>way.command_shutdown_grace_period</code>: this affects the max timeout. The max duration of the job is calculated as follows:<br>
	<timeout job="" on="" set=""> &lt;timeout set on job&gt; + &lt;way.command_shutdown_grace_period&gt; + &lt;150 seconds&gt;<way.command_shutdown_grace_period><br>
	&nbsp;</way.command_shutdown_grace_period></timeout></li>
</ul>

<p>The above is true as long as the parameter value is not -1; if it is -1 then, according to the parameter description, we will wait forever until we get the results.</p>

<h2>Patching / Software Remediation</h2>

<p>Tuning parameters relating to remediation</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.remediate.chunk_size -v 25 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 20)</code></p>

			<p><code>sa_config_checkfix -k way.remediate.max_concurrency -v 40 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 10)</code></p>

			<p><code>sa_config_checkfix -k way.max_remediations -v 200 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 50)</code></p>

			<p><code>sa_config_checkfix -k way.max_remediations.action -v 400 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 100)</code></p>

			<p><code>sa_config_checkfix -k way.remediate.max_app_compliance_calls -v 50 --set&nbsp; (default 25)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>We should have a smaller&nbsp;<code>chunk_size</code>&nbsp;and a higher&nbsp;<code>max_concurrency</code>. More&nbsp;doers&nbsp;will reduce the wait time before remediation commences as fewer servers in a&nbsp;doer&nbsp;need to transition through each phase. The&nbsp;chunker&nbsp;will increase the&nbsp;<code>chunk_size</code>&nbsp;itself once the&nbsp;<code>max_concurrency</code>&nbsp;limit has been reached so this is less important to tune.&nbsp;&nbsp;</p>

<p>Increasing&nbsp;<code>chunk_size</code>&nbsp;too large will have a throughput performance penalty as all the servers in the&nbsp;chunk&nbsp;must complete each phase&nbsp;<em>(analyze/stage/action)</em>&nbsp;in the&nbsp;doer&nbsp;before they can all progress.&nbsp;</p>

<p>The tuning parameter&nbsp;<code>way.max_remediations</code>&nbsp;should really be called&nbsp;<code>way.max_remediates.stage_analyse</code>&nbsp;because it pertains to how much concurrency is allowed in those two phases.&nbsp; &nbsp;The three phases being; stage, analyse and action.&nbsp; &nbsp;The action phase has its own tuning parameter and in the annals of SA one parameter used to be used for all three.</p>

<p>This is how long we wait for the agent to respond to our pokes and callback to get the Command we just queued up for it.&nbsp; The default value is excessive.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.remediate.command_launch_timeout -v 60 --set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (default 240)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>This is used when doing remediation's and it appears when asking a server to execute a script, get its installed units (for patching computations), performing a reconcile post patching, checking available disk space before staging, and other activities during the remediation phases.</p>

<p>Windows patches can take longer to installer than allowed by SA.&nbsp; &nbsp;Old server hardware and Microsoft's&nbsp;every increasing patch payloads are putting a strain on customer infrastructure and SA just needs to wait longer.&nbsp; It's recommended that you increase this timeout:</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.remediate.package_alarm_timeout -v 5400 --set&nbsp; (default 3600)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Increasing the alarm timeout will give servers more time to complete.&nbsp; The downside is that if there&nbsp;is even ONE system that's taking a long time for some activity, the entire job will not complete until that server breaches the alarm timeout.&nbsp; Meaning your SA jobs could potentially take a lot longer to complete.</p>

<p>When SA asks&nbsp;a Managed Server what patches it has Windows, specifically the WUA subsystem, can take longer than&nbsp;30min to answer this question resulting in a&nbsp;timeout.&nbsp; This is a&nbsp;Windows resource problem&nbsp;most often noticed with virtualized servers, or small (4Gb) memory footprint servers, with over-commitment resources&nbsp;the hypervisor struggles when 100's of servers all perform this action during a patching cycle.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k way.remediate.get_dicts_timeout -v 2400 --set&nbsp; (default 1800)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Increasing the alarm timeout to 40min will give servers more time to complete.&nbsp; Balancing out the scheduling&nbsp;is advised instead of just increasing this number to compensate for infrastructure issues.</p>

<h2>Vault</h2>

<p>These values only need a minor adjustment increasing them excessively will yield little benefit and can be detrimental.&nbsp;&nbsp;Slice 0 change only.</p>

<p><strong>/etc/opt/opsware/vault/vault_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>com.opsware.omb.dbpool.poolSize=15</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 5)</code></p>

			<p><code>com.opsware.omb.dbpool.cacheSize=20</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 10)</code></p>

			<p><code>com.opsware.omb.payloadcache.size=40</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 20)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Additional memory may need to be assigned to the vault to handle large transactions. Unless out of memory error are being experienced don't increase the JVM. There is no tuning parameter to accomplish this task requiring a direct edit of a script. On upgrade, this may be reverted which is why its recommended not to adjust it unless OOM's are occurring.&nbsp;&nbsp;</p>

<p>For SA &lt;= 10.60 an increase in the vault memory requires the editing of the vault startup script</p>

<p><strong>/opt/opsware/vault/bin/vault</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>JAVAARGS="-ms120m -mx1024m ${OPTION_COMPRESSEDOOPS} -Dcom.opsware.vault.config.file=${CONF_FILE} -Djava.util.logging.config.file=${LOG_PROPS}"</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Increase the mx value from 1024</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>-mx2048m</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>In SA 2018.08 and above a tuning option is available that will be preserved across upgrade boundaries.</p>

<p><strong>/etc/opt/opsware/vault/vault_custom.conf</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>vault.mxMem=2048m</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Spin</p>

<p><strong>/etc/opt/opsware/spin/spin_custom.args</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>spin.db.initial_concurrency: 10</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 5)</code></p>

			<p><code>spin.db.max_concurrency: 160</code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>(default 80)</code></p>
			</td>
		</tr>
	</tbody>
</table>

<p>Minimizes risk of 'spin.databaseConnection - No available connections' error during agent install surges.</p>

<p>AIX apars software registration can put a load of load on the spin.&nbsp; You may consider disabling this feature if its not required</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k spin.cogbot.swreg.ignore_apars -v 1 --set</code></p>
			</td>
		</tr>
	</tbody>
</table>

<h2>opswsshd</h2>

<p>Allows more simultaneous Global Shell session startups.&nbsp; Especially for SA/OO integrations.</p>

<p><strong>/etc/opt/opsware/sshd/sshd_config</strong></p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>MaxStartups 256</code></p>
			</td>
		</tr>
	</tbody>
</table>

<ul>
	<li>QCCR1D168985 MaxStartups param to be added to our sshd config file with a higher default value</li>
</ul>

<h2>NGUI</h2>

<p>Increase NGUI heap from the default of 512Mb.&nbsp; The OCC will take up to 2 min before it loads this change.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code>sa_config_checkfix -k ngui.max.heap_size -v 1024 --set</code></p>
			</td>
		</tr>
	</tbody>
</table>

<h2>Gateway</h2>

<p>The ident service&nbsp;is single-threaded on port 4040.&nbsp;By default the "net.core.somaxconn"&nbsp;is set to 128, which&nbsp;limits the listen to queue to only 128.&nbsp;The opswgw asks for&nbsp;a listen to queue size of 512.&nbsp;When this queue is full,&nbsp;it will cause&nbsp;SYN and&nbsp;SYN/ACK packets to&nbsp;be ignored on&nbsp;the server-side.</p>

<table border="0" cellpadding="0" cellspacing="0">
	<tbody>
		<tr>
			<td>
			<p><code><font face="monospace">echo 'net.core.somaxconn = 512' &gt;&gt; /etc/sysctl.conf<br>
			sysctl&nbsp;-p</font></code></p>
			</td>
		</tr>
	</tbody>
</table>

<ul>
	<li>OCTCR19L728183 Scalability issue around on-demand batchbot configuration query, spin's ident based authentication, and lack of egress gw load balancing.</li>
</ul>

<h2>Entropy</h2>

<p>Starvation of the random number pool&nbsp;available in /dev/random will cause components that require randomness for SSL key generation to stall, as SA is a highly secure system making use of cryptographic communication it's important a steady supply of randomness is available as&nbsp;/dev/random will block when it runs dry causing those using it to wait.</p>

<p>These notes are for RHEL7. Install EPEL and the <u>haveged </u>package.</p>

<pre>yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum -y install haveged</pre>

<p>Adjust the entropy from 1024 to 3072; <tt>/usr/lib/systemd/system/haveged.service</tt><br>
3072 is an optimal value going above this will have a detrimental effect on the system.</p>

<pre>ExecStart=/usr/sbin/haveged -w 3072 -v 1 --Foreground</pre>

<p>Start it up</p>

<pre>systemctl enable haveged
systemctl start haveged</pre>

<p></p>
</html>