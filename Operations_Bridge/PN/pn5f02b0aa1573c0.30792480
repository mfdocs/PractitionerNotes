<html><h2>Introduction</h2>

<div><span style="color:#000000;"><span style="font-size:11pt"><span style="font-family:Calibri,sans-serif">Customers who are willing to upgrade to the latest version of OpsBridge Suite 2019.11, has to go through multiple (quarterly) upgrades, since there is no direct upgrade supported so far. To avoid the huge effort, time and complexity the aberrant suggestion was to go ahead with a plan to migrate their current environment WITHOUT undergoing multiple HOPS.</span></span><br>
<br>
<span style="font-size:11pt"><span style="line-height:107%"><span style="font-family:Calibri,sans-serif">Customers who are running their PRODUCTION environment in OpsBridge Suite with embedded Postgres and lots of integrations with Operations Agent (incl. Management packs / OpsC) and SiS servers, etc., also need to be preserved for upgrade use case.</span></span></span><br>
<span style="font-size:11pt"><span style="line-height:107%"><span style="font-family:Calibri,sans-serif">This document tries to capture every detailed aspect which needs to be followed to achieve a successful migration, without (or minimal) Configuration DATA loss (from the Postgres side of the house). VERTICA DB (used for COSO) is NOT considered, as there is NO migration plan.</span></span></span></span><br>
&nbsp;</div>

<div class="Admonition_Note"><span style="color:#000000;"><span style="font-size:11pt"><span style="line-height:107%"><span style="font-family:Calibri,sans-serif">Scope of migration: Any OpsBridge suite with embedded Postgres (not less than 2018.11 version) to OpsBridge suite 2019.11 with external Postgres.</span></span></span></span></div>

<h2>Migration Pre-requisites</h2>

<h3>Install mlocate package on Master node</h3>

<div>In case if you see that ‘mlocate’&nbsp;&nbsp;package is not installed on the CentOS, Master node. Run the below commands (after consulting your system admin).</div>

<pre><code>yum install mlocate -y</code></pre>

<h3>Install jq package on Master node</h3>
In case if you see that ‘jq’&nbsp; package is not installed on the CentOS, Master node. Run the below commands (after consulting your system admin).

<pre><code>yum install epel-release -y
yum install jq -y</code></pre>

<h3>Install External Postgres</h3>
Install supported Postgres server version on an external database server. Refer OpsBridge 2019.11 installation document for the supported Postgres version.&nbsp;<br>
External Postgres server must be accessible to Master node.&nbsp;Ensure Postgres server has enough disk space required for database restore. Refer embedded Postgres size as illustrated in following section as a reference to estimate disk size required.&nbsp;
<h3><span style="color: rgb(0, 0, 0); font-family: MetricHPE_Medium; font-size: 20px;">Configure NFS volumes</span></h3>
Configure two new NFS volumes:

<ol>
	<li>&nbsp;The first volume will be used to store&nbsp;backup data for this migration workflow.</li>
	<li>The second volume needs to be configured as per OpsBridge suite 2019.11 install document. This volume will be used for installation of OpsBridge 2019.11.</li>
</ol>

<div class="Admonition_Note">Note:&nbsp; Any additional pre-requisites as per OpsBridge 2019.11 install document can be pre-configured to reduce migration window.&nbsp;</div>

<h2>Back up Cluster</h2>

<h3 style="margin-top: 3px;">Note down all db connections in BVD (Skip if BVD capability not installed)</h3>
Login to BVD UI and navigate to Settings -&gt; Data Collectors. Note down the db connections created in BVD for data collectors on 2018.11. The db connection need to be re-created manually on BVD post migration to 2019.11.<br>
<img alt="" border="0" file="" height="352" hspace="0" src="https://docs.microfocus.com/mediawiki/images/2/2b/bvd1-pre-req.jpg" style="width:672px;height:352px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:0px solid black;" vspace="0" width="672">
<h3><br>
Note down all users, user groups and user roles attributes in BVD (Skip if BVD capability not installed)</h3>
Login to BVD UI and navigate to Settings -&gt; Users and Roles. &nbsp;Note down all the users, user groups and user roles attributes which is created in BVD. This has to be restored manually if attributes for these objects are observed to be missing post migration.<br>
<span style="color: rgb(0, 0, 0); font-family: MetricHPE_Medium; font-size: 20px;">Configure Event Auto Archiving</span><br>
Follow the instructions&nbsp;<a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.11/oprArchiveEventsCLI" target="_blank">here</a> for Event Archiving.

<h3>Clear UCMDB history</h3>
Follow the instructions <a href="https://docs.microfocus.com/itom/Operations_Bridge_Manager:2019.11/ServerAdminPurgingTr" target="_blank">here</a> to clear/purge the UCMDB history

<h3>Get the current embedded Postgres Version (optional)</h3>
Execute below command to get embedded Postgres pod name

<pre><code>kubectl get pods -n core |grep postgres | awk 'NR==1 {print $1}'</code></pre>
Execute below command on the Master node to note Postgres version

<pre><code>kubectl exec -it  &lt;postgres-pod-name&gt;  --namespace core  -- bash -c "psql -U postgres -d postgres -c 'SHOW server_version;'"</code></pre>

<h3>Get the DB list from the embedded Postgres</h3>
Execute this command on the Master node.&nbsp; The databases listed would be targeted for migration. The list of databases here may vary based on the capabilities selected during OpsBridge installation. Verify these databases are successfully restored on External postgres after restore operation.

<pre><code>kubectl exec -it &nbsp;&lt;postgres-pod-name&gt; --namespace core -c itom-postgresql-default -- bash -c "psql -U postgres &nbsp;-c 'SELECT datname FROM pg_database WHERE datistemplate = false and datname != \$\$replication_db\$\$ and datname != \$\$postgres\$\$;'"
</code></pre>

<h3>Check suite db ownership on embedded POSTGRES (Optional)</h3>
As a super user, run the below command to check ownership rights to OpsBridge Suite DB list. Choose the database name from the above section to list ownership.&nbsp;

<pre><code>kubectl exec -it &nbsp;&lt;postgres-pod-name&gt; --namespace core &nbsp;-c itom-postgresql-default &nbsp;-- bash -c "psql -U postgres -d postgres &nbsp;-c '\l+'" | grep -E 'Owner|&lt;database-name&gt;'</code></pre>

<h3>Get the Total embedded database size (Optional)</h3>
As a super user, run the below command to get the size of the DB.&nbsp;

<pre><code>kubectl exec -it &nbsp;&lt;postgres-pod-name&gt; &nbsp;--namespace core -c itom-postgresql-default -- bash -c "psql -U postgres -d postgres -c 'SELECT sum(pg_database_size(pg_database.datname)/1024.0) FROM pg_database ;'"</code></pre>

<h3>Execute backup script&nbsp;</h3>
Download and execute backupScript.sh script on Master node to backup following configurations:

<ol>
	<li>Certificates:&nbsp; chain.pem, key.pm,&nbsp;RE_ca&nbsp;</li>
	<li>Configmaps</li>
	<li>Embedded Postgres configuration files: postgressql.conf &amp; pg_hba.conf</li>
	<li>UCMDB Master key&nbsp;</li>
	<li>Embedded Postgres user credentials.&nbsp;</li>
	<li>Embedded Postgres database&nbsp;</li>
</ol>
Script requirements&nbsp;

<ol>
	<li>Must be executed with super user permission on Master node</li>
	<li>Ensure all pods are running before executing the script&nbsp;</li>
</ol>
The script will save backed up data under &lt;NFS-Folder&gt;/opsb_migration/ directory. DO NOT DELETE any data from this folder as it will be used for database restore and post migration step workflows.&nbsp;<br>
Download the script on master node and assign required execution permission before script execution.
<pre><code class="language-bash">chmod +x backupScript.sh
./backupScript.sh</code></pre>
&nbsp;

<div class="Admonition_Note">&nbsp;Note: &nbsp;<br>
1.&nbsp;&nbsp;It is recommended to take backup on NFS mount which has sufficient disk space to hold all the backup files including database backup<br>
2.&nbsp;&nbsp;To maintain embedded Postgres data consistency this script will scale down the deployment under opsbridge-xxx namespace.&nbsp;</div>
Use scp to transfer files from master to remote postgres server.

<pre><code>scp -r &lt;NFS-Folder&gt;/opsb_migration/&lt;time-stamp&gt;/dbbackups/*.dump &nbsp; &nbsp; root@&lt;External-Postgres-hostname&gt;:/var/2018.11.Backup/dbbackups
scp -r &lt;NFS-Folder&gt;/opsb_migration/&lt;time-stamp&gt;/conf/* &nbsp; &nbsp; &nbsp;root@&lt; External-Postgres-hostname&gt;:/var/2018.11.Backup/conf
</code></pre>

<h2>Postgres DB migration</h2>

<h3>Configuration file changes</h3>
Add the below entries at the end of the pg_hba.conf file

<pre><code>host all all 0.0.0.0/0 md5
host all all ::/0 md5
</code></pre>
<br>
Edit postgresql.conf file for&nbsp;listen_addresses , port and lo_compat_privileges parameters to as shown below
<pre><code>listen_addresses = '*'
port = 5432
lo_compat_privileges = on
</code></pre>
<br>
Restart postgres service
<pre><code>systemctl restart postgresql-10.service</code></pre>
Get the current external Postgres Version (optional)<br>
Once logged in as postgres super user into database, run the below command.&nbsp;
<pre><code>SHOW server_version ;</code></pre>
Check External Postgres connection with super user. From the master node, run the below command

<pre><code>psql "dbname=&lt;dbname&gt; password=&lt;password&gt; host=&lt;postgres_node&gt; port=&lt;postgres_port&gt; user=&lt;super_user&gt;"</code></pre>

<h3>Execute database restore script&nbsp;</h3>
Download &amp; Execute restoredbSchema.sh script on External Postgres Server to restore database backup. This script will:

<ol>
	<li>Restore database using .dump files&nbsp;</li>
	<li>Set database user credentials for postgres &amp; cdfidm users to one existed in embedded postgres setup.</li>
</ol>
Script requirements:&nbsp;

<ol>
	<li>Script supports restore operation only for database backup files generated using backupScript.sh in the previous step</li>
	<li>Must be executed with super user permission on External Postgres server</li>
	<li>Ensure .dump and config.properties files are available on External Postgres server and have read permissions.</li>
	<li>Database instance must be a fresh installed instance.&nbsp;</li>
</ol>
Assign required execution permission before script execution as below

<pre><code>chmod +x restoredbSchema.sh
./restoredbSchema.sh
</code></pre>

<div class="Admonition_Note">Note: &nbsp;<br>
1.&nbsp; Script uses Postgres database 'postgres' user (superuser) to restore the backup on Postgres server<br>
2.&nbsp;&nbsp;Note down cdfidm &amp; postgres user credentials post execution of this script (also available in config.properties file). &nbsp;In the later steps you need to use cdfidm user credentials to connect to External Postgres server while installing OpsBridge 2019.11.&nbsp; &nbsp; &nbsp;</div>
Try connecting to the database as cdfidm user. Execute the command from master node and check database table ownership. Use the password from <strong>config.properties</strong> file&nbsp;

<pre><code>psql "dbname=&lt;dbname&gt; password=&lt;password&gt; host=&lt;postgres_node&gt; port=5432 user=cdfidm"
\dt+</code></pre>

<h2>Bring down the cluster&nbsp;</h2>
Make sure all the PODS are up and running.&nbsp;Shut down Suite pods by Running the below command:

<pre><code class="language-bash">${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l DOWN</code></pre>
<br>
Make sure that all the PODS are DOWN and the Suite too. Ignore the Completed pods.<br>
List of pods in Core namespace post execution of cdfctl.sh command
<pre><code>kubectl get pods -n core&nbsp;</code></pre>
&lt;Screenshot&gt;<br>
<br>
List of pods in OpsBridge namespace post execution of cdfctl.sh
<pre><code>kubectl get pods -n &lt;OpsBridge-namespace&gt;</code></pre>
&lt;Screenshot&gt;

<h3>Take a snapshot of Master and Worker nodes (Optional)</h3>
Run the below command on all the Worker Nodes, followed by the Master Nodes

<pre><code class="language-bash">${K8S_HOME}/bin/kube-stop.sh -y</code></pre>
&lt;screenshot&gt;<br>
<br>
Check if there any mount points still active. On the Master node, run the following command. If you see any residual, then kindly run the second command to un-mount the same.
<pre><code class="language-bash">df -k | grep kubernetes
for f in $(df | awk '/kubernetes/ {print $6}'); do umount $f; done
</code></pre>
Shut down the VM’s and take a snapshot of the same Virtual Machines.

<div class="Admonition_Note">Note: It is recommended to take snapshot of Master and Worker nodes in case restore of this setup is needed at later point in time.</div>
Run the below command on the Master nodes, followed by worker nodes

<pre><code class="language-bash">${K8S_HOME}/bin/kube-start.sh</code></pre>
Bring up Suite pods by Running the below command:

<pre><code class="language-bash">${K8S_HOME}/scripts/cdfctl.sh runlevel set -l UP -f</code></pre>

<h3>Check the status of Agent Nodes (Optional)</h3>
Once the cluster is totally down, one would see that the Integrations like Agent and SiS, will be in buffering state.<br>
&lt;Screenshot&gt;
<h2>Uninstall OpsBridge</h2>
Refer to the uninstallation document corresponding to the OpsBridge version you have.&nbsp;<br>
For example, uninstallation document for OpsBridge 2018.11 with embedded Postgres can be found <a href="https://docs.microfocus.com/itom/Operations_Bridge:2018.11/Uninstall_OpsB" target="_blank">here</a>

<h2>Install OpsBridge 2019.11&nbsp;</h2>

<h4>Maintain the old Administrator Password</h4>
&lt;Screenshot&gt;

<h4>Select Capabilities</h4>
Select the capabilities as chosen in previous OpsBridge installation.<br>
&lt;Screenshot&gt;
<h4>Database details</h4>
Maintain the same entries for Database Name, User and Password. Ignore the warning.<br>
&lt;Screenshot&gt;<br>
&lt;Screenshot&gt;
<h4>Deployment Size</h4>
Choose the deployment size in accordance to the current requirement

<h4>Single Master to Multi-Master?</h4>
Single Master to Single Master, keep the same FQDN as that of the previous set up as the External Hostname.<br>
<br>
Single Master to Multi Master (for Master High availability), make sure you configure the previous value of External Hostname as a VIP and update the same in the field External Hostname. Which would mean, one would need 3 FRESH nodes to be added as Master Nodes. And the old Master’s IP and FQDN should be used as HA_VIRTUAL_IP. The Old node should NOT be running.<br>
<br>
This is very crucial for integrations.<br>
The Agents and SiS nodes would be in buffering state till the DOWN time. Once the Cluster and the capabilities are UP, and the connectivity is established, the buffered events should reach to the OMi event Browser.
<h4>External Hostname and Custom Certificates</h4>
In the Connection page (After selecting the Deployment Size), make sure that the External Hostname is properly configured.

<table class="MsoTableGrid" style="border-collapse:collapse; border:none">
	<tbody>
		<tr>
			<td style="border-bottom:2px solid black; width:138px; padding:0in 7px 0in 7px; background-color:#bfbfbf; border-top:2px solid black; border-right:2px solid black; border-left:2px solid black" valign="top"><font face="Calibri, sans-serif"><span style="font-size: 14.6667px;"><b>Previous OpsBridge Installation</b></span></font></td>
			<td style="border-bottom:2px solid black; width:168px; padding:0in 7px 0in 7px; background-color:#bfbfbf; border-top:2px solid black; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif"><b><span style="color:black">OpsBridge 2019.11<br>
			Installation</span></b></span></span></span></td>
			<td style="border-bottom:2px solid black; width:318px; padding:0in 7px 0in 7px; background-color:#bfbfbf; border-top:2px solid black; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif"><b><span style="color:black">External Hostname</span></b></span></span></span></td>
		</tr>
		<tr>
			<td style="border-bottom:2px solid black; width:138px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:2px solid black" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Multi-Master</span></span></span></td>
			<td style="border-bottom:2px solid black; width:168px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Multi-Master</span></span></span></td>
			<td style="border-bottom:2px solid black; width:318px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Keep the same as used in 2018.11, which would the HA_VIRTUAL_IP</span></span></span></td>
		</tr>
		<tr>
			<td style="border-bottom:2px solid black; width:138px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:2px solid black" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Single Master</span></span></span></td>
			<td style="border-bottom:2px solid black; width:168px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Single Master</span></span></span></td>
			<td style="border-bottom:2px solid black; width:318px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Same of the Master FQDN</span></span></span></td>
		</tr>
		<tr>
			<td style="border-bottom:2px solid black; width:138px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:2px solid black" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Single Master</span></span></span></td>
			<td style="border-bottom:2px solid black; width:168px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Multi-Master</span></span></span></td>
			<td style="border-bottom:2px solid black; width:318px; padding:0in 7px 0in 7px; border-top:none; border-right:2px solid black; border-left:none" valign="top"><span style="font-size:11pt"><span style="line-height:normal"><span style="font-family:Calibri,sans-serif">Configure the External Hostname of 2018.11 as the HA_VIRTUAL_IP of the new 2019.11 set up. And this would mandate new Master nodes.</span></span></span></td>
		</tr>
	</tbody>
</table>
<br>
<br>
&nbsp;<br>
<br>
&nbsp;</html>