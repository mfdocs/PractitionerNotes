<html><span id="cke_bm_39272S" style="display: none;">&nbsp;</span>
<h2 style="margin-top: 16px;">Introduction</h2>

<p style="margin-top: 16px;">Customers who are willing to upgrade to the latest version of OpsBridge Suite 2019.11, have to go through multiple (quarterly) upgrades, since there is no direct upgrade supported so far. To avoid the huge effort, time and complexity the aberrant suggestion was to go ahead with a plan to migrate their current environment WITHOUT undergoing multiple HOPS.<br>
<br>
Customers who are running their PRODUCTION/DEV&nbsp;environment in containerized OpsBridge Suite with External Postgres and lots of integrations with Operations Agent (incl. Management packs / OpsC) and SiS servers, etc., also need to be preserved for upgrade use case.<br>
<br>
This document tries to capture every detailed aspect which needs to be followed to achieve a successful migration, without (or minimal) Configuration DATA loss (from the Postgres side of the house).</p>

<div class="Admonition_Important" style="margin-top: 16px;"><strong>VERTICA DB</strong> (used for COSO) is <strong>NOT </strong>considered, as there is NO migration plan.</div>

<h2 style="margin-top: 16px;">Back up configuration</h2>

<p style="margin-top: 16px;">This section focuses on a set of MUST do steps before the start of Migration. This would ensure that the existing integrations and configurations, post migration to 2019.11 would be in a healthy state and functional.</p>

<h3 style="margin-top: 16px;">Get the number of Replica Sets</h3>

<p style="margin-top: 16px;">Store the output of the below command to keep a track of the replicasets being configured in your current working (old environment)</p>

<pre><code>kubectl get replicasets --all-namespaces &gt;&gt; /tmp/&lt;ReplicaSets.txt&gt;</code></pre>

<h3 style="margin-top: 16px;">Store the XPL configurations from current environment</h3>

<p style="margin-top: 16px;">One needs to take a backup of the below files from the NFS location, in order to save the XPL settings being made. The changes need to be manually set on the new environment.<br>
Collection Service (Collect once data broker pod)</p>

<div style="background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;">&lt;NFS-conf-volume&gt;/co/oa/oa1/conf/OV/conf/xpl/config/local_settings.ini</div>

<h4 style="margin-top: 16px;">For OBM in container</h4>

<div style="background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;">&lt;NFS-omi-volume-0&gt;/omi/var/opt/OV/conf/xpl/config/local_settings.ini<br>
&lt;NFS-omi-volume-0&gt;/omi/var/opt/OV/shared/server/conf/xpl/config/local_settings.ini<br>
<br>
&lt;NFS-omi-volume-1&gt;/omi/var/opt/OV/conf/xpl/config/local_settings.ini<br>
&lt;NFS-omi-volume-1&gt;/omi/var/opt/OV/shared/server/conf/xpl/config/local_settings.ini</div>

<div class="Admonition_Note" style="margin-top: 16px;">&nbsp;<strong>Note</strong>: There is no use case of backing up the entire Data directory, as with new version of Suite, customer would be deploying new policies. (May be monitoring policies or say the policies for virtualization collector).</div>

<h3 style="margin-top: 16px;">DNS Configmap for non-DNS environment</h3>

<p style="margin-top: 16px;">In case if the environment was a non-DNS environment, kindly take a backup of the entries in the configuration map named dns-hosts-configmap, under kube-system namespace, to be used in the new 2019.11 environment too.</p>

<pre><code class="language-bash">kubectl edit cm dns-hosts-configmap -n kube-system</code></pre>

<h3 style="margin-top: 16px;">Backup custom node list and ucmdb config.json</h3>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: Skip if you have not configured it as part of Agent Metric Collector in your older version of OpsBridge Suite</div>

<p style="margin-top: 16px;">This is needed only if the current version of OpsBridge Suite installed, has Collection Service capability selected and Agent Metric Collector configured. In case if you have NOT configured or customized the node list, then feel free to skip this section. One needs to back up the below files from NFS location.</p>

<h4 style="margin-top: 16px;">OBM containerized</h4>

<p style="margin-top: 16px;"><strong>File name:</strong> Custom Node list file<br>
<strong>Location:</strong> &lt;NFS-conf-volume&gt;/content-administration/var/content-administration/content/custom/</p>

<h4 style="margin-top: 16px;">OBM&nbsp;External</h4>

<p style="margin-top: 16px;">Login to the itom-opsb-content-administration POD, and back up the below mentioned file. Use the ‘kubectl cp’ command mentioned below to take a back-up of the same file.<br>
<br>
<strong>File name:</strong> sample_ucmdb_config.json<br>
<strong>Location:</strong> /opt/content-administration/conf</p>

<pre><code>kubectl -n &lt;opsbridge_namespace&gt; exec -ti &lt;itom-opsb-content-administration-pod&gt; -c itom-opsb-content-administration bash
kubectl cp &lt;opsbridge_namespace&gt;/&lt;itom-opsb-content-administration-pod&gt;:/opt/content-administration/conf/&lt;sample_ucmdb_config.json&gt; /tmp/sample_ucmdb_config.json -c itom-opsb-content-administration</code></pre>

<h3 style="margin-top: 16px;">Store the YAML Changes</h3>

<p style="margin-top: 16px;">Run the script (will be shared to the customers) to capture the values changed/edited in an html format. Internally it would use the ‘kubectl diff’ command. As these values/changes needs to be done in the YAML files, post installation of 2019.11 version too.</p>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: The kubectl diff option may not be available in older version of K8S.</div>

<h3 style="margin-top: 16px;">Store the MASTER_KEY from ucmdb POD</h3>

<p style="margin-top: 16px;">Login to the ucmdb POD and get the secret by executing the below command</p>

<pre><code class="language-bash">get_secret MASTER_KEY</code></pre>

<div class="Admonition_Important" style="margin-top: 16px;">This VALUE should be saved. And to be used in the new environment.</div>

<p style="margin-top: 16px;">One can also get the secret from the below command</p>

<pre><code class="language-bash">kubectl -n core exec -i $(kubectl -n core get pods -o name -l app=suite-conf-sel-opsbridge | cut -d '/' -f 2) -c suite-config -- get_secret MASTER_KEY</code></pre>

<h3 style="margin-top: 16px;"><a id="Back up Certificate realms and PEM files" name="Back up Certificate realms and PEM files">Back up the Certificate&nbsp;realms and PEM files</a></h3>

<p style="margin-top: 16px;">Back up the RE: Realm External, the Cert loaded on NGINX controllers.</p>

<pre><code>kubectl cp core/&lt;suite-conf-pod-opsbridge&gt;:/var/run/secrets/boostport.com/trustedCAs/RE_ca.crt /tmp/RE_ca.crt -c kubernetes-vault-renew</code></pre>

<p style="margin-top: 16px;">Basically, one needs to retrieve the cert and key from the secret of the OpsBridge namespace (as this is the cert which is used to terminate port 443). Run the below command to retrieve the same. The name of the secret would be nginx-default-secret under OpsBridge Namespace for later version of Suites.</p>

<pre><code>kubectl get secret ingress-controller-tls-secret -n &lt;opsbridge_namespace&gt; -o json | jq '.data."tls.crt"' --raw-output | base64 -d &gt; chain.pem
kubectl get secret ingress-controller-tls-secret -n &lt;opsbridge_namespace&gt; -o json | jq '.data."tls.key"' --raw-output | base64 -d &gt; key.pem</code></pre>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: In case if you see that ‘jq’ is not installed on the CentOS, Master node. Run the below commands (after consulting your system admin).</div>

<pre><code class="language-bash">yum install epel-release -y
yum install jq -y</code></pre>

<h3 style="margin-top: 16px;">Configure Event Auto Archiving</h3>

<p style="margin-top: 16px;">Follow the instructions in the below document for Event Archiving.<br>
<a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.11/oprArchiveEventsCLI" target="_blank">https://docs.microfocus.com/itom/Operations_Bridge:2019.11/oprArchiveEventsCLI</a></p>

<h3 style="margin-top: 16px;">Clear the UCMDB history</h3>

<p style="margin-top: 16px;">Follow the instructions in the below document to clear/purge the UCMDB history<br>
<a href="https://docs.microfocus.com/itom/Operations_Bridge_Manager:2019.11/ServerAdminPurgingTr" target="_blank">https://docs.microfocus.com/itom/Operations_Bridge_Manager:2019.11/ServerAdminPurgingTr</a></p>

<h3 style="margin-top: 16px;">Back up db connections in BVD</h3>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: Chose to skip this section if BVD as a capability was not installed in your previous OpsBridge Suite environment.</div>

<h4 style="margin-top: 16px;">DB Connections Settings</h4>

<p style="margin-top: 16px;">Login to BVD UI and navigate to Administration -&gt; Data Collectors. Note down the configured values in DB&nbsp;connections settings, created in BVD for data collectors. The db connection need to be re-created manually on BVD post migration to 2019.11.<br>
<img alt="" border="1" file="" height="243" hspace="0" src="https://docs.microfocus.com/mediawiki/images/5/5b/DB_connection_settings.JPG" style="width:496px;height:243px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;" vspace="0" width="496"></p>

<h4 style="margin-top: 16px;">Users and Roles</h4>

<p style="margin-top: 16px;">Login to BVD UI and navigate to Administration -&gt; Users and Roles. &nbsp;Note down all the users, user groups and user roles attributes which is created in BVD. This has to be restored manually if attributes for these objects are observed to be missing post migration.</p>

<h2 style="margin-top: 16px;">Bring down the cluster</h2>

<p style="margin-top: 16px;">Make sure all the PODS are up and running.&nbsp;Shut down Suite pods by Running the below command.&nbsp;Make sure that all the PODS are DOWN and the Suite too. Ignore the Completed pods.</p>

<pre><code class="language-bash">${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l DOWN</code></pre>

<p style="margin-top: 16px;">Run the below command on all the Worker Nodes. Once it completes on the Worker Nodes, execute the same on the Master Nodes.</p>

<pre><code class="language-bash">${K8S_HOME}/bin/kube-stop.sh -y</code></pre>

<p style="margin-top: 16px;">Check if there any mount points still active. On the Master node, run the following command. If you see any residual, then kindly run the second command to un-mount the same. To be executed, in case if the NFS is on the Master node.</p>

<pre><code class="language-bash">df -k | grep kubernetes
for f in $(df | awk '/kubernetes/ {print $6}'); do umount $f; done</code></pre>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: Set ${K8S_HOME} to /opt/kubernetes if not resolved.</div>

<p style="margin-top: 16px;">Shut down the VM’s and take a snapshot of the same Virtual Machines, which are part of the cluster set up.&nbsp;</p>

<h2 style="margin-top: 16px;">Postgres DB migration</h2>

<h3 style="margin-top: 16px;">Pre-requisite</h3>

<ul>
	<li style="margin-top: 16px;">
	<p>Make sure that the CDF cluster is DOWN.</p>
	</li>
	<li style="margin-top: 16px;">
	<p>Enough space for taking the Postgres dump</p>
	</li>
</ul>

<h3 style="margin-top: 16px;">Check postgres connection with super user</h3>

<p style="margin-top: 16px;">From the Postgres Node, run the below command</p>

<pre><code>psql "dbname=&lt;dbname&gt; password=&lt;password&gt; host=&lt;pg_host&gt; port=&lt;pg_port&gt; user=&lt;super_user&gt;"</code></pre>

<p style="margin-top: 16px;">OR</p>

<pre><code class="language-bash">su - postgres
psql postgres postgres</code></pre>

<h3 style="margin-top: 16px;">Get the current Postgres Version</h3>

<p style="margin-top: 16px;">Once logged in as super user, run the below command. This would be good to have a check on the version and its compatibility for upgrade to higher version of Postgres.</p>

<pre><code class="language-sql">SHOW server_version ;</code></pre>

<h3 style="margin-top: 16px;">Get the DB list from the External POSTGRES Node</h3>

<p style="margin-top: 16px;">As a super user, run the below command to get the OpsBridge Suite DB list. As of today, the below listed Databases would be targeted for migration.</p>

<h3 style="margin-top: 16px;">Check if suite username passed is correct</h3>

<pre><code class="language-sql">SELECT datname FROM pg_database WHERE datistemplate = false and datname != 'replication_db' and datname != 'postgres' ;</code></pre>

<p style="margin-top: 16px;">As a super user, run the below command to check if the owner has the access to the OpsBridge Suite DB list. This would be necessary in case it’s automated in the script and also make sure that we know the credentials in the Source and Target environment are same.</p>

<pre><code>Select count(name) from (SELECT d.datname as name,pg_catalog.pg_get_userbyid(d.datdba) as owner FROM pg_catalog.pg_database d )a where name='&lt;suite_db_name&gt;' and owner != 'postgres' and owner = '&lt;suite_db_user&gt;' ;</code></pre>

<h3 style="margin-top: 16px;">Get the Total database size</h3>

<p style="margin-top: 16px;">As a super user, run the below command to get the size of the DB</p>

<pre><code class="language-sql">SELECT sum(pg_database_size(pg_database.datname)/1024.0) FROM pg_database ;</code></pre>

<h3 style="margin-top: 16px;">Dump&nbsp;the OpsBridge databases</h3>

<p style="margin-top: 16px;">Make sure the Postgres DB is UP and RUNNING.&nbsp;Run the below command to get the postgres service name:</p>

<pre><code class="language-bash">systemctl status | grep postgresql | grep service</code></pre>

<p style="margin-top: 16px;">And then check the status of the service using the command. It should show active (running):</p>

<pre><code>systemctl status &lt;postgresql-service-name&gt;</code></pre>

<h4 style="margin-top: 16px;">For large Database:</h4>

<p style="margin-top: 16px;">Run VACCUMLO on Event&nbsp;DB and RTSM&nbsp;DB (not on other DBs). Run the below command to analyze.</p>

<pre><code>/usr/pgsql-&lt;version&gt;/bin/vacuumlo -v -n -h &lt;pg_host&gt; -U &lt;super_user&gt; -W -p &lt;pg_port&gt; &lt;suite_db_name&gt;</code></pre>

<div style="background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;"><strong>Note</strong>: -n is for don’t remove anything, just show what would be done</div>

<p style="margin-top: 16px;">Once confirmed, execute without ‘-n’ option.</p>

<pre><code>vacuumlo -v -h &lt;pg_host&gt; -U &lt;super_user&gt; -W -p &lt;pg_port&gt; &lt;suite_db_name&gt;</code></pre>

<p style="margin-top: 16px;">On completion, one should see the below message<br>
Successfully removed #N large objects from database "&lt;suite_db_name&gt;".<br>
Run the below command to create the dump file for all the databases:</p>

<pre><code>pg_dump -Fp -c -v -h &lt;pg_host&gt; -d &lt;autopassdb&gt; -p &lt;pg_port&gt; -U &lt;super-user&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;autopassdb&gt;.sql ;
pg_dump -Fp -c -v -h &lt;pg_host&gt; -d &lt;cdfidmdb&gt; -p &lt;pg_port&gt; -U &lt;super-user&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;cdfidmdb&gt;.sql ;
pg_dump -Fp -c -v -h &lt;pg_host&gt; -d &lt;bvd&gt; -p &lt;pg_port&gt; -U &lt;super-user&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;bvddb&gt;.sql ;
pg_dump -Fp -c -v -h &lt;pg_host&gt; -d &lt;event&gt; -p &lt;pg_port&gt; -U &lt;super-user&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;eventdb&gt;.sql ;
pg_dump -Fp -c -v -h &lt;pg_host&gt; -d &lt;mgmt&gt; -p &lt;pg_port&gt; -U &lt;super-user&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;mgmtdb&gt;.sql ;
pg_dump -Fp -c -v -h &lt;pg_host&gt; -d &lt;rtsm&gt; -p &lt;pg_port&gt; -U &lt;super-user&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;rtsmdb&gt;.sql ;</code></pre>

<h3 style="margin-top: 16px;">Upgrade the Postgres version</h3>

<p style="margin-top: 16px;">Back up the existing pg_hba.conf and postgresql.conf<br>
Take a backup of the below .conf files from the Postgres server.</p>

<div style="background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;">/var/lib/pgsql/9.5/data/pg_hba.conf<br>
/var/lib/pgsql/9.5/data/postgresql.conf</div>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: The path might change based on the version installed.</div>

<h3 style="margin-top: 16px;">Shutdown the DB</h3>

<p style="margin-top: 16px;">Login as Postgres super user and run the below command.</p>

<pre><code>su - &lt;postgres_su_user&gt;
/usr/pgsql-9.5/bin/pg_ctl stop</code></pre>

<div class="Admonition_Note" style="margin-top: 16px;"><strong>Note</strong>: The path for pg_ctl might change based on the version installed.</div>

<p style="margin-top: 16px;">Rename the old Installation directory</p>

<pre><code class="language-bash">mv /usr/pgsql-9.5 /usr/pgsql-9.5_old</code></pre>

<p style="margin-top: 16px;">Install the new version<br>
Select the suitable options from the below page. Below list of commands is for installing the version Postgres 10.<br>
<a href="https://www.postgresql.org/download/linux/redhat/" target="_blank">https://www.postgresql.org/download/linux/redhat/</a></p>

<pre><code class="language-bash">yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm;
yum install postgresql10;
yum install postgresql10-server;
yum install postgresql10-contrib;
/usr/pgsql-10/bin/postgresql-10-setup initdb;
systemctl enable postgresql-10;
systemctl start postgresql-10;</code></pre>

<p style="margin-top: 16px;">Restore the changes in the conf files<br>
<strong>File Name</strong>:&nbsp;pg_hba.conf<br>
<strong>Location</strong>: /var/lib/pgsql/10/data<br>
Add the below entries at the end of the file</p>

<div style="background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;">host all all 0.0.0.0/0 md5<br>
host all all ::/0 md5</div>

<p style="margin-top: 16px;"><strong>File Name:&nbsp;</strong>postgresql.conf<br>
<strong>Location:</strong> /var/lib/pgsql/10/data<br>
Remove the comment and change the entries of listen_addresses and port to below.</p>

<div style="background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;">listen_addresses = '*'<br>
port = &lt;pg_port&gt;</div>

<p style="margin-top: 16px;">Followed by the restart of the service by running the below command.</p>

<pre><code class="language-bash">systemctl restart postgresql-10.service</code></pre>

<h3 style="margin-top: 16px;">Restore the DUMP</h3>

<p style="margin-top: 16px;">Recreate the Databases and users which existed in the previous version of OpsBridge Postgres DB. The below example is from 2018.11.&nbsp; Kindly refer to respective version of OpsBridge Suite doc, under Prepare Relational Data base section, and execute the same, before importing the data.</p>

<pre><code>CREATE ROLE &lt;cdfidmuser&gt; WITH LOGIN PASSWORD '&lt;Password&gt;' ;
CREATE DATABASE &lt;cdfidmdb&gt; OWNER &lt;cdfidmuser&gt; ;
CREATE DATABASE &lt;autopassdb&gt; OWNER &lt;cdfidmuser&gt; ;
CREATE DATABASE &lt;bvddb&gt; OWNER &lt;cdfidmuser&gt; ;

CREATE DATABASE &lt;rtsmdb&gt; OWNER &lt;cdfidmuser&gt; ;
\c &lt;rtsmdb&gt; ;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp" ;

CREATE DATABASE &lt;mgmtdb&gt; OWNER &lt;cdfidmuser&gt; ;
\c &lt;mgmtdb&gt; ;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp" ;

CREATE DATABASE &lt;eventdb&gt; OWNER &lt;cdfidmuser&gt; ;
\c &lt;eventdb&gt; ;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp" ;</code>
</pre>

<p style="margin-top: 16px;">Check for<strong> \list+, \dn+ and \du+</strong> commands to check the current state.<br>
<br>
Restore the DUMP using the PSQL of upgraded version. One can check the respective DB log files for any errors. Make sure the database names should be same to that of ones in the previous version of OpsBridge Suite set up.</p>

<pre><code>psql -X -q -h &lt;pg_host&gt; -p &lt;pg_port&gt; -U postgres -d &lt;autopassdb&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;autopassdb&gt;.sql -L /tmp/restore_&lt;autopassdb&gt;.log ;
psql -X -q -h &lt;pg_host&gt; -p &lt;pg_port&gt; -U postgres -d &lt;cdfidmdb&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;cdfidmdb&gt;.sql -L /tmp/restore_&lt;cdfidmdb&gt;.log ;
psql -X -q -h &lt;pg_host&gt; -p &lt;pg_port&gt; -U postgres -d &lt;bvd&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;bvddb&gt;.sql -L /tmp/restore_&lt;bvd&gt;.log ;
psql -X -q -h &lt;pg_host&gt; -p &lt;pg_port&gt; -U postgres -d &lt;event&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;eventdb&gt;.sql -L /tmp/restore_&lt;event&gt;.log ;
psql -X -q -h &lt;pg_host&gt; -p &lt;pg_port&gt; -U postgres -d &lt;mgmt&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;mgmtdb&gt;.sql -L /tmp/restore_&lt;mgmt&gt;.log ;
psql -X -q -h &lt;pg_host&gt; -p &lt;pg_port&gt; -U postgres -d &lt;rtsm&gt; -f &lt;PG_DUMP_DIR&gt;/&lt;rtsmdb&gt;.sql -L /tmp/restore_&lt;rtsm&gt;.log ;</code></pre>

<p style="margin-top: 16px;">Once the DB is restored, run the below commands to make the necessary changes for all the Suite Databases.</p>

<pre><code>GRANT &lt;cdfidmuser&gt; TO postgres ; 

\c &lt;cdfidmdb&gt; ;
ALTER SCHEMA public OWNER TO &lt;cdfidmuser&gt; ;
ALTER SCHEMA public RENAME TO &lt;cdfidmschema&gt; ;
REVOKE ALL ON SCHEMA &lt;cdfidmschema&gt; from public ;
GRANT ALL ON SCHEMA &lt;cdfidmschema&gt; to &lt;cdfidmuser&gt; ;
GRANT ALL PRIVILEGES ON DATABASE &lt;cdfidmdb&gt; TO &lt;cdfidmuser&gt; ;
ALTER USER &lt;cdfidmuser&gt; SET search_path TO &lt;cdfidmschema&gt; ;

\c &lt;autopassdb&gt; ;
ALTER SCHEMA public OWNER TO &lt;cdfidmuser&gt; ;
ALTER SCHEMA public RENAME TO &lt;cdfidmschema&gt; ;
REVOKE ALL ON SCHEMA &lt;cdfidmschema&gt; from public ;
GRANT ALL ON SCHEMA &lt;cdfidmschema&gt; to &lt;cdfidmuser&gt; ;
GRANT ALL PRIVILEGES ON DATABASE &lt;autopassdb&gt; TO &lt;cdfidmuser&gt; ;
ALTER USER &lt;cdfidmuser&gt; SET search_path TO &lt;cdfidmschema&gt; ;

\c &lt;bvd&gt; ;
ALTER SCHEMA public OWNER TO &lt;cdfidmuser&gt; ;
ALTER SCHEMA public RENAME TO &lt;cdfidmschema&gt; ;
REVOKE ALL ON SCHEMA &lt;cdfidmschema&gt; from public ;
GRANT ALL ON SCHEMA &lt;cdfidmschema&gt; to &lt;cdfidmuser&gt; ;
GRANT ALL PRIVILEGES ON DATABASE &lt;bvd&gt; TO &lt;cdfidmuser&gt; ;
ALTER USER &lt;cdfidmuser&gt; SET search_path TO &lt;cdfidmschema&gt; ;

\c &lt;rtsm&gt; ;
ALTER SCHEMA public OWNER TO &lt;cdfidmuser&gt; ;
ALTER SCHEMA public RENAME TO &lt;cdfidmschema&gt; ;
REVOKE ALL ON SCHEMA &lt;cdfidmschema&gt; from public ;
GRANT ALL ON SCHEMA &lt;cdfidmschema&gt; to &lt;cdfidmuser&gt; ;
GRANT ALL PRIVILEGES ON DATABASE &lt;rtsm&gt; TO &lt;cdfidmuser&gt; ;
ALTER USER &lt;cdfidmuser&gt; SET search_path TO &lt;cdfidmschema&gt; ;

\c &lt;mgmt&gt; ;
ALTER SCHEMA public OWNER TO &lt;cdfidmuser&gt; ;
ALTER SCHEMA public RENAME TO &lt;cdfidmschema&gt; ;
REVOKE ALL ON SCHEMA &lt;cdfidmschema&gt; from public ;
GRANT ALL ON SCHEMA &lt;cdfidmschema&gt; to &lt;cdfidmuser&gt; ;
GRANT ALL PRIVILEGES ON DATABASE &lt;mgmt&gt; TO &lt;cdfidmuser&gt; ;
ALTER USER &lt;cdfidmuser&gt; SET search_path TO &lt;cdfidmschema&gt; ;

\c &lt;event&gt; ;
ALTER SCHEMA public OWNER TO &lt;cdfidmuser&gt; ;
ALTER SCHEMA public RENAME TO &lt;cdfidmschema&gt; ;
REVOKE ALL ON SCHEMA &lt;cdfidmschema&gt; from public ;
GRANT ALL ON SCHEMA &lt;cdfidmschema&gt; to &lt;cdfidmuser&gt; ;
GRANT ALL PRIVILEGES ON DATABASE &lt;event&gt; TO &lt;cdfidmuser&gt; ;
ALTER USER &lt;cdfidmuser&gt; SET search_path TO &lt;cdfidmschema&gt; ;
</code></pre>

<h3>Check Database and Users</h3>

<p>Login as super&nbsp;user and check if the databases and users are intact.</p>

<pre><code>su - &lt;postgres_su_user&gt;

\list+
\du+
\dn+</code></pre>

<p>One should see a similar output as shown below.<br>
<img alt="" border="1" file="" height="341" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f02ff2aa3b944.28065111.jpeg" style="width:800px;height:341px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;" vspace="0" width="800"><br>
Try connecting to the databases using the Suite user name</p>

<pre><code>psql -h &lt;pg_host&gt; dbname=&lt;suite_db&gt; -p &lt;pg_port&gt; -U &lt;suite_user_name&gt;</code></pre>

<h2>Upgrade the Vertica Version</h2>

<p>Drop the old data bases after stopping them. And make sure to upgrade the Vertica to the version supported by OpsBridge Suite 2019.11</p>

<h2>Uninstall older version of OpsBridge Suite</h2>

<p>Follow the steps mentioned in the respective OpsBridge Suite version document to completely uninstall the Suite.&nbsp;</p>

<h2>Install 2019.11&nbsp;</h2>

<p>This section guides you through the settings/credentials which needs to be used from the previous version of OpsBridge Suite, during the course of installing OpsBridge Suite 2019.11 with External Postgres.</p>

<h3>Maintain the old Administrator Password</h3>

<p><img alt="" border="1" file="" height="295" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f02ff2b52dbc6.88211836.jpeg" style="width:800px;height:295px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;" vspace="0" width="800"></p>

<h3>Capabilities</h3>

<p>Feel free to choose additional capabilities as compared to ones chosen in the previous version of OpsBridge Suite set up.<br>
<img alt="" border="1" file="" height="299" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f0302d89f4407.30330001.jpeg" style="width: 800px; height: 299px; margin: 0px; border: 1px solid black;" vspace="0" width="800"></p>

<h3>Database</h3>

<p>Select PostgreSQL option instead of Out-of-the-box PostgreSQL. Make sure you enter the same Database Host, Port associated with the external Postgres which was used in the previous version of OpsBridge Suite set up. Similarly maintain the same entries for Database Name, User and Password, which was used in the previous version of OpsBridge Suite set up. Ignore the warning as the DB (though created) would be empty. &nbsp;</p>

<h3>Deployment Size</h3>

<p>Choose the deployment size in accordance to the current requirement</p>

<h3>Connection</h3>

<h4>Single Master to Multi-Master switch</h4>

<p>Single Master to Single Master, keep the same FQDN as that of the previous version of OpsBridge Suite set up as the External Hostname.<br>
Single Master to Multi Master (for Master High availability), make sure you configure the previous value of External Hostname as a VIP and update the same in the field External Hostname. Which would mean, one would need 3 new nodes to be added as Master Nodes. And the Master’s IP and FQDN from the previous version of&nbsp;OpsBridge Suite set up should be used as HA_VIRTUAL_IP. The Old Single Master node should NOT be up and&nbsp;running.</p>

<div class="Admonition_Important">This is very crucial for integrations.</div>

<p>The Agents and SiS nodes would be in buffering state till the DOWN time. Once the Cluster and the capabilities are UP, and the connectivity is established, the buffered Events should reach to the OMi Event Browser.</p>

<h4>External Hostname</h4>

<p>In the Connection page (After selecting the Deployment Size), make sure that the External Hostname is properly configured.</p>

<table border="1" cellpadding="1" cellspacing="1" style="width: 1200px;">
	<thead>
		<tr>
			<th scope="col">Older version</th>
			<th scope="col">2019.11</th>
			<th scope="col">External Host Name</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>Multiple Master</td>
			<td>Multiple Master</td>
			<td>Keep the same as used in previous version of OpsBridge Suite set up, which would be the HA_VIRTUAL_IP</td>
		</tr>
		<tr>
			<td>Single Master</td>
			<td>Single Master</td>
			<td>Keep the same as used in previous version of OpsBridge Suite set up, which would be the FQDN of the Single Master</td>
		</tr>
		<tr>
			<td>Single Master</td>
			<td>Multiple Master</td>
			<td>Configure the External Hostname, which was used in previous version of OpsBridge Suite set up, as the HA_VIRTUAL_IP of the new 2019.11 set up. And this would mandate additional Master nodes as part of the cluster infrastructure.</td>
		</tr>
	</tbody>
</table>

<h4>Custom certificates</h4>

<p>Select the option ‘Use custom certificates’ and upload the respective RE_CA.CRT, CHAIN.PEM and KEY.PM <a href="#Back up Certificate realms and PEM files">exported from the previous version of OpsBridge Suite</a>&nbsp;set up as shown below.</p>

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<tbody>
		<tr>
			<td>CA certificate</td>
			<td>RE_CA.CRT</td>
		</tr>
		<tr>
			<td>Server Certificate</td>
			<td>CHAIN.PEM</td>
		</tr>
		<tr>
			<td>Server Private key</td>
			<td>KEY.PEM</td>
		</tr>
	</tbody>
</table>

<p><img alt="" border="1" file="" height="377" hspace="0" src="https://docs.microfocus.com/mediawiki/images/f/ff/CertificateCapture.JPG" style="width:732px;height:377px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;" vspace="0" width="732"></p>

<h3>Deploy Operations Bridge</h3>

<p>Make sure you enter the values (for user name and password as part of credentials) which were used in the previous version of OpsBridge Suite set up. The below set of guidance must be strictly followed.</p>

<div class="Admonition_Important">Choose Custom configuration.</div>

<div class="Admonition_Important">Use the certificate generated by the ITOM platform or use Upload certificates (in case if you had uploaded custom certificates for previous version of OpsBridge Suite set up).<br>
<img alt="" border="1" file="" height="309" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f0cc5a6bdb2a3.86405219.jpeg" style="width: 800px; height: 309px; margin: 0px; border: 1px solid black;" vspace="0" width="800"></div>

<div class="Admonition_Important">For JMX password, set the password for the Jmx administrator to same as configured in the previous version of&nbsp;OpsBridge Suite set up.</div>

<div class="Admonition_Important">For Database settings, set the names to same as configured in the previous version of OpsBridge Suite set up</div>

<div class="Admonition_Important">Enable high availability for Operations Bridge Manager, if it was set in the previous version of OpsBridge Suite set up. If it was not, you can still opt for High Availability for 2019.11.<br>
<img alt="" border="1" file="" height="249" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f0cc5a73036c9.89430749.jpeg" style="width: 700px; height: 249px; margin: 0px; border: 1px solid black;" vspace="0" width="700"></div>

<div class="Admonition_Important">Select the Management Packs which were chosen in the previous version of OpsBridge Suite set up</div>

<div class="Admonition_Important">For BVD Login, use suite default administrative user account in case if you didn’t had custom credentials set in the previous version of OpsBridge Suite set up.<br>
<img alt="" border="1" file="" height="210" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f0cc5a7992c16.76041482.jpeg" style="width: 700px; height: 210px; margin: 0px; border: 1px solid black;" vspace="0" width="700"></div>

<div class="Admonition_Important">For BVD Database, set the names to same as configured in the previous version of OpsBridge Suite set up.</div>

<div class="Admonition_Note"><strong>Note</strong>: The Vertica DB, user and credentials needs to be provided as per the new set up.<br>
<img alt="" border="1" file="" height="347" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f0cc5a80748c7.63197628.jpeg" style="width: 800px; height: 347px; margin: 0px; border: 1px solid black;" vspace="0" width="800"></div>

<h3>Restore MASTER_KEY</h3>

<p>Once the ucmd pod is up and running (1/2), restore the Master_key by logging into the ucmdb container, with the value saved from the previous OpsBridge Suite set up.</p>

<pre><code>update_secret MASTER_KEY &lt;ValueSaved&gt;</code></pre>

<div class="Admonition_Important"><strong>Note</strong>: UCMDB Pod might take some time to be up and running ( internally we have seen the ucmdb pod to take more than a hour ). Since OBM pods depend on UCMDB, so they also would be in 1/2 status.</div>

<h3>Configure the YAML</h3>

<p>Edit the respective kubernetes objects like deployments, replica sets etc., to match to that of the previous configurations and values. One can do the same via the Management Portal or via editing the YAML files. Check the number of Replicas which was saved in <strong>ReplicaSets.txt.</strong></p>

<h3>Operations Agent XPL settings</h3>

<p>Change the values from the old set up (local_settings.ini) to update them in the new one, and restart Operations Agent</p>

<pre><code>ovconfchg -ns &lt;Namespace&gt; -set &lt;Variable&gt; &lt;Value&gt;
ovconfchg -ns &lt;Namespace&gt; -set &lt;Variable&gt; &lt;Value&gt; -ovrg server</code></pre>

<h3>Custom node list and ucmdb config.json</h3>

<div class="Admonition_Note"><strong>Note</strong>: Skip if Agent Metric Collector was NEVER configured in the older version, and also If it is a NEW capability chosen in 2019.11.</div>

<p>In case if you do NOT want to configure or customize the node list, then feel free to skip this section. One needs to place the below files, saved from the old environment to the below NFS location.</p>

<h4>OBM containerized</h4>

<p><strong>File name</strong>: Custom Node list file<br>
<strong>Location:</strong> &lt;NFS-conf-volume&gt;/content-administration/var/content-administration/content/custom/</p>

<h4>External OBM</h4>

<p>Login to the itom-opsb-content-administration POD, and place the old ucmdb config json file in the below mentioned file. Use the ‘kubectl cp’ command mentioned below to place of the same file.<br>
<strong>File name:</strong> sample_ucmdb_config.json<br>
<strong>Location:</strong> /opt/content-administration/conf</p>

<pre><code>kubectl -n &lt;opsbridge_namespace&gt; exec -ti &lt;itom-opsb-content-administration-pod&gt; -c itom-opsb-content-administration bash
kubectl cp /tmp/sample_ucmdb_config.json &lt;opsbridge_namespace&gt;/&lt;itom-opsb-content-administration-pod&gt;:/opt/content-administration/conf/sample_ucmdb_config.json -c itom-opsb-content-administration</code></pre>

<h3>For Streaming use cases&nbsp;</h3>

<p>In order to continue streaming from Sources like Operations Agent node, Site Scope, BPM and APM, one must update the “MF CDF RID CA” as the Trusted Certificates on both OBM and then on the connected streaming data sources.</p>

<h4>Get the new RID CA</h4>

<p>Run the below command to get RID CA and copy it inside the OBM container&nbsp;</p>

<pre><code>kubectl cp core/&lt;suite-conf-pod-opsbridge-pod&gt;:/var/run/secrets/boostport.com/trustedCAs/RID_ca.crt /tmp/RID_ca.crt -c kubernetes-vault-renew</code></pre>

<h4>Import the RID CA</h4>

<h5>OBM container</h5>

<p>Copy the RID_ca.crt into the OBM container. And then login to the OBM pod and run the following commands to update the Trusted Certificates.</p>

<pre><code>kubectl cp /tmp/RID_ca.crt &lt;opsbridge-namespace&gt;/omi-0:/tmp -c omi

kubectl exec &lt;omi-POD&gt; -c omi -n &lt;opsbridge-namespace&gt; -ti -- bash
su – omiuser

/opt/OV/bin/ovcert -list
/opt/OV/bin/ovcert -remove "&lt;MF CDF RID CA on &gt;" -ovrg server -f
/opt/OV/bin/ovcert -importtrusted -file /tmp/RID_ca.crt -ovrg server
/opt/OV/bin/ovcert -remove "&lt;MF CDF RID CA on &gt;" -f
/opt/OV/bin/ovcert -importtrusted -file /tmp/RID_ca.crt</code></pre>

<p>Once done, restart the ovcs on OBM pod (execute on both omi-0 and omi-1 pods if in HA)</p>

<pre><code class="language-bash">/opt/OV/bin/ovc -restart ovcs</code></pre>

<h5>OBM Classic</h5>

<p>Transfer the file securely to the OBM. And run the same commands as shared above. Make sure you provide the right location of RID_ca.crt. And also restart ‘ovcs’</p>

<h4>Update on Data Sources</h4>

<h5>Linux Nodes</h5>

<pre><code>/opt/OV/bin/ovcert -list
/opt/OV/bin/ovcert -remove "&lt;MF CDF RID CA on &gt;" -ovrg server -f
/opt/OV/bin/ovcert -updatetrusted</code></pre>

<h5>Windows Nodes</h5>

<pre><code>ovcert -list
ovcert -remove "&lt;MF CDF RID CA on &gt;" -ovrg server -f
ovcert -updatetrusted</code></pre>

<h5>AIX nodes</h5>

<pre><code>/usr/lpp/OV/bin/ovcert -list
/usr/lpp/OV/bin/ovcert -remove "&lt;MF CDF RID CA on &gt;" -ovrg server -f
/usr/lpp/OV/bin/ovcert -updatetrusted</code></pre>

<h4>Customers with multiple Data Sources</h4>

<p>They can use the feature of Tools to achieve the same. Login to the OBM UI and create a new tool by following the below instructions.</p>

<address><strong>Administration &gt;&gt; Operations Console &gt;&gt; Tools<br>
CI Types &gt;&gt; Infrastructure Element &gt;&gt; Node &gt;&gt; Computer</strong></address>

<p>Select the option to create a new tool. Name it as per your choice.<br>
Make sure to select the Category as Other Category and Operations Agent from the Drop down Menu.<br>
<img alt="" border="1" file="" height="436" hspace="0" src="https://docs.microfocus.com/mediawiki/images/a/a2/Enable_streaming.jpg" style="width:502px;height:436px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;" vspace="0" width="502"><br>
In the command Tab, enter the following command. Make sure to replace the External Host Name in the command with the one reflected in the MF CDF RID CA on the Data Sources/Nodes.</p>

<pre><code>ovdeploy -cmd "ovcert -remove \"MF CDF RID CA on &lt;External Host Name&gt;\" -f " -host ${opr.monitor.host} -ovrg server; ovdeploy -cmd "ovcert -updatetrusted" -host ${opr.monitor.host} -ovrg server</code></pre>

<p>Make sure the Target, Run on is chosen as Management Server. Once the details are entered, click on OK to SAVE.<br>
<img alt="" border="1" file="" height="438" hspace="0" src="https://docs.microfocus.com/mediawiki/images/a/a7/Enable_streaming_2.jpg" style="width:501px;height:438px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;" vspace="0" width="501"><br>
On the execution side, traverse to below UI. Select the Node or Node list and Select Launch Tool option. Select the above Tool and click on RUN button. One would see the execution results as shown below.</p>

<address><strong>Administration &gt;&gt; Setup and Maintenance &gt;&gt; Monitored Nodes &gt;&gt; Nodes with Operations Agent</strong></address>

<p><img alt="" border="1" file="" height="449" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f071aaf0e1c51.60527513.jpeg" style="width: 800px; height: 449px; margin: 0px; border: 1px solid black;" vspace="0" width="800"></p>

<h4>Validate the connectivity</h4>

<h5>Check the details of the RID CA on Server and Node</h5>

<p>If you have keytool installed on your Linux nodes, then run the below command. Once must take a note on the Owner, Issues and MD5 values.</p>

<pre><code class="language-bash">keytool -printcert -v -file ca.crt | egrep "Certificate\[|Owner|Issuer|MD5"</code></pre>

<p>Wherein, the ca.crt would the result of the below command (on both OBM and Managed streaming nodes)<br>
<strong>Linux Nodes</strong></p>

<pre><code class="language-bash">/opt/OV/bin/ovcert -exporttrusted -file ca.crt</code></pre>

<p>For Windows, one can get the certificate file to a Linux node, and do the above steps mentioned.<br>
<strong>AIX Nodes</strong></p>

<pre><code class="language-bash">/opt/OV/bin/ovcert -exporttrusted -file ca.crt</code></pre>

<h5><img alt="" border="1" file="" height="138" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f071aaf7440c2.60709731.jpeg" style="width: 800px; height: 138px; margin: 0px; border: 1px solid black;" vspace="0" width="800"><br>
Using the bbcutil ping</h5>

<p>One can also validate to see if there is NO eSSLError, while executing bbcutil -ping to the Data Receiver Endpoint. eServiceError is what is expected, which would mean that there are NO SSL errors.<br>
<strong>Linux Nodes</strong></p>

<pre><code>/opt/OV/bin/bbcutil -ping https://&lt;External_HostName&gt;:36500/itomdi/receiver</code></pre>

<p><strong>Windows Nodes</strong></p>

<pre><code>bbcutil -ping https://&lt;External_HostName&gt;:36500/itomdi/receiver</code></pre>

<p><strong>AIX Nodes</strong></p>

<pre><code>/usr/lpp/OV/bin/bbcutil -ping https://&lt;External_HostName&gt;:36500/itomdi/receiver</code></pre>

<p><img alt="" border="1" file="" height="81" hspace="0" src="/mediawiki/images/pn_images/pn_image_5f071ab01b6677.42670546.jpeg" style="width: 800px; height: 81px; margin: 0px; border: 1px solid black;" vspace="0" width="800"></p>

<h3>Restore BVD db connections</h3>

<p>You need to set up database connection in BVD 2019.11 as shown below with the external database that was used for BVD with previous version of&nbsp;OpsBridge Suite set up to ensure that the dashboard is able to display all the data elements. &nbsp;</p>

<ol>
	<li>Click Setting and select Data Collectors.&nbsp;</li>
	<li>On Data Collectors page, click three dots&nbsp;and select <strong>DB Connection Settings</strong>.&nbsp;</li>
	<li>Fill in all required details in SET UP DB CONNECTION form. The details you enter here should contain the same information as in previous BVD database connection.</li>
	<li>Click TEST CONNECTION to ensure the database connection.&nbsp;</li>
	<li>Click SAVE SETTINGS.&nbsp;</li>
</ol>

<p>Once you have set up the database connection, you need to run all the queries at least once in BVD 2019.11 to enable dashboards to show all the data elements.&nbsp;</p>

<ol>
	<li>Click settings, select Data Collectors to list all the data or parameter queries.&nbsp;</li>
	<li>Click edit button to view all the details of the query.&nbsp;</li>
	<li>Click RUN button&nbsp;under QUERY*.</li>
	<li>After you run the query, click SAVE DATA QUERY.&nbsp;</li>
	<li>You need to edit and run all the queries listed under Data Collector.&nbsp;</li>
</ol>

<p>Once you perform all of the above steps, view the dashboards to verify whether you can see all the dashboard elements.<br>
For example, when you run a data query called Multi-category and save the query, you will be able to see the updated values on the dashboard. When you hover mouse-pointer over Multi-category under Data Channel, you will see when was the latest update made.</p>

<h3>OBM User Roles</h3>

<p>Post migration you may observe few custom user roles do not retain permissions granted in previous version of OpsBridge suite set up. &nbsp;Follow below steps to fix this issue:</p>

<ol>
	<li>Login to OBM UI</li>
	<li>Navigate to <strong>Administration &gt; Users &gt; Users, Groups, and Roles</strong></li>
	<li>Select the role</li>
	<li>Edit the role</li>
	<li>Save Role</li>
</ol>

<h3>BVD Users &amp; Groups</h3>

<p>In few cases it&nbsp;was observed that post migration BVD does not retain Users and Groups attributes. Hence edit the settings for each of these objects and restore them manually.</p>
</html>