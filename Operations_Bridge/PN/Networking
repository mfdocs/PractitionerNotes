<h1 style="margin-bottom:0px;">Networking</h1><div class="f14 metric" style="color:#656668;display:flex;"><img src="/assets/images/calendar.png" style="height:12px;margin-top:auto;margin-bottom:auto;"><span class="updateDate">&nbsp; Updated on 26/06/2019</span> &nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp; <img src="/assets/images/updated.png" style="height:12px;margin-top:auto;margin-bottom:auto;"><span class="readTime">&nbsp; 14 minutes to read</span>&nbsp;&nbsp;&nbsp;&nbsp;</div><br><div class="mw-parser-output"><p><br> 
</p><p><br>
Networking in the CDF is based on Docker and Kubernetes networking concepts and implementation so first some information about these.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Docker"><span class="tocnumber">1</span> <span class="toctext">Docker</span></a></li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#CDF"><span class="tocnumber">2</span> <span class="toctext">CDF</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Cluster_network_configuration"><span class="tocnumber">2.1</span> <span class="toctext">Cluster network configuration</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Network_fabric_for_Kubernetes"><span class="tocnumber">2.2</span> <span class="toctext">Network fabric for Kubernetes</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Flannel"><span class="tocnumber">2.3</span> <span class="toctext">Flannel</span></a>
<ul>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Host-gw_configuration_and_data_flow"><span class="tocnumber">2.3.1</span> <span class="toctext">Host-gw configuration and data flow</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Vxlan_configuration_and_data_flow"><span class="tocnumber">2.3.2</span> <span class="toctext">Vxlan configuration and data flow</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Using_Flannel_with_Docker"><span class="tocnumber">2.3.3</span> <span class="toctext">Using Flannel with Docker</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Pods_and_Services"><span class="tocnumber">2.4</span> <span class="toctext">Pods and Services</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Ingress_.2F_egress"><span class="tocnumber">2.5</span> <span class="toctext">Ingress / egress</span></a>
<ul>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Ingress"><span class="tocnumber">2.5.1</span> <span class="toctext">Ingress</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Egress"><span class="tocnumber">2.5.2</span> <span class="toctext">Egress</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Name_resolution_.28DNS.29"><span class="tocnumber">2.6</span> <span class="toctext">Name resolution (DNS)</span></a>
<ul>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#When_a_customer_DNS_resolution_is_available"><span class="tocnumber">2.6.1</span> <span class="toctext">When a customer DNS resolution is available</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#When_a_customer_DNS_resolution_is_not_available"><span class="tocnumber">2.6.2</span> <span class="toctext">When a customer DNS resolution is not available</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#Ports"><span class="tocnumber">2.7</span> <span class="toctext">Ports</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/Operations_Bridge:2019.05/Networking#ITOM_Suites"><span class="tocnumber">3</span> <span class="toctext">ITOM Suites</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Docker">Docker</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-1" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-1" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>See: <a target="1" rel="nofollow" class="external free" href="https://docs.docker.com/engine/userguide/networking/">https://docs.docker.com/engine/userguide/networking/</a>
</p><p>In the Docker networking model every container has a single network interface eth0 which is mapped into the container from a virtual interface (veth) at the host level that Docker creates for every container. These virtual interfaces are connected to a bridge-type interface "cni0" or “docker0” and a subnet from which the containers receive an IP address.
</p><p><a onclick="javascript:loadingImage(this);" class="image"><img alt="pic22.png" src="/mediawiki/images/thumb/7/79/pic22.png/800px-pic22.png" width="800" height="301" srcset="/mediawiki/images/thumb/7/79/pic22.png/1200px-pic22.png 1.5x, /mediawiki/images/thumb/7/79/pic22.png/1600px-pic22.png 2x" data-file-width="1712" data-file-height="645"></a>
</p><p>Figure - Docker host/container networking
</p><p>This model allows each container on a Docker host to talk to every other container on the same host but not to a container running on another host.
</p><p>To allow cross-host communication requires setting up port forwarding and/or proxying.
</p><p>Cross-host communication is required in a Kubernetes cluster which almost always consists of multiple Docker hosts.
</p>
<h2><span class="mw-headline" id="CDF">CDF</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-2" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-2" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Cluster_network_configuration">Cluster network configuration</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-3" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-3" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a onclick="javascript:loadingImage(this);" class="image"><img alt="pic23.png" src="/mediawiki/images/thumb/2/2d/pic23.png/900px-pic23.png" width="900" height="490" srcset="/mediawiki/images/thumb/2/2d/pic23.png/1350px-pic23.png 1.5x, /mediawiki/images/2/2d/pic23.png 2x" data-file-width="1730" data-file-height="942"></a>
</p><p>Figure - Cluster network diagram
</p>
<h3><span class="mw-headline" id="Network_fabric_for_Kubernetes">Network fabric for Kubernetes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-4" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-4" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>See: <a target="1" rel="nofollow" class="external free" href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a>
</p><p>Setting up port forwarding across multiple containers, hosts and users is not a viable model as the number of containers, hosts and users rises.
</p><p>Therefore Kubernetes requires a slightly different network fabric configuration.
</p><p>Kubernetes by itself will not work if you were to install it across multiple Docker hosts without the configuration of some network fabric.
</p><p>The fabric provides the ability for each container on every host to talk to every other container on every other node.
</p><p>No network address translation (NAT) is required. The Kubernetes documentation words this nicely as three items:
</p>
<ul><li>all containers can communicate with all other containers without NAT</li>
<li>all nodes can communicate with all containers (and vice-versa) without NAT</li>
<li>the IP that a container sees itself as is the same IP that others see it as</li></ul>
<h3><span class="mw-headline" id="Flannel">Flannel</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-5" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-5" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>When the CDF installs and configures Kubernetes, it configures additional network fabric. CDF supports only the Flannel network fabric.
</p><p>Every host gets its own flannel daemon running which creates a subnet allocated in a preset range: 172.16.0.0/16 for containers and 172.17.17.0/24 for Kubernetes services.
</p><p>The subnet configuration is handed over to Kubernetes and the Docker daemon which uses it to generate IP addresses for its containers.
</p><p>The subnet that Flannel generates is unique for cluster node. This guarantees that all Pods get a cluster-wide unique IP address. The uniqueness is guaranteed by Flannel using Etcd to store the subnet definitions.
</p><p>The Flannel subnet is passed onto the Docker daemon using the --bip option. This happens at install time.
</p><p>The subnet that Flannel generates does not change for a node during the node lifecycle. It persists across reboots.
</p><p>CDF supports two flannel configuration modes:
</p>
<ul><li>host-gw
<ul><li>Packet forwarding via iptables rules set by flanneld</li>
<li>Packets not encapsulated</li>
<li>Requires layer 2 connectivity between nodes</li>
<li>Packet forwarding occurs at the physical layer ie NIC</li>
<li>Requires all CDF nodes to be in a single subnet</li></ul></li>
<li>vxlan
<ul><li>Packet forwarding via Linux kernel VXLAN functionality</li>
<li>Creates a virtual L2 network over a physical L3 network.</li>
<li>Works by encapsulating L2 packet in an L3 packet.</li>
<li>Requires only the master nodes to be in the same subnet</li></ul></li></ul>
<h5><span class="mw-headline" id="Host-gw_configuration_and_data_flow">Host-gw configuration and data flow</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-6" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-6" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p><a onclick="javascript:loadingImage(this);" class="image"><img alt="pic24.png" src="/mediawiki/images/thumb/f/f8/pic24.png/900px-pic24.png" width="900" height="549" srcset="/mediawiki/images/thumb/f/f8/pic24.png/1350px-pic24.png 1.5x, /mediawiki/images/f/f8/pic24.png 2x" data-file-width="1371" data-file-height="837"></a>
</p><p>Figure - Flannel host-gw configuration and data flow
</p>
<h5><span class="mw-headline" id="Vxlan_configuration_and_data_flow">Vxlan configuration and data flow</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-7" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-7" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p><a onclick="javascript:loadingImage(this);" class="image"><img alt="pic25.png" src="/mediawiki/images/thumb/5/59/pic25.png/900px-pic25.png" width="900" height="299" srcset="/mediawiki/images/thumb/5/59/pic25.png/1350px-pic25.png 1.5x, /mediawiki/images/5/59/pic25.png 2x" data-file-width="1575" data-file-height="523"></a>
</p><p>Figure - Flannel vxlan configuration and data flow
</p>
<h5><span class="mw-headline" id="Using_Flannel_with_Docker">Using Flannel with Docker</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-8" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-8" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>See also <a href="./itom/ITOM_Container_Deployment_Foundation:2019.05/WhitepaperAppendix1" title="ITOM Container Deployment Foundation:2019.05/WhitepaperAppendix1">APPENDIX 1: Docker and Flannel</a>
</p><p>The Docker daemon assigns DHCP IP addresses for the containers on this node from its assigned network CIDR.
</p><p>Kubernetes records this container IP address and stores the information in Etcd and decorates the pause container with the network information.
</p><p>IP address allocation has two exceptions:
</p><p>Case 1: containers that share the host IP address:
</p>
<ul><li>Containers started directly on Docker with --net=host: keepalived</li>
<li>Kubernetes Pods started on Workload Docker with “hostNetwork: true”: Kubernetes API server, Kubernetes controller manager, Kube Proxy, Kube Scheduler</li></ul>
<p>Case 2: Kubernetes Services (Kind:Service): assigned from a network defined when the Kubernetes API server is started. This does not overlap with any Pod IPs.
</p>
<pre class="syntaxhighlighter-pre"># kubectl describe svc mng-portal -n core

Name: mng-portal
Namespace: core
Labels: app=mng-portal
Annotations: &lt;none&gt;
Selector: app=mng-portal
Type: ClusterIP
IP: 172.17.17.117
Port: &lt;unset&gt; 80/TCP
TargetPort: 9090/TCP
Endpoints: 172.16.98.2:9090
Session Affinity: None
Events: &lt;none&gt;
</pre>
<p>The cluster services IP address is specified on the Kubernetes API server, see &lt;CDF_ROOT&gt;/manifests/kube-apiserver.yaml:
</p>
<pre class="syntaxhighlighter-pre">…..
- command:
- /hyperkube
- apiserver
# - --advertise-address=myd-vm01847.mycompany.net
- --bind-address=0.0.0.0
- --etcd-servers=https://myd-vm01847.mycompany.net:4001
- --insecure-port=0
- --secure-port=8443
# - --token-auth-file=/etc/kubernetes/ssl/token
- --service-cluster-ip-range=172.17.17.0/24
…..
</pre>
<p>All other CDF system/core containers have an IP addresses assigned by the Docker daemon from the network defined by Flannel.
</p><p>The following diagram shows a practical example:
</p>
<ul><li>The host node has a statically assigned IP address from 16.0.0.0/16</li>
<li>The Docker daemon is only accessible via localhost and has a container subnet selected from 172.16.0.0/16. The actual subnet will be selected by Flannel for this node.</li>
<li>Kubernetes Kubelet runs directly on the node and shares the host’s IP address.</li>
<li>Various other Kubernetes components run containerized but share the host’s IP address (*).</li>
<li>All other CDF and Suite components run containerized and get an IP addressed assigned in the 172.16.0.0/16 Flannel selected subnet (*2).</li></ul>
<p><a onclick="javascript:loadingImage(this);" class="image"><img alt="pic26.png" src="/mediawiki/images/thumb/a/ac/pic26.png/900px-pic26.png" width="900" height="398" srcset="/mediawiki/images/thumb/a/ac/pic26.png/1350px-pic26.png 1.5x, /mediawiki/images/thumb/a/ac/pic26.png/1800px-pic26.png 2x" data-file-width="1820" data-file-height="804"></a>
</p><p>Figure - CDF networking
</p><p>The Pod is decorated with the DHCP IP address in the local node subnet address range.
</p><p>All containers that make up the Pod share the same IP address. Inside the Pod, containers can talk to each other’s processes using “localhost”.
</p><p>The decoration is stored on a container called “pause”, using the image gcr.io/google_containers/pause-amd64:3.1 (different CDF releases may use a newer or different image). These containers are only visible when using direct “docker” commands.
</p><p>For more detail on Flannel, see: <a target="1" rel="nofollow" class="external free" href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a>
</p><p>Kubernetes clusters are closed from a network perspective. All intra-container traffic stays inside the cluster. No outside access to any internal cluster services is possible. All Ingress has to be declared explicitly: traffic from outside the cluster is only possible if explicitly allowed.
</p><p>Cross-node traffic is secured using TLS.
</p>
<div class="Admonition_Note">
<p><span class="autonumber">Note</span><br>
Refer to the Suite documentation for any exceptions.
</p>
</div>
<h3><span class="mw-headline" id="Pods_and_Services">Pods and Services</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-9" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-9" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Pods in Kubernetes are ephemeral. They can be created and destroyed dynamically. When destroyed, they will not have the same network identify.
</p><p>For this reason, Services were defined as a logical group of Pods. Services are long lived. Services load balance incoming traffic across the set of Pods that they group.
</p><p>A service represents a micro-service, such as a web server or a database engine. Services can be discovered using environment variables (every container gets injected with all of the services in its namespace) or DNS (services are named get a fixed long-lived DNS name and can be addressed this way).
</p><p>Inside the cluster containers can talk to micro-services exposed via Services without having to know where the service runs, how many replicas serve it or what its IP address is.
</p><p>Cluster services receive IP address from the cluster service IP range. This is preset to 172.17.17.0/24. All cluster services will receive an IP address allocated in this range. (This range is a configuration parameter to the Kubernetes API server, see <a target="1" rel="nofollow" class="external free" href="https://kubernetes.io/docs/admin/kube-apiserver/">https://kubernetes.io/docs/admin/kube-apiserver/</a>, --service-cluster-ip-range parameter.
</p>
<h3><span id="Ingress_/_egress"></span><span class="mw-headline" id="Ingress_.2F_egress">Ingress / egress</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-10" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-10" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<h5><span class="mw-headline" id="Ingress">Ingress</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-11" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-11" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>To access a service from an external network requires adding some additional information to expose it via a port on all nodes or via a load balancer.
</p><p>In the CDF services are of type clusterIP so by default they are not visible from outside the cluster. They may or may not be grouped under an Ingress controller to provide a unified URL namespace and access to the cluster services.
</p><p>A CDF plus a suite install register many services:
</p><p>Run: <code>kubectl get svc --all-namespaces</code>
</p>
<pre class="syntaxhighlighter-pre">NAMESPACE     NAME                            TYPE           CLUSTER-IP      EXTERNAL-IP                              PORT(S)                        AGE
core          cdf-suitefrontend-svc           ClusterIP      172.17.17.224   &lt;none&gt;                                   8443/TCP                       6d14h
core          cdf-svc                         ClusterIP      172.17.17.44    &lt;none&gt;                                   8080/TCP,8443/TCP              6d16h
core          default-postgresql-svc          ClusterIP      172.17.17.190   &lt;none&gt;                                   5432/TCP                       6d14h
core          idm                             ClusterIP      172.17.17.142   &lt;none&gt;                                   443/TCP,444/TCP                6d14h
core          idm-postgresql-svc              ClusterIP      172.17.17.63    &lt;none&gt;                                   5432/TCP                       6d14h
core          idm-svc                         ClusterIP      172.17.17.156   &lt;none&gt;                                   443/TCP,444/TCP                6d14h
core          itom-cdf-ingress-frontend-svc   NodePort       172.17.17.89    &lt;none&gt;                                   3000:3000/TCP                  6d16h
core          itom-k8s-dashboard-svc          ClusterIP      172.17.17.35    &lt;none&gt;                                   443/TCP                        6d14h
core          itom-vault                      NodePort       172.17.17.62    &lt;none&gt;                                   8200:8200/TCP,8201:16916/TCP   6d16h
core          kube-dns                        ExternalName   &lt;none&gt;          kube-dns.kube-system.svc.cluster.local   &lt;none&gt;                         6d16h
core          kube-registry                   NodePort       172.17.17.106   &lt;none&gt;                                   5000:5000/TCP                  6d14h
core          kubernetes-vault                ClusterIP      None            &lt;none&gt;                                   80/TCP,8898/TCP                6d16h
core          metrics-server                  ClusterIP      172.17.17.21    &lt;none&gt;                                   443/TCP                        6d14h
core          mng-portal                      ClusterIP      172.17.17.246   &lt;none&gt;                                   80/TCP                         6d14h
core          nginx-ingress-controller-svc    NodePort       172.17.17.216   &lt;none&gt;                                   5443:5443/TCP,5444:5444/TCP    6d14h
core          suite-conf-svc-demo             ClusterIP      172.17.17.189   &lt;none&gt;                                   8080/TCP,8081/TCP              6d14h
core          suite-db-svc                    ClusterIP      172.17.17.83    &lt;none&gt;                                   5432/TCP                       6d14h
core          suite-installer-svc             ClusterIP      172.17.17.117   &lt;none&gt;                                   8080/TCP,8443/TCP              6d16h
default       kubernetes                      ClusterIP      172.17.17.1     &lt;none&gt;                                   443/TCP                        6d16h
demo-6zgp0    app-api-svc                     ClusterIP      172.17.17.95    &lt;none&gt;                                   8060/TCP                       6d14h
demo-6zgp0    app-ui-svc                      NodePort       172.17.17.2     &lt;none&gt;                                   82:20826/TCP                   6d14h
demo-6zgp0    autopass-lm-svc                 ClusterIP      172.17.17.231   &lt;none&gt;                                   5814/TCP                       6d14h
demo-6zgp0    idm                             ExternalName   &lt;none&gt;          idm.core.svc.cluster.local               &lt;none&gt;                         6d14h
demo-6zgp0    itom-pg-backup                  ClusterIP      172.17.17.52    &lt;none&gt;                                   8443/TCP                       6d14h
demo-6zgp0    postgres-svc                    ClusterIP      172.17.17.109   &lt;none&gt;                                   5432/TCP                       6d14h
kube-system   heapster                        ClusterIP      172.17.17.121   &lt;none&gt;                                   80/TCP                         6d14h
kube-system   kube-dns                        ClusterIP      172.17.17.78    &lt;none&gt;                                   53/UDP,53/TCP                  6d16h
</pre>
<p>There are services for ultimately externally-facing services, and services that will never be exposed.
</p><p>By default, none of the services are exposed externally. The default Kubernetes service type is ClusterIP which keeps the services externally invisible but internally fully visible.
</p><p>To allow external access for CDF services, an Ingress definition processed by an Ingress controller is needed or redefinition of a service to type nodePort.
</p><p>For example, to access the CDF Management Portal, an Ingress definition is created. Run: <code>kubectl describe ing mng-portal -n core</code>
</p>
<pre class="syntaxhighlighter-pre">Name:             mng-portal
Namespace:        core
Address:
Default backend:  default-http-backend:80 (&lt;none&gt;)
TLS:
  nginx-default-secret terminates t2vm036.hpeswlab.net
Rules:
  Host                  Path  Backends
  ----                  ----  --------
  t2vm036.hpeswlab.net
                        /mngPortal   mng-portal:80 (&lt;none&gt;)
                        /            mng-portal:80 (&lt;none&gt;)
Annotations:
  ingress.kubernetes.io/enable-cors:                 true
  ingress.kubernetes.io/rewrite-target:              /
  ingress.kubernetes.io/secure-backends:             true
  kubectl.kubernetes.io/last-applied-configuration:  {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{"ingress.kubernetes.io/enable-cors":"true","ingress.kubernetes.io/rewrite-target":"/","ingress.kubernetes.io/secure-backends":"true"},"name":"mng-portal","namespace":"core"},"spec":{"rules":[{"host":"t2vm036.hpeswlab.net","http":{"paths":[{"backend":{"serviceName":"mng-portal","servicePort":80},"path":"/mngPortal"},{"backend":{"serviceName":"mng-portal","servicePort":80},"path":"/"}]}}],"tls":[{"hosts":["t2vm036.hpeswlab.net"],"secretName":"nginx-default-secret"}]}}

Events:  &lt;none&gt;
</pre>
<ul><li>Both the / and /mngPortal URLs will redirect traffic to the mng-portal service port 80.</li>
<li>TLS termination happens at the Ingress layer.</li></ul>
<p>The mng-portal service is defined as follows. Run: <code>kubectl describe svc mng-portal -n core</code>
</p>
<pre class="syntaxhighlighter-pre">Name:              mng-portal
Namespace:         core
Labels:            app=mng-portal
Annotations:       kubectl.kubernetes.io/last-applied-configuration:
                     {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"mng-portal"},"name":"mng-portal","namespace":"core"},"sp...
Selector:          app=mng-portal
Type:              ClusterIP
IP:                172.17.17.246
Port:              &lt;unset&gt;  80/TCP
TargetPort:        9090/TCP
Endpoints:         172.16.63.19:9090
Session Affinity:  None
Events:            &lt;none&gt;
</pre>
<ul><li>The service itself is of type ClusterIP so only visible externally when explicitly described by an Ingress rule.</li>
<li>The cluster internal IP is 172.17.17.246.</li>
<li>The actual Management Portal service is running inside a Pod with IP address 172.63.190 on port 9090.</li></ul>
<p>Ingress for suites components is configured in the same way.
</p>
<div class="Admonition_Note">
<p><span class="autonumber">Note</span><br>
</p>
 CDF and  suites use Ingress rules and Ingress controller and also expose their functionality via nodePort-type services or a combination of both.</div>
<h5><span class="mw-headline" id="Egress">Egress</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-12" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-12" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>All containers can talk to the outside world. They are only limited by firewall and network rules at the host and host network(s) level. Kubernetes network policies are not used in CDF or suites.
</p>
<h3><span id="Name_resolution_(DNS)"></span><span class="mw-headline" id="Name_resolution_.28DNS.29">Name resolution (DNS)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-13" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-13" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<h5><span class="mw-headline" id="When_a_customer_DNS_resolution_is_available">When a customer DNS resolution is available</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-14" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-14" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>Inside the cluster, a container resolves a DNS names as follows:
</p>
<pre class="syntaxhighlighter-pre">"&lt;namespace&gt;.svc.cluster.local",
"svc.cluster.local",
"cluster.local",
"&lt;search in host /etc/resolv.conf&gt;"
</pre>
<p>So if a container running in Kubernetes namespace “suite1” wants to talk to service svc1, then it will try the following path:
</p>
<pre class="syntaxhighlighter-pre">svc1.suite1.svc.cluster.local",
svc1.svc.cluster.local",
svc1.cluster.local",
svc1.&lt;prefix(es)&gt; from search statement in host /etc/resolv.conf
</pre>
<h5><span class="mw-headline" id="When_a_customer_DNS_resolution_is_not_available">When a customer DNS resolution is not available</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-15" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-15" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>All of the internal Kubernetes resolution will be unaffected. Hostnames outside of the cluster (including the host nodes) will have to be statically specified.
</p><p>For this purpose, customers can specify a fixed hostname-to-IP list using a Kubernetes config map. See the Administration Guide for details.
</p>
<h3><span class="mw-headline" id="Ports">Ports</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-16" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-16" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Open port requirements for CDF are listed in <a href="./itom/ITOM_Platform:2019.02/PrepareServer" title="ITOM Platform:2019.02/PrepareServer">Prepare the server</a>.
</p>
<h2><span class="mw-headline" id="ITOM_Suites">ITOM Suites</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;veaction=edit&amp;section=T-17" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=ITOM_Container_Deployment_Foundation:2019.05/WhitepaperNetworking&amp;action=edit&amp;section=T-17" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Accessing Suite functionality can be via defined Ingress resources and its controller and/or directly via services exposed using nodePort or both.
</p><p>Suites may use:
</p>
<ul><li>“clusterIP” services exposed via Ingress resource and Ingress controller in the suite namespace</li>
<li>“nodePort” services available on a particular port on every cluster node</li>
<li>A hybrid of both approaches.</li></ul>
<p>Refer to the Suite installation/administration guides for details.
</p><p><a onclick="javascript:loadingImage(this);" class="image"><img alt="cdferwp201801 fig19.png" src="/mediawiki/images/thumb/5/56/cdferwp201801_fig19.png/900px-cdferwp201801_fig19.png" width="900" height="422" srcset="/mediawiki/images/5/56/cdferwp201801_fig19.png 1.5x" data-file-width="1269" data-file-height="595"></a>
</p><p>Figure - Cluster ingress
</p><p>Refer to the Suite installation/administration documentation for details on accessible end points.
</p>
<!-- 
NewPP limit report
Cached time: 20190715100231
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.044 seconds
Real time usage: 0.165 seconds
Preprocessor visited node count: 47/1000000
Preprocessor generated node count: 146/1000000
Post‐expand include size: 11899/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 7551/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key docops_wiki:pcache:idhash:795621-0!canonical and timestamp 20190715100231 and revision id 1398429
 -->
</div>