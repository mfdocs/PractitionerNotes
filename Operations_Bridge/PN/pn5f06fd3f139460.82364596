<html><h2>Introduction</h2>

<div>Scope of this document is to provide working steps to migrate an embedded Postgres on containerized OpsBridge Suite 2019.11 to an external Postgres.&nbsp;</div>

<div><br>
In a production environment, we recommend that you use an external PostgreSQL instance for the&nbsp;<code>default-db</code>&nbsp;database. To migrate an existing&nbsp;<code>default-db</code>&nbsp;database from the embedded PostgreSQL instance to an external PostgreSQL database, follow these steps<br>
<br>
The steps involved are:</div>

<ol>
	<li>Turn Off Suite Pods using cdfctl utility.</li>
	<li>Take database dump of all suite databases present in the internal postgres.</li>
	<li>Transport and restore all suite database dumps into the external postgres.</li>
	<li>Update the passwords in external postgres.</li>
	<li>Take backup of the configmaps.</li>
	<li>Update the configmaps to reflect host, port and certificate of external postgres.</li>
	<li>Turn On Suite Pods using cdfctl utility.</li>
</ol>

<div class="Admonition_Important">In the commands mentioned below the values enclosed in angular brackets (&lt;&gt;), you need to replace only those values with respect to your environment.</div>

<h2>Prepare</h2>

<p>Before you start the migration process ensure that you are performing the following tasks:</p>

<ol>
	<li>Install External Postgres Server 10.10 with TLS Enabled&nbsp;(same Postgres server version as embedded Postgres, consult your database administrator for TLS certificates)&nbsp;</li>
	<li>Note down the External Postgres Server Certificate in .crt and base64 of <code>server.crt</code> file if it is&nbsp;self signed or <code>root.crt</code>&nbsp;&nbsp;otherwise using the command:
	<pre><code class="language-bash">base64 server.crt | sed -e ':a;N;$!ba;s/\n//g'</code></pre>
	</li>
	<li>&nbsp;External Postgres server must be accessible to Master node. Use below command to check if database is accessible:
	<pre><code>psql -h &lt;db-server-fqdn&gt; -p &lt;db-server-port&gt; -U postgres -c "SHOW server_version"</code></pre>
	If database is configured in HA, then provide virtual host name.<span id="cke_bm_3754S" style="display: none;">&nbsp;</span></li>
	<li>
	<p>&nbsp;Note down the password of the following keys (covered under topic Take database dump of all the suite databases present in the internal Postgres)</p>
	</li>
</ol>

<ul>
	<li>
	<div class="Admonition_Important"><code>defaultdb_user_password</code></div>
	</li>
	<li>
	<div class="Admonition_Important"><code>defaultdb_cdfidm_user_password</code></div>
	</li>
</ul>

<h3>For External Postgres Server in HA</h3>

<div>
<p>For the Operations Bridge Suite to be able to connect to an external PostgreSQL clustered environment, the cluster must be configured in such a way that it is accessible through a single virtual host name. This can be done by using a middleware (for example, pgpool-II) that maps the virtual host name to the currently active database node. The virtual host name can then be used as the dbHost parameter. Cluster in this sense are at least two database instances that form a logical unit. Strong data consistency must be ensured.</p>

<p>It is important that any load balancing of queries is disabled in the middleware. All queries should always reach the active (a.k.a leader) database node at any given point in time.</p>

<p>In case the number of maximum parallel connections is configurable in the middleware it must be ensured that this value is set to a value that allows enough parallel connections up to the value set as max_connections (see "PostgreSQL parameter settings" in&nbsp;OpsBridge 2019.11 installation document for more details).</p>

<p>The following example gives an example of a possible pgpool-II configuration. Microfocus does not recommend nor prefer pgpool-II over other cluster solutions. Although the following configuration gives a possible configuration, additional parameters may be required, pgpool-II might change the definition or meaning of configuration parameters which may invalidate the example set of parameters. Please refer to the documentation of pgpool-II as the only source of information &nbsp;for sizing and configuring pgpoool-II.</p>

<table border="1" cellpadding="1" cellspacing="1">
	<thead>
		<tr>
			<th scope="col">Parameter name</th>
			<th scope="col">Small</th>
			<th scope="col">Medium</th>
			<th scope="col">Large</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>num_int_children</td>
			<td>400</td>
			<td>400</td>
			<td>400</td>
		</tr>
		<tr>
			<td>max_pool</td>
			<td>2</td>
			<td>2</td>
			<td>2</td>
		</tr>
		<tr>
			<td>reserved_connections</td>
			<td>2</td>
			<td>2</td>
			<td>2</td>
		</tr>
		<tr>
			<td>child_max_connections</td>
			<td>default value</td>
			<td>default value</td>
			<td>default value</td>
		</tr>
		<tr>
			<td>load_balance_mode</td>
			<td>false</td>
			<td>false</td>
			<td>false</td>
		</tr>
	</tbody>
</table>

<p>It is important to run pgpool-II in no other mode than "streaming replication mode".</p>

<p>Please ensure the following inequation applies when choosing values for max_pool and num_init_children:</p>

<div class="Admonition_Note">max_pool * num_init_children &nbsp;&lt;= max_connections</div>

<p>The above values for max_pool and num_init_children requires at least 2GB of process private memory.</p>

<p>Add the below entries at the end of the pg_hba.conf file</p>

<pre><code>host all all 0.0.0.0/0 md5
host all all ::/0 md5</code></pre>

<p>Edit postgresql.conf file for listen_addresses , port and lo_compat_privileges parameters to as shown below.</p>

<pre><code>listen_addresses = '*'
port = 5432
lo_compat_privileges = on</code></pre>

<p>Restart postgres service</p>

<pre><code>systemctl restart postgresql-10.service</code></pre>

<p>Check the status of all nodes using below command:</p>

<pre><code class="language-bash">psql -h &lt;db-server-node1&gt; -p &lt;db-server-port&gt; -U postgres -c 'show pool_nodes'</code></pre>

<p>The status of all the nodes must be "up".&nbsp;</p>
</div>

<h2>Turn off suite pods using cdfctl utility</h2>

<ol>
	<li>Log in to the master node, run the following command to turn off suite pods:<br>
	&nbsp;
	<pre><code class="language-bash">${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l DOWN -n &lt;suite-namespace&gt;</code></pre>
	<tt>Example:</tt>

	<pre><code class="language-bash">${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l DOWN -n opsbridge-azm6h
</code></pre>

	<div class="Admonition_Note">This will only shut off Pods of the suite, the Pods of CDF will remain operational (which includes the internal database pod)</div>
	</li>
</ol>

<h2>Take database dump of all the suite databases present in the internal Postgres</h2>

<p>Log on to internal database pod shell, note passwords and take database dumps.&nbsp;&nbsp;Use “<code>kubectl get pods -n core</code>” to note down internal database pod name.</p>

<pre><code class="language-bash">kubectl exec -it -c itom-postgresql-default -n core &lt;itom-postgresql-default-656d7bbb74-j7jrg&gt; /bin/bash  
su - postgres
get_secret defaultdb_user_password
get_secret defaultdb_cdfidm_user_password</code></pre>

<p>For Embedded postgres in HA, use the below commands to login to the pod</p>

<pre><code class="language-bash">kubectl exect -it -c cdf-pgnode1 -n core itom-postgresql-node1 /bin/bash
su - postgres
get_secret defaultdb_user_password
get_secret defaultdb_cdfidm_user_password</code></pre>

<ul>
	<li>Note down the passwords for <code>defaultdb_user_password</code> and <code>defaultdb_cdfidm_user_password</code>, you’ll need it later.</li>
	<li>You can copy and paste all below commands in a shell script and run it as well, from /tmp folder.</li>
</ul>

<h3>Take database Backup</h3>

<p>From the internal database pod shell, take database dumps of the following databases. Make sure to run the statements across the databases which are present as part of the capabilities installed. For example, if the customer has installed BVD capability alone, then they wont have databases related to OBM, which are "Event", "Mgmt" and "RTSM".</p>

<pre><code>mkdir -p /var/log/cdf/dbbackups
export PGPASSWORD=$(get_secret defaultdb_user_password | cut -d "=" -f2)
vacuumlo -v  -h default-postgresql-svc  -U  postgres -p 5432 rtsm
vacuumlo -v  -h default-postgresql-svc  -U  postgres -p 5432 event
pg_dump -Fc -h default-postgresql-svc -d autopassdb  -p 5432 -U postgres -v  -f /var/log/cdf/dbbackups/autopassdb.dump
pg_dump -Fc -h default-postgresql-svc -d bvd  -p 5432 -U postgres -v -f /var/log/cdf/dbbackups/bvd.dump
pg_dump -Fc -h default-postgresql-svc -d defaultdb  -p 5432 -U postgres -v  -f /var/log/cdf/dbbackups/defaultdb.dump
pg_dump -Fc -h default-postgresql-svc -d event  -p 5432 -U postgres -v  -f /var/log/cdf/dbbackups/event.dump
pg_dump -Fc -h default-postgresql-svc -d mgmt  -p 5432 -U postgres -v  -f /var/log/cdf/dbbackups/mgmt.dump
pg_dump -Fc -h default-postgresql-svc -d rtsm  -p 5432 -U postgres -v  -f /var/log/cdf/dbbackups/rtsm.dump
</code></pre>

<p>When done, exit the pod shell.</p>

<h2>Transport and restore all the suite database dumps into the external Postgres</h2>

<h3>Transfer the database dumps:</h3>

<p>To transfer the database dumps into your external Postgres server, execute the following command:</p>

<pre><code>scp -r /var/log/cdf-deployments/core/dbbackups/ &lt;postgres@server.example.net&gt;:/tmp/</code></pre>

<p>On Your External Postgres Server, log on to Postgres user, then to psql prompt:</p>

<pre><code>su -l postgres
psql -U postgres -d postgres</code></pre>

<p>Create <code>cdfidm</code> user with its password to the value you noted earlier for <code>defaultdb_cdfidm_user_password:</code></p>

<pre><code>CREATE USER cdfidm LOGIN PASSWORD '&lt;tXjwBkqyIQhShVBwuFs=&gt;';
GRANT cdfidm TO &nbsp;postgres;
\q</code></pre>

<h2>Restore the database dumps:</h2>

<p>Execute the following commands to restore the database dumps:</p>

<div class="Admonition_Note"><span style="color: rgb(51, 51, 51); font-family: Roboto, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px;">You can even copy the restoration commands in a shell script and run it as well, but make sure to change directory to /tmp/dbbackups ( cd /tmp/dbbackups). Also make sure that the postgres user has permissions to access the folder and contents.</span></div>

<pre><code>cd /tmp/dbbackups
pg_restore --create -p 5432 -d postgres -U postgres autopassdb.dump
pg_restore --create -p 5432 -d postgres -U postgres bvd.dump
pg_restore --create -p 5432 -d postgres -U postgres defaultdb.dump
pg_restore --create -p 5432 -d postgres -U postgres event.dump
pg_restore --create -p 5432 -d postgres -U postgres mgmt.dump
pg_restore --create -p 5432 -d postgres -U postgres rtsm.dump</code></pre>

<div class="Admonition_Note">All required databases will be created automatically as part of the restore procedure.</div>

<h2>Update the passwords in external Postgres</h2>

<p>Change the password of ‘postgres’ user to the value you noted earlier for <code>defaultdb_user_password</code></p>

<pre><code>psql -U postgres -d postgres
ALTER USER postgres LOGIN PASSWORD '&lt;00AuYXMMWL&gt;’;
\q</code></pre>

<h2>Take a backup of the configmaps</h2>

<p>Execute the following commands to take a backup of the ConfigMaps:</p>

<pre><code>kubectl get cm default-database-configmap -n core -o yaml &gt; /tmp/default-database-configmap.yaml
kubectl get cm obsuite-config -n &lt;suite-namespace&gt; -o yaml &gt; /tmp/obsuite-config.yaml
kubectl get cm bvd-config -n &lt;suite-namespace&gt; -o yaml &gt; /tmp/bvd-config.yaml
kubectl get cm omi-config -n &lt;suite-namespace&gt; -o yaml &gt; /tmp/omi-config.yaml
</code></pre>

<p><strong>Note:</strong><br>
<code>default-database-configmap</code> -&gt; responsible for <code>IDM</code><br>
<code>obsuite-config</code> -&gt; responsible for <code>autopass</code><br>
<code>bvd-config</code> -&gt; responsible for <code>bvd</code><br>
<code>omi-config</code> -&gt; responsible for <code>obm</code></p>

<h2>Update the ConfigMaps to reflect host, port, and certificate of external Postgres</h2>

<pre><code>kubectl edit cm default-database-configmap -n core
kubectl edit cm obsuite-config -n &lt;suite-namespace&gt;
kubectl edit cm bvd-config -n &lt;suite-namespace&gt;
kubectl edit cm omi-config -n &lt;suite-namespace&gt;</code></pre>

<p>Again, the presence of Config maps are purely dependent on the capabilities being installed as part of OpsBridge Suite.</p>

<div class="Admonition_Important">You can skip below section "Manual Execution" if you want to do this automatically using the script available <a href="https://marketplace.microfocus.com/itom/content/migrate-embedded-postgresql-to-external-postgresql">here</a></div>

<h3>Manual Execution</h3>

<div class="Admonition_Important">Do not change the indentation while editing.</div>

<ol>
	<li>Edit&nbsp;default-database-configmap and replace with values as specified below&nbsp;:
	<pre><code>kubectl edit cm default-database-configmap -n core

DEFAULT_DB_CA: &lt;base64 of .crt&nbsp;&gt;
DEFAULT_DB_HOST: &lt;FQDN_OF_EXT_POSTGRES_SERVER&gt;
DEFAULT_DB_PORT: &lt;POSTGRES_PORT_OF_EXT_SERVER&gt;
EMBEDDED_DB: &lt;false&gt;</code></pre>
	</li>
</ol>

<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="" border="0" height="358" hspace="0" src="/mediawiki/images/pn_images/pn_image_5ebe1dda89c661.84339105.png" vspace="0" width="800"></p>

<ol start="2">
	<li>Edit&nbsp;obsuite-config and replace with values as specified:
	<pre><code>kubectl edit cm obsuite-config -n &lt;suite-namespace&gt;

suite.autopass_dbCa: &lt;content of .crt&gt;
suite.autopass_dbhost: &lt;FQDN_OF_EXT_POSTGRES_SERVER&gt;
suite.autopass_dbport: &lt;POSTGRES_PORT_OF_EXT_SERVER&gt;</code></pre>
	</li>
</ol>

<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="" border="0" height="601" hspace="0" src="/mediawiki/images/pn_images/pn_image_5ebe1ddb39a665.10350970.png" vspace="0" width="726"></p>

<ol start="2">
	<li>Edit&nbsp;bvd-config&nbsp;and replace with values as specified:
	<pre><code>kubectl edit cm bvd-config -n &lt;suite-namespace&gt;</code></pre>
	</li>
	<li>Edit&nbsp;omi-config&nbsp;and replace with values as specified:
	<pre><code>kubectl edit cm omi-config -n &lt;suite-namespace&gt;

omi.dbCa.base64: &lt;base64 of .crt&gt;
omi.dbhost: &lt;FQDN_OF_EXT_POSTGRES_SERVER&gt;
omi.dbport: &lt;POSTGRES_PORT_OF_EXT_SERVER&gt;</code></pre>
	</li>
</ol>

<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="" border="0" height="243" hspace="0" src="/mediawiki/images/pn_images/pn_image_5ebe1ddbd76525.18210953.png" vspace="0" width="800"></p>

<p>Edit the deployments from the Primary Master</p>

<pre><code>export opsbNamespace="&lt;opsbridge-namespace&gt;"

kubectl get deployment bvd-controller-deployment -n ${opsbNamespace} -o json &gt; json_bvd-controller-deployment.json
jq -M '.spec.template.spec.containers[0].env += [{ "name": "BVD_DB_HOST", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbhost", "name": "bvd-config" } } }, { "name": "BVD_DB_PORT", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbport", "name": "bvd-config" } } }] | .spec.template.spec.initContainers[3].env += [{ "name": "BVD_DB_HOST", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbhost", "name": "bvd-config" } } }, { "name": "BVD_DB_PORT", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbport", "name": "bvd-config" } } }]' json_bvd-controller-deployment.json &gt; updated_json_bvd-controller-deployment.json
kubectl apply -f updated_json_bvd-controller-deployment.json
 
kubectl get deployment bvd-www-deployment -n ${opsbNamespace} -o json &gt; json_bvd-www-deployment.json
jq -M '.spec.template.spec.containers[0].env += [{ "name": "BVD_DB_HOST", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbhost", "name": "bvd-config" } } }, { "name": "BVD_DB_PORT", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbport", "name": "bvd-config" } } }]' json_bvd-www-deployment.json &gt; updated_json_bvd-www-deployment.json
kubectl apply -f updated_json_bvd-www-deployment.json
 
kubectl get deployment bvd-receiver-deployment -n ${opsbNamespace} -o json &gt; json_bvd-receiver-deployment.json
jq -M '.spec.template.spec.containers[0].env += [{ "name": "BVD_DB_HOST", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbhost", "name": "bvd-config" } } }, { "name": "BVD_DB_PORT", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbport", "name": "bvd-config" } } }]' json_bvd-receiver-deployment.json &gt; updated_json_bvd-receiver-deployment.json
kubectl apply -f updated_json_bvd-receiver-deployment.json

kubectl get deployment bvd-explore-deployment -n ${opsbNamespace} -o json &gt; json_bvd-explore-deployment.json
jq -M '.spec.template.spec.containers[0].env += [{ "name": "BVD_DB_HOST", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbhost", "name": "bvd-config" } } }, { "name": "BVD_DB_PORT", "valueFrom": { "configMapKeyRef": { "key": "bvd.dbport", "name": "bvd-config" } } }]' json_bvd-explore-deployment.json &gt; updated_json_bvd-explore-deployment.json
kubectl apply -f updated_json_bvd-explore-deployment.json
</code></pre>

<h2>Turn on the suite pods using cdfctl utility</h2>

<p>Execute these steps to turn on the suite pods:</p>

<ol>
	<li>Restart IDM Pods
	<pre><code>kubectl scale --replicas=0 deployment/idm -n core
kubectl scale --replicas=2 deployment/idm -n core</code></pre>
	</li>
	<li>Turn On Suite Pods
	<pre><code>${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l UP -n &lt;suite-namespace&gt;</code></pre>
	</li>
	<li>IDM Pods are not controlled by suite deployment, hence IDM pods need separate restart commands.</li>
	<li>Update ucmdb’s CA key to&nbsp;&nbsp;<code>omi_db_ca_cert</code>
	<pre><code>kubectl exec -it -c omi-artemis -n &lt;suite-namespace&gt; &lt;omi-artemis pod name&gt; /bin/bash
update_secret omi_db_ca_cert &lt;base64 of .crt&gt;&nbsp;
get_secret omi_db_ca_cert
exit</code></pre>
	</li>
</ol>

<p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="" border="0" height="253" hspace="0" src="/mediawiki/images/pn_images/pn_image_5ebe1ddc98a954.33881351.png" vspace="0" width="800"></p>

<ol start="5">
	<li>Restart ucmdb and omi Pods
	<pre><code>kubectl -n &lt;suite-namespace&gt; scale --replicas=0 deployment/ucmdb
kubectl -n &lt;suite-namespace&gt; scale --replicas=1 deployment/ucmdb
kubectl -n &lt;suite-namespace&gt; scale --replicas=0 Statefulset/omi
kubectl -n &lt;suite-namespace&gt; scale --replicas=1 Statefulset/omi</code></pre>
	</li>
</ol>

<div class="Admonition_Note"><strong>Note:</strong>&nbsp; <code>ucmdb</code> and <code>omi</code> pods will totally take 45 min to start (get to 2/2)</div>

<h2>Restoring internal database in case of failures</h2>

<p>If your migration fails for any reason and you want to roll back, follow the below mentioned steps.</p>

<ol>
	<li>Execute these commands to roll back to internal database:
	<pre><code>kubectl delete cm default-database-configmap -n core
kubectl apply -n core -f /tmp/default-database-configmap.yaml
kubectl delete cm obsuite-config -n &lt;suite-namespace&gt;&nbsp;
kubectl apply -n &lt;suite-namespace&gt; -f /tmp/obsuite-config.yaml
kubectl delete cm bvd-config -n &lt;suite-namespace&gt;
kubectl apply -n &lt;suite-namespace&gt; -f /tmp/bvd-config.yaml
kubectl delete cm omi-config -n &lt;suite-namespace&gt;
kubectl apply -n &lt;suite-namespace&gt; -f /tmp/omi-config.yaml
kubectl scale --replicas=0 deployment/idm -n core
kubectl scale --replicas=2 deployment/idm -n core
</code></pre>
	</li>
	<li>Restore <code>omi_db_ca_cert</code> value to the internal Postgres base64 CA value
	<pre><code>${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l DOWN -n &lt;suite-namespace&gt;
${K8S_HOME}/scripts/cdfctl.sh -f runlevel set -l UP -n &lt;suite-namespace&gt;</code></pre>
	</li>
</ol>

<h2>Deleting the internal database after success migration</h2>

<ol>
	<li>Note down internal database location on NFS folder using:
	<pre><code>kubectl exec -it&nbsp; &lt;postgres-pod-name&gt; -n core -c itom-postgresql-default -- bash -c "mount |grep &lt;NFS-Server-fqdn&gt;" | awk -F ' ' '{ print $1 }' | tr -d '\r'</code></pre>
	</li>
</ol>

<ol start="2">
	<li>You can delete the internal database once you have successfully migrated to external database by executing these commands:
	<pre><code>kubectl get deployment itom-postgresql-default -n core -o yaml &gt; /tmp/itom-postgresql-default.yaml
kubectl delete deployment itom-postgresql-default -n core</code></pre>
	</li>
	<li>Clean the NFS folder locations for the internal database.</li>
</ol>

<p><br>
&nbsp;</p>
</html>