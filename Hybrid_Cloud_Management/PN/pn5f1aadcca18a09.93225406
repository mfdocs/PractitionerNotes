<html>This document describes the process to upgrade an older version of HCM to HCM 2020.05. The validated older versions of HCM are 2018.11, 2019.02, and 2019.05. Typically the HCM upgrade process includes upgrading the HCM to each individual incremental of available version causing multiple hops, and cannot be upgraded directly to the latest available version. This would mean upgrading HCM 2018.11 to 2020.05 includes upgrading to 2019.02, 2019.05, 2019.08, 2019.11.<br>
<br>
The side-by-side upgrade (parallel instance-based) would allow you to upgrade directly to the HCM 2020.05, this process requires you to set up a new version of the HCM 2020.05 version to which the data, configurations, and customizations of older HCM version would be copied to and thereby bring up the new version of HCM in another instance. The old version of the HCM instance can be later decommissioned once the new version is LIVE.
<h2>Pre-requisites</h2>
The important requirement for side-by-side upgrade would be to have new servers and resources to bring up the new HCM version. It is recommended that the new hardware specifications should be the same as the older version of HCM.&nbsp;<br>
<br>
There can be a change in deployment architecture on the new HCM version, this would allow you to make an adjustment to the HCM environment. For example, if you were using a single master in the old version of HCM, you can deploy 2020.05 as a multi-master.<br>
<br>
You have to follow the HCM 2020.05 system requirements to ensure you have the supported hardware and OS requirements met.
<h3>NFS requirements</h3>
The new HCM version would need its own NFS repository different from the old HCM version, the old NFS can be decommissioned after the upgrade.

<h3>Database requirements</h3>
In this upgrade, a new CDF IDM database is required, you cannot reuse the existing CDF IDM database.&nbsp;The HCM databases from the older version can be reused. During the HCM 2020.05 installation, you should use a new database for the CDF IDM which you would continue to use and while configuring the HCM database you can use the Internal Postgresql non-HA or create temp databases since you will reconfigure to use older version HCM databases during the upgrade process.<br>
&nbsp;<br>
It is recommended to clone the HCM databases to use with the new HCM version. This way you could still run the older version of HCM until you have the upgrade process completed and validated.
<h3>Virtual IP and Load Balancer configurations</h3>
When the older version of HCM is using a Virtual IP (VIP), we cannot use the same VIP with new HCM version deployment, the new HCM 2020.05 installation will have to use a new VIP. You have to change the VIP of the HCM 2020.05 to use the existing VIP (from the older version) unless the DNS can be reconfigured to use the same FQDN for a new VIP.<br>
<br>
When the older version HCM is using an External Load Balancer, then you can deploy the new version of HCM using the temporary load balancer and then reconfigure the external hostname in HCM after the upgrade to use the actual Load Balancer. You must also change the load balancer configurations to use the new HCM instance URLs.<br>
<br>
This is further described later in this document.
<h2>Upgrade Procedure</h2>
The upgrade process assumes that the new HCM 2020.05 is deployed and is accessible. The databases are cloned and are available on a database server.

<h3>Expose the database port in older version HCM (Only in case of internal Postgresql)</h3>
You can skip this step if you are not using internal Postgresql for the CDF IDM database.<br>
If you are using the internal Postgresql database for the CDF IDM database on the older version of HCM, then you need to expose the Postgresql port on that server. This is required to migrate the IDM database to 2020.05. Follow the below steps on the older version of HCM.
<h4>Single node Postgres Service</h4>
Steps to expose ITOM Postgresql port outside of Kubernetes cluster

<ol>
	<li>Modify <code>pg_hba.conf</code> in <code>&lt;db-single-vol&gt;/baseinfra-1.0/postgresql95/</code> folder. Add below line to the file

	<pre><code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	</li>
	<li>Add below port mapping to the Postgresql yaml in the&nbsp;<code>&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</code>. Container section, under image add lines
	<pre><code class="language-yaml">image: localhost:5000/hpeswitomsandbox/itom-postgresql:9.5.11-0021
ports:
- containerPort: 5432
  hostPort: 5432</code></pre>
	</li>
	<li>Once YAML is modified execute below command for the changes to take effect.<br>
	<code>kubectl apply -f &lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</code></li>
</ol>

<h4>Postgres High Available</h4>
Steps to follow to expose ITOM Postgresql port outside of Kubernetes cluster

<ol>
	<li>Add the following line to both the <code>pg_hba.conf</code> files;

	<pre><code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	<db-node1-vol><code>&lt;db-node1-vol&gt;/baseinfra-1.0/postgresql95HA/node1data/pg_hba.conf</code><br>
	<db-node2-vol><code>&lt;db-node2-vol&gt;/baseinfra-1.0/postgresql95HA/node2data/pg_hba.conf </code></db-node2-vol></db-node1-vol></li>
	<li>Add <code>hostPort: 5432 </code>to the file, as shown below<br>
	<code>&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</code>
	<pre><code class="language-yaml">ports:
- containerPort: 5432
  hostPort: 5432
  name: pg-port
- containerPort: 5050
  name: pgrest-port</code></pre>
	</li>
	<li>Once yaml is modified execute below command for the changes to take effect.
	<pre>kubectl apply -f &lt;core-volume&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</pre>
	</li>
</ol>

<h3>Bring down the old version of HCM</h3>
The following are the criteria to bring down the older version of HCM, otherwise, you can skip this step.

<ol>
	<li>If you are planning to reuse the HCM databases with the new version.</li>
	<li>If you are going to make the final switch using the existing database with the new version.</li>
</ol>
To bring down the HCM, execute the following command on the older version of HCM

<pre>kubectl get ns
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;</pre>

<h3>Bring down HCM 2020.05</h3>
Before bringing down, note down the following secrets on the HCM 2020.05 to use it later.<br>
<code>kubectl exec -it &lt;csa-pod&gt; -n &lt;hcm-namespace&gt; -c hcm-csa -- get_secret HCM_IDM_SVC_PASSWORD<br>
<kbd><span style="color:#e67e22;">OUTPUT : PASS=qpfy_an63?LL.9i8</span></kbd></code><br>
<br>
Now you can bring down HCM using the following commands;
<pre>kubectl get ns
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;</pre>

<h3>Migrate IDM&nbsp;</h3>

<h4>Recreate IDM database</h4>
We need to recreate the IDM database in the new version of HCM installation since the IDM would have initialized with the default seeded user and credentials during the installation, we need to clear all data in IDM and migrate from older IDM database and then reinitialize with HCM seeded user data. On the HCM 2020.05 instance, follow the below steps.

<ol>
	<li>Stop IDM
	<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl delete -f idm.yaml</pre>
	</li>
	<li>If the database used to install HCM is Internal Postgresql, follow the below steps
	<ol>
		<li>Stop Internal Postgresql
		<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl delete -f itom-postgresql-single-svc.yaml</pre>
		</li>
		<li>Move the Postgresql&nbsp;folder to a backup location
		<pre><code class="language-vim">mkdir ~/backupPG
cd /baseinfra-1.0
mv postgresql106 ~/backupPG</code></pre>
		</li>
		<li>Start Internal Postgresql
		<hr>
		<pre><code class="language-vim">cd /suite-install/yamlContent/
kubectl create -f itom-postgresql-single-svc.yaml</code></pre>
		</li>
	</ol>
	</li>
	<li>If the database used to install HCM is External Oracle or Postgresql, then back up the database following the vendor documentation. Drop and create a new database based on the HCM document.</li>
	<li>Start IDM and open&nbsp;to CDF management portal
	<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl create -f idm.yaml</pre>
	<br>
	Open the&nbsp;CDF management portal to ensure you can access the portal, you might not be able to login using your existing admin password.</li>
</ol>

<h4>Reset the CDF admin password</h4>
To reset the CDF admin password execute the following commands<br>
&nbsp;
<pre>kubectl exec -it idm-7f44898bf6-h4q44 -n core -c idm -- bash
sh /idmtools/idm-installer-tools/idm.sh databaseUser resetPassword -org Provider -name "admin" -plainPwd "&lt;new-temp-password&gt;"
</pre>
<br>
Login into the CDF management console and reset the password correctly to the password which you had used while installing HCM.
<h4>Create idmTransportUser in IDM</h4>
Execute the following command and note down the password which has to be used in the subsequent command<br>
&nbsp;
<pre>kubectl exec -it &lt;idm-pod&gt; -n core -c idm -- get_secret idm_transport_admin_password
OUTPUT : PASS=CqeSFXzWvb3Wdw==

echo -n transport_admin:CqeSFXzWvb3Wdw== | base64
OUTPUT: dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=
</pre>
<br>
Replace the base64 encoded password in the below curl commands against the authorization header.
<pre>curl -k -X POST \
  https://arunp-hcm-02-m1.swinfra.net:5443/idm-service/v2.0/tokens \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -d '{
    "passwordCredentials": {
        "username": "admin",
        "password": "&lt;cdf-admin-password&gt;"
    },
    "tenantName": "Provider"
}'
</pre>
Extract the token from the above command and replace it against the x-auth-token in the below curl command.

<pre>curl -k -X POST \
  https://arunp-hcm-02-m1.swinfra.net:5443/idm-service/api/system/jsondata \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -H 'x-auth-token: eyJ0eXAiOiJKV1MiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIyYzkwY2RhZjcxMmZmOGJkMDE3MTJmZjkwNmY2MDEwMiIsImlzcyI6IklkTSAxLjI3LjAiLCJjb20uaHBlLmlkbTp0cnVzdG9yIjpudWxsLCJleHAiOjE1ODU2NTExMjAsImNvbS5ocC5jbG91ZDp0ZW5hbnQiOnsiaWQiOiIyYzkwY2RhZjcxMmZmOGJkMDE3MTJmZjkwMWUyMDBhZiIsIm5hbWUiOiJQcm92aWRlciIsImVuYWJsZWQiOnRydWV9LCJwcm4iOiJhZG1pbiIsImlhdCI6MTU4NTY0OTMyMCwianRpIjoiZTIyNWNjZjEtNWQyYi00NzBhLWI3N2ItODIzZjgzNzMxMDc4In0.jnFuSFzXicWl_pP_a6VQSLbRB5JC7KVsfzczw-HUgh0' \
  -d '[{
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "PROVIDER"
        },
        "attributes": {
            "name": "admin",
            "password": "&lt;cdf-admin_password&gt;",
            "type" : "INTERNAL_SEEDED_USER"
        }
    },
   {
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "IdMIntegration"
        },
        "attributes": {
            "name": "idmTransportUser",
              "password": "qpfy_an63?LL.9i8",
             "type" : "INTEGRATION_USER"
        }
    }]'</pre>
<span style="background-color:#f1c40f;">&nbsp;qpfy_an63?LL.9i8</span> is the password you have backed up before bringing down HCM 2020.05 under the topic “Bring down HCM 2020.05”

<h4>Download the IDM migration tool</h4>
Download the IDM Migration tool from the below path. And copy to a directory on one of the master nodes.<br>
<span style="background-color:#e74c3c;">&lt;TODO&gt; Provide tool down link.</span>

<h4>Copy Lib file to IDM migration tool directory</h4>
Go to the directory where you have copied the IDM migration tool.

<pre>mkdir ../lib
cp &lt;hcm-vol&gt;/shared/content-tools/csa/lib/CLI-lib.jar ../lib/.</pre>

<h4>Copy the src IDM encryption directory from the older version of HCM to 2020.05 migrate folder</h4>

<pre>mkdir hcm-idm-sec
cd hcm-idm-sec/
scp &lt;old-hcm-master-node&gt;:/opt/kubernetes/cfg/idm/security/*.* .</pre>
&lt;old-hcm-master-node&gt; is one of the master nodes from the old version of HCM

<h4>Migrate IDM data from the older version of HCM to 2020.05</h4>
Run the following command to generate the property files;<br>
<code>java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -g</code><br>
<br>
Update the config.properties according to the source and target environment. You can ignore property destEncryptionDirectory.<br>
Sample configure property would have the following entries;
<pre>jdbc.dest.username=cdfidm
srcDbType=postgres
destEncryptionDirectory=/opt/kubernetes/cfg/idm/security/
jdbc.src.username=cdfidm
jdbc.src.databaseUrl=jdbc\:postgresql\://&lt;oldversion-hcm-node&gt;\:5432/defaultdb
destDbType=postgres
jdbc.dest.password=lnwivCoci8QzgyUQPzM=
jdbc.src.password=eX3uAhiA05KIfax75B4=
srcEncryptionDirectory=/root/migrate/idm/hcm-idm-sec
jdbc.dest.databaseUrl=jdbc\:postgresql\://172.17.17.47\:5432/defaultdb
</pre>
<br>
Database password for internal Postgresql
<pre>kubectl exec -it &lt;postgresql-pod&gt; -n core -c itom-postgresql-default -- get_secret defaultdb_cdfidm_user_password
OUTPUT : PASS=eX3uAhiA05KIfax75B4=
</pre>
Destination Database URL – If you are using the internal Postgresql for CDF IDM, then IP address is the Postgresql service IP which you can get from below command;

<pre>kubectl get svc -n core | grep idm-postgresql-svc
OUTPUT: idm-postgresql-svc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ClusterIP &nbsp; 172.17.17.47 &nbsp; &lt;none&gt; &nbsp; &nbsp; &nbsp; &nbsp;5432/TCP,5050/TCP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 439d</pre>

<h4>Execute the migration tool</h4>
<code>java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -c config.properties</code><br>
<br>
Sample output
<pre><code class="language-bash">This tool will merge the data of the source database to the destination database.
It is recommended to take a backup of the data present in jdbc:postgresql://172.16.52.39:5432/defaultdb
Enter 'yes' if the backup of the destination database has been taken. Enter 'no' otherwise [yes/no]:
yes
Successfully initialized application
Start of database migrate ....
There may be a delay in the progress of the tool depending on the number of rows in the table and network latency. Please refer to the logs for the latest status.
INFO    [22/22] rows from ABSTRACT_GROUP merged in 163 ms .................... [ DONE ]
INFO    [40/40] rows from ABSTRACT_GROUP_METADATA merged in 77 ms ............ [ DONE ]
INFO    [22/22] rows from ABSTRACT_GROUP_REPRESENTATION merged in 91 ms ...... [ DONE ]
INFO    [4/4] rows from BASEAUTHCONFIGURATION merged in 30 ms ................ [ DONE ]
INFO    [12/12] rows from DATABASE_GROUP_REP merged in 23 ms ................. [ DONE ]
INFO    [3/3] rows from LDAP_CONFIGURATION merged in 73 ms ................... [ DONE ]
INFO    [10/10] rows from LDAP_GROUP_REP merged in 27 ms ..................... [ DONE ]
INFO    [264/264] rows from METADATA merged in 345 ms ........................ [ DONE ]
INFO    [6/6] rows from ORGANIZATIONS merged in 26 ms ........................ [ DONE ]
INFO    [49/49] rows from PERMISSION merged in 77 ms ......................... [ DONE ]
INFO    [117/117] rows from PERMISSION_ROLE merged in 87 ms .................. [ DONE ]
INFO    [50/50] rows from ROLES merged in 136 ms ............................. [ DONE ]
INFO    [62/62] rows from ROLES_GROUPS merged in 36 ms ....................... [ DONE ]
INFO    [1/1] rows from SAML_CONFIGURATION merged in 18 ms ................... [ DONE ]
The table SCHEMA_VERSION is not present in destination database. So ignoring it and hence the contents of the table will not be merged to destination database
INFO    [207/207] rows from TOKEN_STORE merged in 474 ms ..................... [ DONE ]
The migration of data has been successful
End of database migrate ....
</code></pre>

<h3><br>
Run IDM Config JOB</h3>

<pre>cd &lt;core-vol&gt;/suite-install/hcm/output
kubectl delete -f hcm-idm-config-data-job.yaml
kubectl create -f hcm-idm-config-data-job.yaml
kubectl logs hcm-idm-config-data-xj8l4 -n &lt;hcm-namespace&gt;</pre>

<p>Wait for the hcm-idm-config-data job to complete. Check the logs of the&nbsp;hcm-idm-config-data to ensure there is no error and the job is complete successfully.</p>

<h3>Create new secret for the HCM database password and use it in all YAML (Optional)</h3>
Follow this step if the password to the HCM databases you installed is different from the password on the database server you point to.

<pre>kubectl exec -it &lt;any-hcm-pod&gt; -n &lt;hcm-namespace&gt; -c &lt;container&gt; -- bash
update_secret NEW_DB_PASSWORD &lt;new-database-password&gt;</pre>

<h3>Migrate Autopass</h3>
Update <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-autopass.yaml </code>database configurations to point to the actual HCM database used with the old version. The properties that need to be updated are – DBHOST, DBPORT, DBNAME, DBUSER, DBPASSWORD_KEY, DBSYSTEMID.<br>
Delete and create Autopass pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-autopass.yaml
kubectl create -f hcm-autopass.yaml</code></pre>

<h3>Migrate CSA</h3>

<h4>Copy the CSA encryption folder from the old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,

<pre>cd &lt;hcm-vol&gt;/shared/
mv encryption/ encryption_2020_05
mkdir encryption
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/shared/encryption/*.* encryption/
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-csa.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties that need to be updated are – CSA_DB_HOST, CSA_DB_PORT, CSA_DB_USER, CSA_DB_PASSWORD_KEY, CSA_DB_NAME, CSA_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt>&nbsp;in the <code>hcm-csa.yaml</code><br>
<br>
Delete and create CSA pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-csa.yaml
kubectl create -f hcm-csa.yaml</code></pre>

<h3>Migrate OO</h3>

<h4>Copy the OO encryption folder from old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,

<pre>cd &lt;hcm-vol&gt;/conf/
mv oo oo_2020_05
mkdir oo
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oo/* oo/.
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-oo.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_CENTRAL_DB_HOST, OO_CENTRAL_DB_PORT, OO_CENTRAL_DB_USERNAME, OO_CENTRAL_DB_PASSWORD_KEY, OO_CENTRAL_DB_NAME, OO_CENTRAL_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt> in the&nbsp;<code>hcm-oo.yaml</code><br>
<br>
Delete and create OO pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-oo.yaml
kubectl create -f hcm-oo.yaml</code></pre>

<h3>Migrate OO Designer</h3>

<h4>Copy the OO Designer encryption folder from the old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,

<pre>cd &lt;hcm-vol&gt;/conf/
mv oodesigner oodesigner_2020_05
mkdir oodesigner
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oodesigner/* oodesigner/.
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the &lt;<code>core-vol&gt;/suite-install/hcm/output/hcm-oodesigner.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_DESIGNER_DB_HOST, OO_DESIGNER_DB_PORT, OO_DESIGNER_DB_USERNAME, OO_DESIGNER_DB_PASSWORD_KEY, OO_DESIGNER_DB_NAME, OO_DESIGNER_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt>&nbsp;in the <code>hcm-oodesigner.yaml</code><br>
<br>
Delete and create OO Designer pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-oodesigner.yaml
kubectl create -f hcm-oodesigner.yaml</code></pre>

<h3>Migrate UCMDB</h3>

<h4><span style="color: rgb(51, 51, 51); font-family: Roboto, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;;">Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-ucmdb.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – DB_HOST, DB_PORT, .</span></h4>
Change the DEPLOYMENT_TYPE to migrate for the hcm-ucmdb.yaml<br>
<br>
Delete and create UCMDB pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-ucmdb.yaml
kubectl create -f hcm-ucmdb.yaml</code></pre>

<h3>Upgrading Vertica components</h3>
Follow the process to upgrade Vertica if you are using any of the Cloud Service Brokering and Governance features

<ol>
	<li>Upgrade Vertica to the required version level as per the documented process<br>
	Upgrade to Vertica 9.2.0 version. For help, see&nbsp;<a href="https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/InstallationGuide/Upgrade/UpgradingVertica.htm?tocpath=Installing%20Vertica%7CUpgrading%20Vertica%7C_____0" rel="nofollow" target="1">Upgrading vertica</a>&nbsp;in the Vertica documentation.</li>
	<li>Copy Vertica certificate to the HCM 2020.05<br>
	The certificate&nbsp;has to be copied to <code>&lt;hcm-vol&gt;/certs/ca</code> folder</li>
	<li>Clean up the following in the di folder <code>&lt;hcm-vol&gt;/di</code>
	<pre><code class="language-vim">mv vertica_ingestion vertica_ingestion_old
mv administration &nbsp;administration_old</code></pre>
	</li>
	<li>Delete and create <code>hcm-zookeeper.yaml, hcm-kafka.yaml</code>, wait till they are up and running
	<pre><code class="language-vim">kubectl delete -f hcm-zookeeper.yaml
kubectl delete -f hcm-kafka.yaml

kubectl create -f hcm-zookeeper.yaml
kubectl create -f hcm-kafka.yaml</code></pre>
	</li>
	<li>Delete and create <code>hcm-coso-config-data-job.yaml</code>, wait till it's completed
	<pre><code class="language-vim">kubectl delete -f hcm-coso-config-data-job.yaml
kubectl create -f hcm-coso-config-data-job.yaml</code></pre>
	</li>
	<li>Connect to the Vertica database and execute the below query<br>
	<code>drop schema csac cascade;</code></li>
	<li>Update the following yaml with Vertica server details and database details
	<ol>
		<li>hcm-coso-data-ingestion.yaml</li>
		<li>hcm-coso-data-processor-worker.yaml</li>
		<li>hcm-csa-collector.yaml</li>
		<li>hcm-showback.yaml</li>
		<li>hcm-co-optimizer.yaml</li>
		<li>hcm-boost-cm.yaml</li>
	</ol>
	</li>
	<li>Apply the Vertica server changes<br>
	Use <code>kubectl delete -f</code> <filename> and <code>kubectl create -f</code> <filename> for each of the following files to recreate all these yamls to apply Vertica server changes
	<pre><code class="language-vim">kubectl delete -f hcm-coso-data-ingestion.yaml
kubectl delete -f hcm-coso-data-administration.yaml
kubectl delete -f hcm-coso-data-processor-job-submitter.yaml
kubectl delete -f hcm-coso-data-processor-master.yaml
kubectl delete -f hcm-coso-data-receiver.yaml
kubectl delete -f hcm-coso-data-processor-worker.yaml
kubectl delete -f hcm-boost-cm.yaml
kubectl delete -f hcm-csa-collector.yaml
kubectl delete -f hcm-showback.yaml

kubectl create -f hcm-coso-data-ingestion.yaml
kubectl create -f hcm-coso-data-administration.yaml
kubectl create -f hcm-coso-data-processor-job-submitter.yaml
kubectl create -f hcm-coso-data-processor-master.yaml
kubectl create -f hcm-coso-data-receiver.yaml
kubectl create -f hcm-coso-data-processor-worker.yaml
kubectl create -f hcm-boost-cm.yaml
kubectl create -f hcm-csa-collector.yaml
kubectl create -f hcm-showback.yaml</code></pre>
	</filename></filename></li>
	<li>Copy <code>&lt;hcm-vol&gt;/conf/co</code> from old version of HCM NFS to HCM 2020.05, then update Vertica details in <code>hcm-co-optimizer.yaml</code> and apply change
	<ol>
		<li>Once the Cloud Optimizer pod is running, update CSA End Point in the Cloud Optimizer UI to use the current CSA URL</li>
	</ol>
	</li>
	<li>Copy <code>&lt;hcm-vol&gt;/data/csb/elasticsearch</code> from the old version of HCM NFS to HCM 2020.05, (remove the existing folder if exists), then start the <code>hcm-elasticsearch.yaml</code></li>
</ol>

<h3>Update HCM URLs</h3>

<ol>
	<li>Update the OO URLs from old host to new fqdn in access_point_table in database machine</li>
	<li>Update Provider access point of OO, CO&nbsp;to new HCM FQDN in Provider UI</li>
	<li>In OO, update the CSA End Points in Content Management &gt; Configuration Items</li>
	<li>Update ooInboundUser password in OO central with HCM_IDM_PROVIDER_ACCOUNT_OOINBOUNDUSER_PASSWORD</li>
	<li>Run content job container - <code>hcm-content-job.yaml</code></li>
</ol>

<h3>Bring up all rest of the pods</h3>

<pre>kubectl get ns
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l UP -n &lt;hcm-namespace&gt;</pre>

<h3>Changing VIP</h3>
If you have installed the HCM 2020.05 using VIP which is different than the previous version of HCM and you want to reconfigure to use the same VIP as old HCM version then you have to change to the VIP on HCM 2020.05, please follow this document;<br>
<a href="https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange">https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange</a><br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname</a><br>
Once the changes are done based on these, the CDF would have been reconfigured to the new VIP.<br>
Now you need to re-configure the HCM yamls to use the new external hostname/VIP. Find and replace the external hostname with the old version HCM external hostname. Then apply the changes using kubectl delete -f and kubectl create -f for all the HCM yamls in <code>&lt;core-vol&gt;/suite-install/hcm/output</code>.<br>
<br>
Sample command to replace the hostname and apply changes;
<pre>sed -i 's/oldhcm-host.domain/newhcm-host.domain/g' *.yaml
ls -p | grep -v / | tr '\n' ','
kubectl delete -f &lt;comma separated files&gt;
kubectl create -f &lt;comma separated files&gt;</pre>
<br>
&nbsp;</html>