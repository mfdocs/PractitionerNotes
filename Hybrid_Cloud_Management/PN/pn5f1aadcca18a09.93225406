<html>This document describes the process to upgrade an older version of HCM to HCM 2020.05. The validated older versions of HCM are 2018.11, 2019.02, and 2019.05. Typically, the HCM upgrade process includes upgrading the HCM to each individual incremental of available version causing multiple hops, and cannot be upgraded directly to the latest available version. This would mean upgrading HCM 2018.11 to 2020.05 includes upgrading to 2019.02, 2019.05, 2019.08, 2019.11.<br>
<br>
The side-by-side upgrade (parallel instance-based) would allow you to upgrade directly to the HCM 2020.05, this process requires you to set up a new version of the HCM 2020.05 version to which the data, configurations, and customizations of older HCM version would be copied to and thereby bring up the new version of HCM in another instance. The old version of the HCM instance can be later decommissioned once the new version is LIVE.<br>
<br>
Since the older HCM version would still be running while you install the new version of HCM to upgrade, the downtime required with this upgrade approach is very less.
<h2>Plan</h2>
The important requirement for side-by-side upgrade would be to have new servers and resources to bring up the new HCM version. It is recommended that the new hardware specifications should be the same as the older version of HCM.&nbsp;<br>
<br>
There can be a change in deployment architecture on the new HCM version, this would allow you to make an adjustment to the HCM environment. For example, if you were using a single master in the old version of HCM, you can deploy 2020.05 as a multi-master.<br>
<br>
You have to follow the HCM 2020.05 system requirements to ensure you have the supported hardware and OS requirements met.
<h3>NFS requirements</h3>
The new HCM version would need its own NFS repository different from the old HCM version, the old NFS can be decommissioned after the upgrade.

<h3>Database requirements</h3>
In this upgrade, a new CDF IDM database is required, you cannot reuse the existing CDF IDM database.&nbsp;The HCM databases from the older version can be reused. During the HCM 2020.05 installation, you should have a new database for the CDF IDM which you would continue to use, and while configuring the HCM database you can use the Internal Postgresql non-HA or create temporary databases since you will reconfigure to use older version HCM databases during the upgrade process.<br>
&nbsp;<br>
It is recommended to clone the HCM databases to use with the new HCM version. This way you could still run the older version of HCM until you have the upgrade process completed and validated.
<h3>Virtual IP and Load Balancer configurations</h3>
When the older version of HCM is using a Virtual IP (VIP), we cannot use the same VIP with new HCM version deployment, the new HCM 2020.05 installation will have to use a new VIP temporarily. You have to change the VIP of the HCM 2020.05 to use the existing VIP (from the older version) unless the DNS can be reconfigured to use the same FQDN for a new VIP.<br>
<br>
When the older version HCM is using an External Load Balancer, then you can deploy the new version of HCM using the temporary load balancer and then reconfigure the external hostname in HCM after the upgrade to use the actual Load Balancer. You must also change the load balancer configurations to use the new HCM instance URLs.<br>
<br>
This is further described in this document in a later section.
<h2>Pre-requisites</h2>
The HCM 2020.05 is deployed and is accessible.&nbsp;The older version of HCM databases is available to configure with the HCM during the upgrade procedure.

<h2>Upgrade Procedure</h2>
The following tables give you an overview of the upgrade tasks and where they should be performed.

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<thead>
		<tr>
			<th scope="col">Task</th>
			<th scope="col">Where to perform</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><a href="#Expose the database port">Expose the database port</a></td>
			<td>On the master node of the older version of HCM</td>
		</tr>
		<tr>
			<td><a href="#Bring down the HCM">Bring down the HCM</a></td>
			<td>Older version of HCM; HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate IDM">Migrate IDM</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate Autopass">Migrate Autopass</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate CSA">Migrate CSA</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate OO">Migrate OO</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate OO Designer">Migrate OO Designer</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate UCMDB">Migrate UCMDB</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Upgrading Vertica components">Upgrade Vertica components</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Bring up all rest of the pods">Bringing up all the rest of HCM components</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Changing VIP">Changing VIP or Load Balancer</a></td>
			<td>HCM 2020.05</td>
		</tr>
	</tbody>
</table>
&nbsp;

<h3><a id="Expose the database port" name="Expose the database port"></a>Expose the database port in older version HCM (Only in case of internal Postgresql)</h3>

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<thead>
		<tr>
			<th scope="col">Role</th>
			<th scope="col">Location</th>
			<th scope="col">Privileges required</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>System administrator</td>
			<td>Master node on the older version of HCM</td>
			<td>Root or sudo</td>
		</tr>
	</tbody>
</table>
You can skip this step if you are not using internal Postgresql for the CDF IDM database.<br>
If you are using the internal Postgresql database for the CDF IDM database on the older version of HCM, then you need to expose the Postgresql port on that server. This is required to migrate the IDM database to 2020.05. Follow the below steps on the older version of HCM.
<h4>Single node Postgres Service</h4>
Steps to expose ITOM Postgresql port outside of Kubernetes cluster

<ol>
	<li>Modify <code>pg_hba.conf</code> in <code>&lt;db-single-vol&gt;/baseinfra-1.0/postgresql95/</code> folder. Add below line to the file

	<pre><code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	</li>
	<li>Add below port mapping to the Postgresql yaml in the&nbsp;<code>&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</code>. Container section, under image add lines
	<pre><code class="language-yaml">image: localhost:5000/hpeswitom/itom-postgresql:9.5.11-0021
ports:
- containerPort: 5432
  hostPort: 5432</code></pre>
	</li>
	<li>Once YAML is modified execute below command for the changes to take effect.<br>
	<code>kubectl apply -f &lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</code></li>
</ol>

<h4>Postgres High Available</h4>
Steps to follow to expose ITOM Postgresql port outside of Kubernetes cluster

<ol>
	<li>Add the following line to both the <code>pg_hba.conf</code> files;

	<pre><code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	<db-node1-vol><code>&lt;db-node1-vol&gt;/baseinfra-1.0/postgresql95HA/node1data/pg_hba.conf</code><br>
	<db-node2-vol><code>&lt;db-node2-vol&gt;/baseinfra-1.0/postgresql95HA/node2data/pg_hba.conf </code></db-node2-vol></db-node1-vol></li>
	<li>Add <code>hostPort: 5432 </code>to the file, as shown below<br>
	<code>&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</code>
	<pre><code class="language-yaml">ports:
- containerPort: 5432
  hostPort: 5432
  name: pg-port
- containerPort: 5050
  name: pgrest-port</code></pre>
	</li>
	<li>Once yaml is modified execute below command for the changes to take effect.
	<pre>kubectl apply -f &lt;core-volume&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</pre>
	</li>
</ol>

<h3><a id="Bring down the HCM" name="Bring down the HCM"></a>Bring down the old version of HCM</h3>
The following are the criteria to bring down the older version of HCM, otherwise, you can skip this step.

<ol>
	<li>If you are planning to reuse the HCM databases with the new version.</li>
	<li>If you are going to make the final switch using the existing database with the new version.</li>
</ol>
To bring down the HCM, execute the following command on the older version of HCM

<pre>kubectl get ns
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;</pre>

<h3>Bring down HCM 2020.05</h3>
Before bringing down, note down the following secrets on the HCM 2020.05 to use it later.<br>
<code>kubectl exec -it &lt;csa-pod&gt; -n &lt;hcm-namespace&gt; -c hcm-csa -- get_secret HCM_IDM_SVC_PASSWORD</code><br>
<samp>PASS=qpfy_an63?LL.9i8</samp><br>
<br>
Now you can bring down HCM using the following commands;
<pre>kubectl get ns
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;</pre>

<h3><a id="Migrate IDM" name="Migrate IDM"></a>Migrate IDM</h3>

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<thead>
		<tr>
			<th scope="col">Role</th>
			<th scope="col">Location</th>
			<th scope="col">Privileges required</th>
		</tr>
		<tr>
			<td>System administrator</td>
			<td>Master node on the HCM 2020.05</td>
			<td>Root or sudo</td>
		</tr>
	</thead>
</table>
We need to recreate the IDM database in the new version of HCM installation since the IDM would have initialized with the default seeded user and credentials during the installation, we need to clear all data in IDM, migrate from older IDM database and then reinitialize with HCM seeded user data. Follow all the below sections of steps on the HCM 2020.05 instance.

<h4>Recreate IDM database</h4>

<ol>
	<li>Stop IDM
	<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl delete -f idm.yaml</pre>
	</li>
	<li>If the database used to install HCM is Internal Postgresql, follow the below steps
	<ol>
		<li>Stop Internal Postgresql
		<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl delete -f itom-postgresql-single-svc.yaml</pre>
		</li>
		<li>Move the Postgresql&nbsp;folder to a backup location
		<pre><code class="language-vim">mkdir ~/backupPG
cd /baseinfra-1.0
mv postgresql106 ~/backupPG</code></pre>
		</li>
		<li>Start Internal Postgresql
		<hr>
		<pre><code class="language-vim">cd /suite-install/yamlContent/
kubectl create -f itom-postgresql-single-svc.yaml</code></pre>
		</li>
	</ol>
	</li>
	<li>If the database used to install HCM is External Oracle or Postgresql, then back up the database following the vendor documentation. Drop and create a new database based on the HCM document.</li>
	<li>Start IDM and open&nbsp;to CDF management portal
	<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl create -f idm.yaml</pre>
	<br>
	Open the&nbsp;CDF management portal to ensure you can access the portal, you might not be able to login using your existing admin password.</li>
</ol>

<h4>Reset the CDF admin password</h4>
To reset the CDF admin password execute the following commands&nbsp;

<pre>kubectl exec -it &lt;idm-pod&gt; -n core -c idm -- bash
sh /idmtools/idm-installer-tools/idm.sh databaseUser resetPassword -org Provider -name "admin" -plainPwd "&lt;new-temp-password&gt;"
</pre>
<br>
Login into the CDF management console and reset the password correctly to the password which you had used while installing HCM.
<h4>Create idmTransportUser in IDM</h4>
Execute the following command and note down the password which has to be used in the subsequent command&nbsp;

<pre>kubectl exec -it &lt;idm-pod&gt; -n core -c idm -- get_secret idm_transport_admin_password
PASS=CqeSFXzWvb3Wdw==

echo -n transport_admin:CqeSFXzWvb3Wdw== | base64
dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=
</pre>
<br>
Replace the base64 encoded password, &lt;external-fqdn&gt;, and &lt;cdf-admin-password&gt; in the below curl commands against the authorization header.
<pre>curl -k -X POST \
  https://&lt;external-fqdn&gt;:5443/idm-service/v2.0/tokens \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -d '{
    "passwordCredentials": {
        "username": "admin",
        "password": "&lt;cdf-admin-password&gt;"
    },
    "tenantName": "Provider"
}'
</pre>
Extract the token from the above command and replace it against the x-auth-token. Replace &lt;external-fqdn&gt; and &lt;cdf-admin-password&gt; in the below curl command.

<pre>curl -k -X POST \
  https://&lt;external-fqdn&gt;:5443/idm-service/api/system/jsondata \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -H 'x-auth-token: &lt;TOKEN&gt;' \
  -d '[{
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "PROVIDER"
        },
        "attributes": {
            "name": "admin",
            "password": "&lt;cdf-admin_password&gt;",
            "type" : "INTERNAL_SEEDED_USER"
        }
    },
   {
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "IdMIntegration"
        },
        "attributes": {
            "name": "idmTransportUser",
              "password": "qpfy_an63?LL.9i8",
             "type" : "INTEGRATION_USER"
        }
    }]'</pre>
&nbsp;

<div class="Admonition_Note"><span class="autonumber">Note&nbsp; &nbsp;</span><b><i>qpfy_an63?LL.9i8</i></b> is the password you have backed up before bringing down HCM 2020.05 under the topic "Bring down HCM 2020.05"</div>

<h4><br>
Download the IDM migration tool</h4>
Copy the IDM Migration tool from the below path and move it to a directory on one of the master nodes.

<pre>mkdir /root/migrate/idm
cd /root/migrate/idm
kubectl cp suite-conf-pod-hcm-5d79547896-gnnr2:/usr/share/tomcat/webapps/ROOT/tools/hcm-migration-tool.jar /root/migrate/idm/hcm-migration-tool.jar -n core -c suite-config
unzip hcm-migration-tool.jar BOOT-INF/lib/idm-data-migrater-tool-05.00.000-SNAPSHOT.jar
mv BOOT-INF/lib/idm-data-migrater-tool-05.00.000-SNAPSHOT.jar .</pre>

<h4>Copy Lib file to IDM migration tool directory</h4>
Go to the directory where you have copied the IDM migration tool.

<pre>mkdir ../lib
cp &lt;hcm-vol&gt;/shared/content-tools/csa/lib/CLI-lib.jar ../lib/.</pre>

<h4>Copy the src IDM encryption directory from the older version of HCM to 2020.05 migrate folder</h4>

<pre>mkdir hcm-idm-sec
cd hcm-idm-sec/
scp &lt;old-hcm-master-node&gt;:/opt/kubernetes/cfg/idm/security/*.* .</pre>
&lt;old-hcm-master-node&gt; is any one of the master nodes from the old version of HCM

<h4>Prepare the migration tool and configure the properties file</h4>
Run the following command to generate the property files;<br>
<code>java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -g</code><br>
<br>
Update the config.properties according to the source and target environment. You can ignore property <code>destEncryptionDirectory</code>.<br>
Sample configure property would have the following entries for an internal Postgresql;
<pre>jdbc.dest.username=cdfidm
srcDbType=postgres
destEncryptionDirectory=/opt/kubernetes/cfg/idm/security/
jdbc.src.username=cdfidm
jdbc.src.databaseUrl=jdbc\:postgresql\://&lt;oldversion-hcm-node&gt;\:5432/defaultdb
destDbType=postgres
jdbc.dest.password=lnwivCoci8QzgyUQPzM=
jdbc.src.password=eX3uAhiA05KIfax75B4=
srcEncryptionDirectory=/root/migrate/idm/hcm-idm-sec
jdbc.dest.databaseUrl=jdbc\:postgresql\://172.17.17.47\:5432/defaultdb
</pre>
<br>
Database password for internal Postgresql
<pre>kubectl exec -it &lt;postgresql-pod&gt; -n core -c itom-postgresql-default -- get_secret defaultdb_cdfidm_user_password
PASS=eX3uAhiA05KIfax75B4=
</pre>
Destination Database URL – If you are using the internal Postgresql for CDF IDM, then IP address is the Postgresql service IP which you can get from below command;

<pre>kubectl get svc -n core | grep idm-postgresql-svc
idm-postgresql-svc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ClusterIP &nbsp; 172.17.17.47 &nbsp; &lt;none&gt; &nbsp; &nbsp; &nbsp; &nbsp;5432/TCP,5050/TCP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 439d</pre>

<h4>Execute the migration tool to migrate data from the older version of HCM to 2020.05</h4>
<code>java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -c config.properties</code><br>
<br>
The command returns a result that resembles the following
<pre><code class="language-bash">This tool will merge the data of the source database to the destination database.
It is recommended to take a backup of the data present in jdbc:postgresql://172.16.52.39:5432/defaultdb
Enter 'yes' if the backup of the destination database has been taken. Enter 'no' otherwise [yes/no]:
yes
Successfully initialized application
Start of database migrate ....
There may be a delay in the progress of the tool depending on the number of rows in the table and network latency. Please refer to the logs for the latest status.
INFO    [22/22] rows from ABSTRACT_GROUP merged in 163 ms .................... [ DONE ]
INFO    [40/40] rows from ABSTRACT_GROUP_METADATA merged in 77 ms ............ [ DONE ]
INFO    [22/22] rows from ABSTRACT_GROUP_REPRESENTATION merged in 91 ms ...... [ DONE ]
INFO    [4/4] rows from BASEAUTHCONFIGURATION merged in 30 ms ................ [ DONE ]
INFO    [12/12] rows from DATABASE_GROUP_REP merged in 23 ms ................. [ DONE ]
INFO    [3/3] rows from LDAP_CONFIGURATION merged in 73 ms ................... [ DONE ]
INFO    [10/10] rows from LDAP_GROUP_REP merged in 27 ms ..................... [ DONE ]
INFO    [264/264] rows from METADATA merged in 345 ms ........................ [ DONE ]
INFO    [6/6] rows from ORGANIZATIONS merged in 26 ms ........................ [ DONE ]
INFO    [49/49] rows from PERMISSION merged in 77 ms ......................... [ DONE ]
INFO    [117/117] rows from PERMISSION_ROLE merged in 87 ms .................. [ DONE ]
INFO    [50/50] rows from ROLES merged in 136 ms ............................. [ DONE ]
INFO    [62/62] rows from ROLES_GROUPS merged in 36 ms ....................... [ DONE ]
INFO    [1/1] rows from SAML_CONFIGURATION merged in 18 ms ................... [ DONE ]
The table SCHEMA_VERSION is not present in destination database. So ignoring it and hence the contents of the table will not be merged to destination database
INFO    [207/207] rows from TOKEN_STORE merged in 474 ms ..................... [ DONE ]
The migration of data has been successful
End of database migrate ....
</code></pre>

<h3></h3>

<p>Now the IDM migration is complete.</p>

<h3>Run IDM Config JOB</h3>

<pre>cd &lt;core-vol&gt;/suite-install/hcm/output
kubectl delete -f hcm-idm-config-data-job.yaml
kubectl create -f hcm-idm-config-data-job.yaml
kubectl logs hcm-idm-config-data-xj8l4 -n &lt;hcm-namespace&gt;</pre>

<p>Wait for the hcm-idm-config-data job to complete. Check the logs of the hcm-idm-config-data to ensure there is no error and the job is complete successfully.</p>

<h3>Create new secret for the HCM database password and use it in all YAML (Optional)</h3>
Follow this step if the password to the HCM databases you installed is different from the password on the database server you point to.

<pre>kubectl exec -it &lt;any-hcm-pod&gt; -n &lt;hcm-namespace&gt; -c &lt;container&gt; -- bash
update_secret NEW_DB_PASSWORD &lt;new-database-password&gt;</pre>

<h3><a id="Migrate Autopass" name="Migrate Autopass"></a>Migrate Autopass</h3>
Update <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-autopass.yaml </code>database configurations to point to the actual HCM database used with the old version. The properties that need to be updated are – DBHOST, DBPORT, DBNAME, DBUSER, DBPASSWORD_KEY, DBSYSTEMID.<br>
Delete and create Autopass pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-autopass.yaml
kubectl create -f hcm-autopass.yaml</code></pre>

<h3><a id="Migrate CSA" name="Migrate CSA"></a>Migrate CSA</h3>

<h4>Copy the CSA encryption folder from the old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,

<pre>cd &lt;hcm-vol&gt;/shared/
mv encryption/ encryption_2020_05
mkdir encryption
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/shared/encryption/*.* encryption/
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-csa.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties that need to be updated are – CSA_DB_HOST, CSA_DB_PORT, CSA_DB_USER, CSA_DB_PASSWORD_KEY, CSA_DB_NAME, CSA_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt>&nbsp;in the <code>hcm-csa.yaml</code><br>
<br>
Delete and create CSA pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-csa.yaml
kubectl create -f hcm-csa.yaml</code></pre>

<h3><a id="Migrate OO" name="Migrate OO"></a>Migrate OO</h3>

<h4>Copy the OO encryption folder from old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,

<pre>cd &lt;hcm-vol&gt;/conf/
mv oo oo_2020_05
mkdir oo
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oo/* oo/.
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-oo.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_CENTRAL_DB_HOST, OO_CENTRAL_DB_PORT, OO_CENTRAL_DB_USERNAME, OO_CENTRAL_DB_PASSWORD_KEY, OO_CENTRAL_DB_NAME, OO_CENTRAL_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt> in the&nbsp;<code>hcm-oo.yaml</code><br>
<br>
Delete and create OO pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-oo.yaml
kubectl create -f hcm-oo.yaml</code></pre>

<h3><a id="Migrate OO Designer" name="Migrate OO Designer"></a>Migrate OO Designer</h3>

<h4>Copy the OO Designer encryption folder from the old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,

<pre>cd &lt;hcm-vol&gt;/conf/
mv oodesigner oodesigner_2020_05
mkdir oodesigner
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oodesigner/* oodesigner/.
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the &lt;<code>core-vol&gt;/suite-install/hcm/output/hcm-oodesigner.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_DESIGNER_DB_HOST, OO_DESIGNER_DB_PORT, OO_DESIGNER_DB_USERNAME, OO_DESIGNER_DB_PASSWORD_KEY, OO_DESIGNER_DB_NAME, OO_DESIGNER_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt>&nbsp;in the <code>hcm-oodesigner.yaml</code><br>
<br>
Delete and create OO Designer pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-oodesigner.yaml
kubectl create -f hcm-oodesigner.yaml</code></pre>

<h3><a id="Migrate UCMDB" name="Migrate UCMDB"></a>Migrate UCMDB</h3>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-ucmdb.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – DB_HOST, DB_PORT, DB_USER, DB_SCHEMA, DB_PASSWORD_VAULT_KEY.<br>
Delete and create UCMDB pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-ucmdb.yaml
kubectl create -f hcm-ucmdb.yaml</code></pre>

<h3><a id="Upgrading Vertica components" name="Upgrading Vertica components"></a>Upgrading Vertica components</h3>
Ensure you have the older version of HCM stopped and its not accessing the vertica database. Follow the process to upgrade Vertica if you are using any of the Cloud Service Brokering and Governance features.

<ol>
	<li>Upgrade Vertica to the required version level as per the documented process.<br>
	Upgrade to Vertica 9.2.0-7&nbsp;version. For help, see&nbsp;<a href="https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/InstallationGuide/Upgrade/UpgradingVertica.htm?tocpath=Installing%20Vertica%7CUpgrading%20Vertica%7C_____0" rel="nofollow" target="1">Upgrading Vertica</a>&nbsp;in the Vertica documentation.</li>
	<li>Copy Vertica certificate to the HCM 2020.05<br>
	The certificate&nbsp;has to be copied to <code>&lt;hcm-vol&gt;/certs/ca</code> folder</li>
	<li>Clean up the following in the di folder <code>&lt;hcm-vol&gt;/di</code>
	<pre><code class="language-vim">mv vertica_ingestion vertica_ingestion_old
mv administration &nbsp;administration_old</code></pre>
	</li>
	<li>Delete and create <code>hcm-zookeeper.yaml, hcm-kafka.yaml</code>, wait till they are up and running
	<pre><code class="language-vim">kubectl delete -f hcm-zookeeper.yaml
kubectl delete -f hcm-kafka.yaml

kubectl create -f hcm-zookeeper.yaml
kubectl create -f hcm-kafka.yaml</code></pre>
	</li>
	<li>Delete and create <code>hcm-coso-config-data-job.yaml</code>, wait till it's completed
	<pre><code class="language-vim">kubectl delete -f hcm-coso-config-data-job.yaml
kubectl create -f hcm-coso-config-data-job.yaml</code></pre>
	</li>
	<li>Connect to the Vertica database and execute the below query<br>
	<code>drop schema csac cascade;</code></li>
	<li>Update the following yaml with Vertica server details and database details
	<ol>
		<li>hcm-coso-data-ingestion.yaml</li>
		<li>hcm-coso-data-processor-worker.yaml</li>
		<li>hcm-csa-collector.yaml</li>
		<li>hcm-showback.yaml</li>
		<li>hcm-co-optimizer.yaml</li>
		<li>hcm-boost-cm.yaml</li>
	</ol>
	</li>
	<li>Apply the Vertica server changes<br>
	Use <code>kubectl delete -f &lt;filename&gt;</code> and <code>kubectl create -f &lt;filename&gt;</code> for each of the following files. This would recreate all these yamls and apply Vertica server changes
	<pre><code class="language-vim">kubectl delete -f hcm-coso-data-ingestion.yaml
kubectl delete -f hcm-coso-data-administration.yaml
kubectl delete -f hcm-coso-data-processor-job-submitter.yaml
kubectl delete -f hcm-coso-data-processor-master.yaml
kubectl delete -f hcm-coso-data-receiver.yaml
kubectl delete -f hcm-coso-data-processor-worker.yaml
kubectl delete -f hcm-boost-cm.yaml
kubectl delete -f hcm-csa-collector.yaml
kubectl delete -f hcm-showback.yaml

kubectl create -f hcm-coso-data-ingestion.yaml
kubectl create -f hcm-coso-data-administration.yaml
kubectl create -f hcm-coso-data-processor-job-submitter.yaml
kubectl create -f hcm-coso-data-processor-master.yaml
kubectl create -f hcm-coso-data-receiver.yaml
kubectl create -f hcm-coso-data-processor-worker.yaml
kubectl create -f hcm-boost-cm.yaml
kubectl create -f hcm-csa-collector.yaml
kubectl create -f hcm-showback.yaml</code></pre>
	Wait till all the pods are up and running before you go to the next step.</li>
	<li>Copy <code>&lt;hcm-vol&gt;/conf/co</code> from the old version of HCM NFS to HCM 2020.05, then update Vertica details in <code>hcm-co-optimizer.yaml</code> and apply change
	<ol>
		<li>Once the Cloud Optimizer pod is running, update CSA End Point in the Cloud Optimizer UI to use the current CSA URL</li>
	</ol>
	</li>
	<li>Copy <code>&lt;hcm-vol&gt;/data/csb/elasticsearch</code> from the old version of HCM NFS to HCM 2020.05, (remove the existing folder if exists), then start the <code>hcm-elasticsearch.yaml</code></li>
</ol>

<h3>Update HCM URLs</h3>

<ol>
	<li>Update the OO URLs from old host to new HCM endpoint in <code>access_point_table</code> in CSA database</li>
	<li>Update the access point of OO, CO providers to new HCM endpoint in CSA Provider UI</li>
	<li>In OO, update the CSA End Points in Content Management &gt; Configuration Items</li>
	<li>Update ooInboundUser password in OO central with HCM_IDM_PROVIDER_ACCOUNT_OOINBOUNDUSER_PASSWORD. Use the below command to get the password<br>
	<code>kubectl exec -it &lt;hcm-oo-pod&gt; -n &lt;hcm-namespace&gt; -c hcm-oo -- get_secret HCM_IDM_PROVIDER_ACCOUNT_OOINBOUNDUSER_PASSWORD</code></li>
	<li>Run content job container to get the latest OOTB content - <code>hcm-content-job.yaml</code></li>
</ol>

<h3><a id="Bring up all rest of the pods" name="Bring up all rest of the pods"></a>Bring up all the rest of HCM components</h3>

<pre>kubectl get ns
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l UP -n &lt;hcm-namespace&gt;</pre>

<h3><a id="Changing VIP" name="Changing VIP"></a>Changing VIP or Load Balancer</h3>
If you have installed the HCM 2020.05 using VIP which is different than the previous version of HCM and you want to reconfigure to use the same VIP as the older HCM version, then you have to change the VIP on HCM 2020.05. To change the VIP follow the steps in the below documents.<br>
<a href="https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange">https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange</a><br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname</a><br>
<br>
If you have installed the HCM 2020.05 using a&nbsp;temporary load balancer and now want to reconfigure it to the actual External Load Balancer, then follow the steps in the below document.<br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname</a><br>
<br>
Once the changes are done following the documents the CDF would have been reconfigured to the new VIP or Load Balancer.<br>
Now you need to re-configure the HCM yamls to use the new external Load Balancer/VIP. Find and replace the existing external hostname in the yamls with the actual external hostname. Then apply the changes using kubectl delete -f and kubectl create -f for all the HCM yamls in <code>&lt;core-vol&gt;/suite-install/hcm/output</code>.<br>
<br>
Sample command to replace the hostname and apply changes. Backup the files before changing them.
<pre>cd &lt;core-vol&gt;/suite-install/hcm/output
sed -i 's/oldhcm-host.domain/newhcm-host.domain/g' *.yaml
ls -p *.yaml | grep -v / | tr '\n' ','
kubectl delete -f &lt;comma separated files&gt;
kubectl create -f &lt;comma separated files&gt;</pre>
<br>
The HCM side-by-side upgrade to 2020.05 is now complete. You can decommission the older version of HCM.</html>