<html>This document describes the process which can be followed to upgrade an older version HCM to 2020.05. The validated older versions of HCM are 2018.11, 2019.02, and 2019.05. Usually, the upgrade process includes upgrading the HCM to each individual incremental of available version, and cannot be upgraded directly to the latest available version. This would mean upgrading HCM 2018.11 to 2020.05 includes upgrading to 2019.02, 2019.05, 2019.08, 2019.11.<br>
<br>
The side-by-side upgrade (parallel instance-based) includes setting up a new version of the HCM 2020.05 version to which the data, configurations, and customizations would be copied to and thereby bring up the new version of HCM in another instance. The old version of HCM can be later decommissioned once the new version is LIVE.
<h2>Pre-requisites</h2>
The important requirement for side-by-side upgrade would be to have new servers, resources for bringing up the new HCM version. It is recommended that the resources and configurations should be the same as the older version of HCM.&nbsp;<br>
<br>
There can be a change in deployment architecture on the new HCM version, this would allow to make an adjustment to the HCM environment. For example, if you were using single master in the old version of HCM, you can deploy 2020.05 as multi-master.
<h3>NFS requirements</h3>
The new HCM instance would need its own NFS repository different from the old HCM version, the old NFS can be decommissioned after the upgrade.

<h3>Database requirements</h3>
In this upgrade, all the HCM databases of the older version are reused, the CDF IDM database is created new. So while installing the new version of HCM, you should create a new database for the CDF IDM which you would continue to use. For the HCM databases, you can use the internal Postgres or create temp databases since you will reconfigure the HCM to point to HCM databases of the older version during the upgrade process.&nbsp;<br>
It is recommended to clone the HCM databases to use with the new HCM version. This way you could still run the older version of HCM until you have the upgrade process completed and validated.
<h3>Virtual IP and Load Balancer configurations</h3>
When the older version of HCM is using a Virtual IP (VIP), we cannot use the same VIP with new HCM version deployment, the new HCM 2020.05 installation will use a new VIP. You have to change the VIP of the HCM 2020.05 to use the existing VIP (from the older version) unless the DNS can be reconfigured to use the same FQDN for new VIP, this is further described in later part of this document.<br>
<br>
When the older version HCM is using an external Load Balancer, then you can deploy the new version of HCM using the temporary load balancer and then reconfigure the external hostname in HCM after the upgrade to use the old Load Balancer. You must also change the load balancer configurations to use the new HCM instances.
<h2>Upgrade Procedure</h2>
The upgrade process assumes that the new HCM 2020.05 is deployed and is accessible. The databases are cloned and is available on to a database server.

<h3>Expose the database port in older version HCM (Only in case of internal Postgres)</h3>
You can skip this step if you are not using internal Postgres for the CDF IDM database.<br>
If you are using the internal Postgres database for the CDF IDM database, then you need to expose the Postgres port on that server. This is required to migrate the IDM database to 2020.05.
<h4>Single node Postgres Service</h4>
Steps to expose ITOM Postgres port outside of kubernetes cluster

<ol>
	<li>Modifiy pg_hba.conf in &lt;db-single-vol&gt;/baseinfra-1.0/postgresql95/ folder. Add below line to the file
	<pre><code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	</li>
	<li>Add below port mapping to the postgres yaml (&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml). Container section, under image add lines<br>
	&nbsp; &nbsp; &nbsp; &nbsp;
	<pre><code class="language-yaml">image: localhost:5000/hpeswitomsandbox/itom-postgresql:9.5.11-0021
ports:
- containerPort: 5432
  hostPort: 5432 &nbsp;</code></pre>
	</li>
	<li>Once YAML is modified execute below command for the changes to take effect.&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; kubectl apply -f &lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</li>
</ol>

<h4>Postgres High Available</h4>
Steps to follow to expose ITOM Postgres port outside of Kubernetes cluster

<ol>
	<li>Add the following line to both the pg_hba.conf files;<br>
	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;host &nbsp; &nbsp;all &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; all &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0/0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; md5<br>
	<db-node1-vol>/baseinfra-1.0/postgresql95HA/node1data/pg_hba.conf<br>
	<db-node2-vol>/baseinfra-1.0/postgresql95HA/node2data/pg_hba.conf </db-node2-vol></db-node1-vol></li>
	<li>Add hostPort: 5432 to the file, as shown below &lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml<br>
	ports:<br>
	&nbsp; &nbsp; &nbsp; &nbsp; - containerPort: 5432<br>
	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hostPort: 5432<br>
	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; name: pg-port<br>
	&nbsp; &nbsp; &nbsp; &nbsp; - containerPort: 5050<br>
	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; name: pgrest-port</li>
	<li>Once yaml is modified execute below command for the changes to take effect.<br>
	kubectl apply -f &nbsp;&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</li>
</ol>

<h3>Bring down old version of HCM</h3>
You need to bring down the old version of HCM only if you are reusing the HCM databases with the new version. You can skip this step if you have cloned the database.<br>
Additionally, you have to bring down the HCM if you are going to make the finally switch to the new HCM version. This is to prevent any additional artifacts (subscriptions, designs etc.) being created in the older version of HCM.<br>
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;
<h3>Bring down HCM 2020.05</h3>
Before bringing down, note down the following secrets on the HCM 2020.05 to use it later.<br>
kubectl exec -it &lt;csa-pod&gt; -n &lt;hcm-namespace&gt; -c hcm-csa -- get_secret HCM_IDM_SVC_PASSWORD<br>
<span style="color:#e67e22;">OUTPUT : PASS=qpfy_an63?LL.9i8</span><br>
Now you can bring down HCM using the following commands;<br>
kubectl get ns<br>
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;
<h3>Migrate IDM&nbsp;</h3>

<h4>Recreate IDM database</h4>
We need to recreate the IDM database in the new version of HCM installation since the IDM would have initialized with the default seeded user and credentials, we need to clear all data in IDM and migrate from older IDM database and then reinitialize with HCM seeded user data. Login to HCM 2020.05 machine

<ol>
	<li>Stop IDM<br>
	cd &lt;core-vol&gt;/suite-install/yamlContent/<br>
	kubectl delete -f idm.yaml</li>
	<li>If the database used to install HCM is Internal Postgres, follow the below steps
	<ol>
		<li>Stop Internal Postgres<br>
		cd &lt;core-vol&gt;/suite-install/yamlContent/<br>
		kubectl delete -f itom-postgresql-single-svc.yaml</li>
		<li>Move the postgres folder to a backup location<br>
		mkdir ~/backupPG<br>
		cd <db-single-vol>/baseinfra-1.0<br>
		mv postgresql106 ~/backupPG</db-single-vol></li>
		<li>Start Internal Postgres<br>
		cd <core-vol>/suite-install/yamlContent/<br>
		kubectl create -f itom-postgresql-single-svc.yaml</core-vol></li>
	</ol>
	</li>
	<li>If the database used to install HCM is External Oracle or Postgresql, then back up the database following the vendor documentation. Drop and create a new database based on the HCM document.</li>
	<li>Start IDM and Login to CDF management portal<br>
	cd &lt;core-vol&gt;/suite-install/yamlContent/<br>
	kubectl create -f idm.yaml<br>
	Login to CDF management portal to ensure you can access the portal</li>
</ol>

<h4>Reset the IDM password</h4>
kubectl exec -it idm-7f44898bf6-h4q44 -n core -c idm -- bash<br>
sh /idmtools/idm-installer-tools/idm.sh databaseUser resetPassword -org Provider -name "admin" -plainPwd "&lt;new-temp-password&gt;"<br>
Login into management console and reset password correctly to the password which you had used while installation HCM.
<h4>Create idmTransportUser in IDM</h4>
kubectl exec -it &lt;idm-pod&gt; -n core -c idm -- get_secret idm_transport_admin_password<br>
<span style="color:#e67e22;">OUTPUT : PASS=CqeSFXzWvb3Wdw==</span><br>
echo -n transport_admin:CqeSFXzWvb3Wdw== | base64<br>
<span style="color:#e67e22;">OUTPUT: dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=</span><br>
Replace the base64 encoded password in the below curl commands against the authorization header.
<pre><code class="language-vim">curl -k -X POST \
  https://arunp-hcm-02-m1.swinfra.net:5443/idm-service/v2.0/tokens \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -d '{
    "passwordCredentials": {
        "username": "admin",
        "password": "&lt;hcm-password&gt;"
    },
    "tenantName": "Provider"
}'
</code></pre>
Extract the token from the above command and replace it against the x-auth-token in the below curl command.

<pre><code>curl -k -X POST \
  https://arunp-hcm-02-m1.swinfra.net:5443/idm-service/api/system/jsondata \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -H 'x-auth-token: eyJ0eXAiOiJKV1MiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIyYzkwY2RhZjcxMmZmOGJkMDE3MTJmZjkwNmY2MDEwMiIsImlzcyI6IklkTSAxLjI3LjAiLCJjb20uaHBlLmlkbTp0cnVzdG9yIjpudWxsLCJleHAiOjE1ODU2NTExMjAsImNvbS5ocC5jbG91ZDp0ZW5hbnQiOnsiaWQiOiIyYzkwY2RhZjcxMmZmOGJkMDE3MTJmZjkwMWUyMDBhZiIsIm5hbWUiOiJQcm92aWRlciIsImVuYWJsZWQiOnRydWV9LCJwcm4iOiJhZG1pbiIsImlhdCI6MTU4NTY0OTMyMCwianRpIjoiZTIyNWNjZjEtNWQyYi00NzBhLWI3N2ItODIzZjgzNzMxMDc4In0.jnFuSFzXicWl_pP_a6VQSLbRB5JC7KVsfzczw-HUgh0' \
  -d '[{
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "PROVIDER"
        },
        "attributes": {
            "name": "admin",
            "password": "&lt;hcm_password&gt;",
            "type" : "INTERNAL_SEEDED_USER"
        }
    },
   {
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "IdMIntegration"
        },
        "attributes": {
            "name": "idmTransportUser",
              "password": "qpfy_an63?LL.9i8",
             "type" : "INTEGRATION_USER"
        }
    }]'</code></pre>
<span style="background-color:#f1c40f;">&nbsp;qpfy_an63?LL.9i8</span> is the password you have backed up before bringing down HCM 2020.05 under topic “Bring down HCM 2020.05”

<h4>Download the IDM migration tool</h4>
Download the IDM Migration tool from the below path. And copy to a directory on one of the master nodes.<br>
<span style="background-color:#e74c3c;">&lt;TODO&gt; Provide tool down link.</span>

<h4>Copy Lib file to idm migration tool directory</h4>
mkdir ../lib<br>
cp &lt;hcm-vol&gt;/shared/content-tools/csa/lib/CLI-lib.jar ../lib/
<h4>Copy the src IDM encryption directory from 2018.11 to 2020.05 migrate folder</h4>
mkdir 201811-idm-sec<br>
cd 201811-idm-sec/<br>
scp &lt;old-hcm-master-node&gt;:/opt/kubernetes/cfg/idm/security/*.*&nbsp;<br>
&lt;old-hcm-master-node&gt; is one of the master node from the old version of HCM
<h4>Migrate IDM data from 2018.11 to 2020.05</h4>
Run the following command to generate the property files;<br>
java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -g<br>
Update the config.properties according to the source and target environment. You can ignore property destEncryptionDirectory.<br>
Sample configure property would have the following entries;
<pre><code>jdbc.dest.username=cdfidm
srcDbType=postgres
destEncryptionDirectory=/opt/kubernetes/cfg/idm/security/
jdbc.src.username=cdfidm
jdbc.src.databaseUrl=jdbc\:postgresql\://&lt;oldversion-hcm-node&gt;\:5432/defaultdb
destDbType=postgres
jdbc.dest.password=lnwivCoci8QzgyUQPzM=
jdbc.src.password=eX3uAhiA05KIfax75B4=
srcEncryptionDirectory=/root/migrate/idm/201811-idm-sec
jdbc.dest.databaseUrl=jdbc\:postgresql\://172.17.17.47\:5432/defaultdb
</code></pre>
<br>
Database password for internal postgres<br>
kubectl exec -it itom-postgresql-default-54597fc8bd-rtzm7 -n core -c itom-postgresql-default -- get_secret defaultdb_cdfidm_user_password<br>
OUTPUT : PASS=eX3uAhiA05KIfax75B4=<br>
Destination Database URL – If you are using the internal postgres for CDF IDM, then IP address is the postgres service IP which you can get from below command;<br>
kubectl get svc -n core | grep idm-postgresql-svc
<h4>Execute the migration tool</h4>
java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -c config.properties<br>
<br>
Sample output
<pre><code class="language-bash">This tool will merge the data of the source database to the destination database.
It is recommended to take a backup of the data present in jdbc:postgresql://172.16.52.39:5432/defaultdb
Enter 'yes' if the backup of the destination database has been taken. Enter 'no' otherwise [yes/no]:
yes
Successfully initialized application
Start of database migrate ....
There may be a delay in the progress of the tool depending on the number of rows in the table and network latency. Please refer to the logs for the latest status.
INFO    [22/22] rows from ABSTRACT_GROUP merged in 163 ms .................... [ DONE ]
INFO    [40/40] rows from ABSTRACT_GROUP_METADATA merged in 77 ms ............ [ DONE ]
INFO    [22/22] rows from ABSTRACT_GROUP_REPRESENTATION merged in 91 ms ...... [ DONE ]
INFO    [4/4] rows from BASEAUTHCONFIGURATION merged in 30 ms ................ [ DONE ]
INFO    [12/12] rows from DATABASE_GROUP_REP merged in 23 ms ................. [ DONE ]
INFO    [3/3] rows from LDAP_CONFIGURATION merged in 73 ms ................... [ DONE ]
INFO    [10/10] rows from LDAP_GROUP_REP merged in 27 ms ..................... [ DONE ]
INFO    [264/264] rows from METADATA merged in 345 ms ........................ [ DONE ]
INFO    [6/6] rows from ORGANIZATIONS merged in 26 ms ........................ [ DONE ]
INFO    [49/49] rows from PERMISSION merged in 77 ms ......................... [ DONE ]
INFO    [117/117] rows from PERMISSION_ROLE merged in 87 ms .................. [ DONE ]
INFO    [50/50] rows from ROLES merged in 136 ms ............................. [ DONE ]
INFO    [62/62] rows from ROLES_GROUPS merged in 36 ms ....................... [ DONE ]
INFO    [1/1] rows from SAML_CONFIGURATION merged in 18 ms ................... [ DONE ]
The table SCHEMA_VERSION is not present in destination database. So ignoring it and hence the contents of the table will not be merged to destination database
INFO    [207/207] rows from TOKEN_STORE merged in 474 ms ..................... [ DONE ]
The migration of data has been successful
End of database migrate ....
</code></pre>

<h3><br>
Run IDM Config JOB</h3>
cd &lt;core-vol&gt;/suite-install/hcm/output<br>
kubectl delete -f hcm-idm-config-data-job.yaml<br>
kubectl create -f hcm-idm-config-data-job.yaml<br>
kubectl logs hcm-idm-config-data-xj8l4
<h3>Create new secret for the database password and use it in all YAML (Optional)</h3>
Follow this step if the password to the database you installed is different to the password on the database server you point to.<br>
update_secret NEW_DB_PASSWORD &lt;new-database-password&gt;
<h3>Migrate Autopass</h3>
Update &lt;core-vol&gt;/suite-install/hcm/output/hcm-autopass.yaml database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – DBHOST, DBPORT, DBNAME, DBUSER, DBPASSWORD_KEY, DBSYSTEMID.<br>
Delete and create Autopass pods using the following commands;<br>
kubectl delete -f hcm-autopass.yaml<br>
kubectl create -f hcm-autopass.yaml
<h3>Migrate CSA</h3>

<h4>Copy the CSA encryption folder from old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,<br>
cd &lt;hcm-vol&gt;/shared/<br>
mv encryption/ encryption_2020_05<br>
mkdir encryption<br>
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/shared/encryption/*.* encryption/<br>
chown -R itom:itom *
<h4>Update the YAML with the database configuration details</h4>
Update the &lt;core-vol&gt;/suite-install/hcm/output/hcm-csa.yaml database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – CSA_DB_HOST, CSA_DB_PORT, CSA_DB_USER, CSA_DB_PASSWORD_KEY, CSA_DB_NAME, CSA_DB_SERVICENAME.<br>
Change the DEPLOYMENT_TYPE to migrate for the hcm-csa.yaml<br>
Delete and create CSA pods using the following commands;<br>
kubectl delete -f hcm-csa.yaml<br>
kubectl create -f hcm-csa.yaml
<h3>Migrate OO</h3>

<h4>Copy the OO encryption folder from old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,<br>
cd &lt;hcm-vol&gt;/conf/<br>
mv oo oo_2020_05<br>
mkdir oo<br>
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oo/* oo/.<br>
chown -R itom:itom *
<h4>Update the YAML with the database configuration details</h4>
Update the &lt;core-vol&gt;/suite-install/hcm/output/hcm-oo.yaml database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_CENTRAL_DB_HOST, OO_CENTRAL_DB_PORT, OO_CENTRAL_DB_USERNAME, OO_CENTRAL_DB_PASSWORD_KEY, OO_CENTRAL_DB_NAME, OO_CENTRAL_DB_SERVICENAME.<br>
Change the DEPLOYMENT_TYPE to migrate for the hcm-oo.yaml<br>
Delete and create OO pods using the following commands;<br>
kubectl delete -f hcm-oo.yaml<br>
kubectl create -f hcm-oo.yaml
<h3>Migrate OO Designer</h3>

<h4>Copy the OO Designer encryption folder from old version of HCM to 2020.05</h4>
On the HCM 2020.05 master node,<br>
cd &lt;hcm-vol&gt;/conf/<br>
mv oodesigner oodesigner_2020_05<br>
mkdir oodesigner<br>
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oodesigner/* oodesigner/.<br>
chown -R itom:itom *
<h4>Update the YAML with the database configuration details</h4>
Update the &lt;core-vol&gt;/suite-install/hcm/output/hcm-oodesigner.yaml database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_DESIGNER_DB_HOST, OO_DESIGNER_DB_PORT, OO_DESIGNER_DB_USERNAME, OO_DESIGNER_DB_PASSWORD_KEY, OO_DESIGNER_DB_NAME, OO_DESIGNER_DB_SERVICENAME.<br>
Change the DEPLOYMENT_TYPE to migrate for the hcm-oodesigner.yaml<br>
Delete and create OO Designer pods using the following commands;<br>
kubectl delete -f hcm-oodesigner.yaml<br>
kubectl create -f hcm-oodesigner.yaml
<h3>Migrate UCMDB</h3>

<h4><span style="color: rgb(51, 51, 51); font-family: Roboto, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;;">Update the &lt;core-vol&gt;/suite-install/hcm/output/hcm-ucmdb.yaml database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – DB_HOST, DB_PORT, .</span></h4>
Change the DEPLOYMENT_TYPE to migrate for the hcm-ucmdb.yaml<br>
Delete and create UCMDB pods using the following commands;<br>
kubectl delete -f hcm-ucmdb.yaml<br>
kubectl create -f hcm-ucmdb.yaml
<h3>Upgrading Vertica components</h3>
Follow the process to upgrade vertica if you are using any of the Cloud Service Brokering and Governance features

<ol>
	<li>Upgrade vertica to the required version level as per the documented process</li>
	<li>Copy vertica certificate to the HCM 2020.05<br>
	The certificates needs to be copied to &lt;hcm-vol&gt;/certs/ca folder</li>
	<li>Clean up the following in the di folder &lt;hcm-vol&gt;/di<br>
	mv vertica_ingestion vertica_ingestion_old<br>
	mv administration &nbsp;administration_old</li>
	<li>Delete and create hcm-zookeeper.yaml, hcm-kafka.yaml, wait till they are up and running<br>
	kubectl delete -f hcm-zookeeper.yaml<br>
	kubectl delete -f hcm-kafka.yaml<br>
	<br>
	kubectl create -f hcm-zookeeper.yaml<br>
	kubectl create -f hcm-kafka.yaml</li>
	<li>Delete and create hcm-coso-config-data-job.yaml, wait till its completed<br>
	kubectl delete -f hcm-coso-config-data-job.yaml<br>
	kubectl create -f hcm-coso-config-data-job.yaml</li>
	<li>Connect to the vertica database and execute the below query<br>
	drop schema csac cascade;</li>
	<li>Update vertica server details in the hcm-coso-data-ingestion.yaml</li>
	<li>Update the following yaml with vertica server details and database details where ever required
	<ol>
		<li>hcm-coso-data-processor-worker.yaml</li>
		<li>hcm-csa-collector.yaml</li>
		<li>hcm-showback.yaml</li>
		<li>hcm-co-optimizer.yaml</li>
		<li>hcm-boost-cm.yaml</li>
	</ol>
	</li>
	<li>Delete and Create the following yamls
	<ol>
		<li>hcm-coso-data-administration.yaml</li>
		<li>hcm-coso-data-processor-job-submitter.yaml</li>
		<li>hcm-coso-data-processor-master.yaml</li>
		<li>hcm-coso-data-receiver.yaml</li>
		<li>hcm-coso-data-processor-worker.yaml</li>
		<li>hcm-boost-cm.yaml</li>
		<li>hcm-csa-collector.yaml</li>
		<li>hcm-showback.yaml</li>
	</ol>
	</li>
	<li>Copy &lt;hcm-vol&gt;/conf/co from old version HCM NFS to HCM 2020.05, then update vertica details in hcm-co-optimizer.yaml and delete/create it<br>
	a.Update CSA End Point in the Cloud Optimizer UI</li>
	<li>Copy &lt;hcm-vol&gt;/data/csb/elasticsearch from old version HCM NFS to HCM 2020.05, (remove the existing folder if exists), then start the hcm-elasticsearch.yaml</li>
</ol>

<h3>Update HCM URLs</h3>

<ol>
	<li>Update the OO URLs from old host to new fqdn in access_point_table in database machine</li>
	<li>Update Provider access point of OO, CO, DMA to new HCM FQDN in Provider UI</li>
	<li>In OO, update the CSA End Points in Content Management &gt; Configuration Items</li>
	<li>Update ooInboundUser password in OO central with HCM_IDM_PROVIDER_ACCOUNT_OOINBOUNDUSER_PASSWORD</li>
	<li>Run content job container - hcm-content-job.yaml</li>
</ol>

<h3>Bring up all rest of the pods</h3>
/opt/kubernetes/scripts/cdfctl.sh runlevel set -l UP -n hcm-qprrw

<h3>Changing VIP</h3>
If you have installed the HCM 2020.05 using VIP which is different than the previous version of HCM and you want to reconfigure to use the same VIP as old HCM version then you have to change to the VIP on HCM 2020.05, please follow this document;<br>
<a href="https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange">https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange</a><br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname</a><br>
Once the changes are done based on these, the CDF would have been reconfigured to the new VIP.<br>
Now you need to re-configure the HCM yamls to use the new external hostname/VIP. Find and replace the external hostname with the old version HCM external hostname. Then apply the changes using kubectl delete -f and kubectl create -f for all the HCM yamls in &lt;core-vol&gt;/suite-install/hcm/output.<br>
<br>
&nbsp;</html>