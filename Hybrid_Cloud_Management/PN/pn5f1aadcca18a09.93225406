<html>This document describes a streamlined process to help you upgrade HCM to the 2020.05 release. This new process avoids many of the steps that are required by upgrading through all intermediate releases. This approach also reduces downtime, because the older instance remains online until the 2020.05 version is ready.<br>
<br>
You will install a fresh instance of HCM 2020.05 and copy in the older data, configurations, and customizations. When the HCM 2020.05 instance is ready, the older instance can be decommissioned.<br>
<br>
This process has been validated for HCM 2018.11, 2019.02, and 2019.05.
<h2>Plan</h2>
Before you can migrate, you must plan and configure a new environment for HCM 2020.05. The requirements and specifications can be found here:<br>
<br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/Install">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/Install</a><br>
<br>
Note that the operating system and software requirements have probably changed from your pre-2020.05 installation.
<h3>HA Configuration</h3>
During this upgrade, you have the option to upgrade from non-high availability (single master node) to a high availability configuration (3 master nodes)

<h3>NFS</h3>
The HCM 2020.05 installation requires a new NFS mount point. The older mount point can be decommissioned after the upgrade is complete.

<h3>Databases</h3>

<ul>
	<li>You will need to create a new database for Identity Management (IDM) in HCM 2020.05.</li>
	<li>You should clone the existing HCM database. The original copy can be used to keep services running until the 2020.05 migration is complete. If there changes to the original copy of the database while the services are running, then you have to clone and migrate the database for respective components again.</li>
	<li>During the HCM 2020.05 installation, you should have a new database for the CDF IDM which you would continue to use, and while configuring the HCM database you can use the Internal Postgresql non-HA or create temporary databases since you will reconfigure to use older version HCM databases during the upgrade process. You need a temporary Vertica database which you will reconfigure to use from older version of HCM.</li>
</ul>

<h3>Virtual IP Addresses</h3>
If you are using a Virtual IP (VIP) address in your installation, then the 2020.05 installation will need to use a different IP address. When the 2020.05 installation is complete, then you can reconfigure to use the old VIP address, or modify DNS to point to the new address. This is further described in this document in a later section.

<h3>External Load Balancers</h3>
If you are using an External Load Balancer (ELB), then you will deploy HCM 2020.05 with a temporary ELB. You can then re-configure the original ELB to point to the HCM 2020.05 systems. This is further described in this document in a later section.

<h2>Prepare</h2>

<h3>Pre-requisites</h3>

<ul>
	<li>The original HCM database must be cloned. The original version can continue to provide service during the upgrade. The cloned database will be migrated to 2020.05.</li>
	<li>HCM 2020.05 must be installed and deployed as described here:</li>
</ul>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/Install">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/Install</a>

<h2>Upgrade Procedure</h2>
The following tables give you an overview of the upgrade tasks and where they should be performed.

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<thead>
		<tr>
			<th scope="col">Task</th>
			<th scope="col">Where to perform</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><a href="#Expose the database port">Expose the database port</a></td>
			<td>On the master node of the original version of HCM</td>
		</tr>
		<tr>
			<td><a href="#Bring down the HCM">Bring down the HCM</a></td>
			<td>Original version of HCM; HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate IDM">Migrate IDM</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate Autopass">Migrate Autopass</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate CSA">Migrate CSA</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate OO">Migrate OO</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate OO Designer">Migrate OO Designer</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Migrate UCMDB">Migrate UCMDB</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Upgrading Vertica components">Upgrade Vertica components</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Bring up all rest of the pods">Bringing up all the rest of HCM components</a></td>
			<td>HCM 2020.05</td>
		</tr>
		<tr>
			<td><a href="#Changing VIP">Changing VIP or Load Balancer</a></td>
			<td>HCM 2020.05</td>
		</tr>
	</tbody>
</table>

<h3><a id="Expose the database port" name="Expose the database port"></a>Expose the database port in original version of HCM</h3>

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<thead>
		<tr>
			<th scope="col">Role</th>
			<th scope="col">Location</th>
			<th scope="col">Privileges required</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>System administrator</td>
			<td>Master node on the original version of HCM</td>
			<td>Root or sudo</td>
		</tr>
	</tbody>
</table>

<div class="Admonition_Note"><span class="autonumber">Note&nbsp; &nbsp;</span>You can skip this step if you are not using internal Postgresql for the CDF IDM database.</div>
<br>
If you are using internal PostgreSQL for CDF Identity Management, you need to expose the Postgresql port. This is required to migrate the IDM database to the 2020.05 system. Follow the below steps on the existing HCM (pre 2020.05 system):
<h4>Single node Internal Postgres Service</h4>
Steps for ITOM Internal Postgresql to expose port outside Kubernetes cluster

<ol>
	<li>Add the following line to the Postgres configuration file <code>pg_hba.conf</code> in <code>&lt;db-single-vol&gt;/baseinfra-1.0/postgresql95/</code> folder

	<pre><code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	</li>
	<li>Modify the Postgresql yaml file <code>&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</code>. In <code>Container</code> section, insert these line under <code><small>image</small></code>
	<pre><code class="language-yaml">image: localhost:5000/hpeswitom/itom-postgresql:9.5.11-0021
ports:
- containerPort: 5432
  hostPort: 5432</code></pre>
	</li>
	<li>Run the following command to make the changes take effect:<br>
	<code>kubectl apply -f &lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-single-svc.yaml</code></li>
</ol>

<h4>Internal Postgres High Available</h4>
Steps for ITOM Internal Postgresql HA to expose port outside Kubernetes cluster

<ol>
	<li>Modify both of the <code>pg_hba.conf</code> files; <db-node1-vol><code>&lt;db-node1-vol&gt;/baseinfra-1.0/postgresql95HA/node1data/pg_hba.conf</code><br>
	<db-node2-vol><code>&lt;db-node2-vol&gt;/baseinfra-1.0/postgresql95HA/node2data/pg_hba.conf </code></db-node2-vol></db-node1-vol>

	<pre>    <code class="language-vim">host     all     all     0.0.0.0/0     md5</code></pre>
	</li>
	<li>Modify the file itom-postgres-pool-svc.yaml file<br>
	Edit the file<br>
	<code>&lt;core-vol&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</code> Modify it so that the <code>ports</code> section looks like this:
	<pre><code class="language-yaml">ports:
- containerPort: 5432
  hostPort: 5432
  name: pg-port
- containerPort: 5050
  name: pgrest-port</code></pre>
	</li>
	<li>Run the following command to make the changes take effect:
	<pre>kubectl apply -f &lt;core-volume&gt;/suite-install/yamlContent/itom-postgresql-pool-svc.yaml</pre>
	</li>
</ol>

<h3><a id="Bring down the HCM" name="Bring down the HCM"></a>Bring down the old version of HCM</h3>
The following are the criteria to bring down the original version of HCM, otherwise, you can skip this step.

<ol>
	<li>If you are planning to reuse the HCM databases with 2020.05.</li>
	<li>If you are going to make the final switch using the original database with 2020.05.</li>
</ol>
To bring down the HCM, execute the following command on the original version of HCM

<pre>kubectl get ns
$K8S_HOME/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;</pre>

<h3>Bring down HCM 2020.05</h3>
Before bringing down, note down the following secrets on the HCM 2020.05 to use it later.<br>
<code>kubectl exec -it &lt;csa-pod&gt; -n &lt;hcm-namespace&gt; -c hcm-csa -- get_secret HCM_IDM_SVC_PASSWORD</code><br>
<samp>PASS=qpfy_an63?LL.9i8</samp><br>
<br>
You can bring down HCM using the following commands;
<pre>kubectl get ns
$K8S_HOME/scripts/cdfctl.sh runlevel set -l DOWN -n &lt;hcm-namespace&gt;</pre>

<h3><a id="Migrate IDM" name="Migrate IDM"></a>Migrate IDM</h3>

<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<thead>
		<tr>
			<th scope="col">Role</th>
			<th scope="col">Location</th>
			<th scope="col">Privileges required</th>
		</tr>
		<tr>
			<td>System administrator</td>
			<td>Master node on the HCM 2020.05</td>
			<td>Root or sudo</td>
		</tr>
	</thead>
</table>
We need to recreate the IDM database in the new version of HCM installation since the IDM would have initialized with the default seeded user and credentials during the installation, we need to clear all data in IDM, migrate from original IDM database and then reinitialize with HCM seeded user data. Follow all the below sections of steps on the HCM 2020.05 instance.

<h4>Recreate IDM database</h4>

<ol>
	<li>Stop IDM
	<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl delete -f idm.yaml</pre>
	</li>
	<li>If the database used to install HCM is Internal Postgresql, follow the below steps
	<ol>
		<li>Stop Internal Postgresql
		<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl delete -f itom-postgresql-single-svc.yaml</pre>
		</li>
		<li>Move the Postgresql folder to a backup location
		<pre><code class="language-vim">mkdir ~/backupPG
cd /baseinfra-1.0
mv postgresql106 ~/backupPG</code></pre>
		</li>
		<li>Start Internal Postgresql
		<hr>
		<pre><code class="language-vim">cd /suite-install/yamlContent/
kubectl create -f itom-postgresql-single-svc.yaml</code></pre>
		</li>
	</ol>
	</li>
	<li>If the database used to install HCM is External Oracle or Postgresql, then back up the database following the vendor documentation. Drop and create a new database based on the HCM document.</li>
	<li>Start IDM and open to CDF management portal
	<pre>cd &lt;core-vol&gt;/suite-install/yamlContent/
kubectl create -f idm.yaml</pre>
	<br>
	Open the CDF management portal to ensure you can access the portal, you might not be able to login using your existing admin password.</li>
</ol>

<h4>Reset the CDF admin password</h4>
To reset the CDF admin password execute the following commands

<pre>kubectl exec -it &lt;idm-pod&gt; -n core -c idm -- bash
sh /idmtools/idm-installer-tools/idm.sh databaseUser resetPassword -org Provider -name "admin" -plainPwd "&lt;new-temp-password&gt;"
</pre>
<br>
Login into the CDF management console and reset the password correctly to the password which you had used while installing HCM.
<h4>Create idmTransportUser in IDM</h4>
Execute the following command and note down the base64 encoding of the username and password which has to be used in the subsequent command

<pre>kubectl exec -it &lt;idm-pod&gt; -n core -c idm -- get_secret idm_transport_admin_password
<tt><em>PASS=CqeSFXzWvb3Wdw==</em></tt>

echo -n transport_admin:CqeSFXzWvb3Wdw== | base64
<tt><code><em>dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=</em></code></tt>
</pre>
<br>
Replace the base64 encoded password, &lt;external-fqdn&gt;, and &lt;cdf-admin-password&gt; in the below curl commands against the authorization header.
<pre>curl -k -X POST \
  https://&lt;external-fqdn&gt;:5443/idm-service/v2.0/tokens \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -d '{
    "passwordCredentials": {
        "username": "admin",
        "password": "&lt;cdf-admin-password&gt;"
    },
    "tenantName": "Provider"
}'
</pre>
Extract the token from the above command and replace it against the x-auth-token. Replace &lt;external-fqdn&gt; and &lt;cdf-admin-password&gt; in the below curl command.

<pre>curl -k -X POST \
  https://&lt;external-fqdn&gt;:5443/idm-service/api/system/jsondata \
  -H 'accept: application/json' \
  -H 'authorization: Basic dHJhbnNwb3J0X2FkbWluOkNxZVNGWHpXdmIzV2R3PT0=' \
  -H 'cache-control: no-cache' \
  -H 'content-type: application/json' \
  -H 'x-auth-token: &lt;TOKEN&gt;' \
  -d '[{
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "PROVIDER"
        },
        "attributes": {
            "name": "admin",
            "password": "&lt;cdf-admin_password&gt;",
            "type" : "INTERNAL_SEEDED_USER"
        }
    },
   {
        "operation": "ADD_OR_UPDATE",
        "type": "databaseUser",
        "names": {
            "organizationName": "IdMIntegration"
        },
        "attributes": {
            "name": "idmTransportUser",
              "password": "&lt;<code>HCM_IDM_SVC_PASSWORD&gt;</code>",
             "type" : "INTEGRATION_USER"
        }
    }]'</pre>
&nbsp;

<div class="Admonition_Note"><span class="autonumber">Note&nbsp; &nbsp;</span><strong><i>&lt;</i><code>HCM_IDM_SVC_PASSWORD&gt;</code></strong> is the password you have backed up before bringing down HCM 2020.05 under the topic "Bring down HCM 2020.05"</div>

<h4><br>
Download the IDM migration tool</h4>
Copy the IDM Migration tool from the below path and move it to a directory on one of the master nodes.

<pre>mkdir /root/migrate/idm
cd /root/migrate/idm
kubectl cp suite-conf-pod-hcm-5d79547896-gnnr2:/usr/share/tomcat/webapps/ROOT/tools/hcm-migration-tool.jar /root/migrate/idm/hcm-migration-tool.jar -n core -c suite-config
unzip hcm-migration-tool.jar BOOT-INF/lib/idm-data-migrater-tool-05.00.000-SNAPSHOT.jar
mv BOOT-INF/lib/idm-data-migrater-tool-05.00.000-SNAPSHOT.jar .</pre>

<h4>Copy Lib file to IDM migration tool directory</h4>
Go to the directory where you have copied the IDM migration tool.

<pre>mkdir ../lib
cp &lt;hcm-vol&gt;/shared/content-tools/csa/lib/CLI-lib.jar ../lib/.</pre>

<h4>Copy the src IDM encryption directory from the original version of HCM to 2020.05 migrate folder</h4>

<pre>mkdir hcm-idm-sec
cd hcm-idm-sec/
scp &lt;old-hcm-master-node&gt;:$K8S_HOME/cfg/idm/security/*.* .</pre>
&lt;old-hcm-master-node&gt; is any one of the master nodes from the old version of HCM

<h4>Prepare the migration tool and configure the properties file</h4>
Run the following command to generate the property files;<br>
<code>java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -g</code><br>
<br>
Update the config.properties according to the source and target environment. You can ignore property <code>destEncryptionDirectory</code>.<br>
Sample configure property would have the following entries for an internal Postgresql;
<pre>jdbc.dest.username=cdfidm
srcDbType=postgres
destEncryptionDirectory=/opt/kubernetes/cfg/idm/security/
jdbc.src.username=cdfidm
jdbc.src.databaseUrl=jdbc\:postgresql\://&lt;oldversion-hcm-node&gt;\:5432/defaultdb
destDbType=postgres
jdbc.dest.password=lnwivCoci8QzgyUQPzM=
jdbc.src.password=eX3uAhiA05KIfax75B4=
srcEncryptionDirectory=/root/migrate/idm/hcm-idm-sec
jdbc.dest.databaseUrl=jdbc\:postgresql\://172.17.17.47\:5432/defaultdb
</pre>
<br>
Database password for internal Postgresql
<pre>kubectl exec -it &lt;postgresql-pod&gt; -n core -c itom-postgresql-default -- get_secret defaultdb_cdfidm_user_password
PASS=eX3uAhiA05KIfax75B4=
</pre>
Destination Database URL – If you are using the internal Postgresql for CDF IDM, then IP address is the Postgresql service IP which you can get from below command;

<pre>kubectl get svc -n core | grep idm-postgresql-svc
idm-postgresql-svc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ClusterIP &nbsp; 172.17.17.47 &nbsp; &lt;none&gt; &nbsp; &nbsp; &nbsp; &nbsp;5432/TCP,5050/TCP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 439d</pre>

<h4>Execute the migration tool to migrate data from the original version of HCM to 2020.05</h4>
<code>java -jar idm-migration-tool-05.00.000-SNAPSHOT.jar -c config.properties</code><br>
<br>
The command returns a result that resembles the following
<pre><code class="language-bash">This tool will merge the data of the source database to the destination database.
It is recommended to take a backup of the data present in jdbc:postgresql://172.16.52.39:5432/defaultdb
Enter 'yes' if the backup of the destination database has been taken. Enter 'no' otherwise [yes/no]:
yes
Successfully initialized application
Start of database migrate ....
There may be a delay in the progress of the tool depending on the number of rows in the table and network latency. Please refer to the logs for the latest status.
INFO    [22/22] rows from ABSTRACT_GROUP merged in 163 ms .................... [ DONE ]
INFO    [40/40] rows from ABSTRACT_GROUP_METADATA merged in 77 ms ............ [ DONE ]
INFO    [22/22] rows from ABSTRACT_GROUP_REPRESENTATION merged in 91 ms ...... [ DONE ]
INFO    [4/4] rows from BASEAUTHCONFIGURATION merged in 30 ms ................ [ DONE ]
INFO    [12/12] rows from DATABASE_GROUP_REP merged in 23 ms ................. [ DONE ]
INFO    [3/3] rows from LDAP_CONFIGURATION merged in 73 ms ................... [ DONE ]
INFO    [10/10] rows from LDAP_GROUP_REP merged in 27 ms ..................... [ DONE ]
INFO    [264/264] rows from METADATA merged in 345 ms ........................ [ DONE ]
INFO    [6/6] rows from ORGANIZATIONS merged in 26 ms ........................ [ DONE ]
INFO    [49/49] rows from PERMISSION merged in 77 ms ......................... [ DONE ]
INFO    [117/117] rows from PERMISSION_ROLE merged in 87 ms .................. [ DONE ]
INFO    [50/50] rows from ROLES merged in 136 ms ............................. [ DONE ]
INFO    [62/62] rows from ROLES_GROUPS merged in 36 ms ....................... [ DONE ]
INFO    [1/1] rows from SAML_CONFIGURATION merged in 18 ms ................... [ DONE ]
The table SCHEMA_VERSION is not present in destination database. So ignoring it and hence the contents of the table will not be merged to destination database
INFO    [207/207] rows from TOKEN_STORE merged in 474 ms ..................... [ DONE ]
The migration of data has been successful
End of database migrate ....
</code></pre>

<h3></h3>

<p>Now the IDM migration is complete.</p>

<h3>Run IDM Config JOB</h3>

<pre>cd &lt;core-vol&gt;/suite-install/hcm/output
kubectl delete -f hcm-idm-config-data-job.yaml
kubectl create -f hcm-idm-config-data-job.yaml
kubectl logs hcm-idm-config-data-xj8l4 -n &lt;hcm-namespace&gt;</pre>

<p>Wait for the hcm-idm-config-data job to complete. Check the logs of the hcm-idm-config-data to ensure there is no error and the job is complete successfully.</p>

<h3>Create new secret for the HCM database password and use it in all YAML (Optional)</h3>
Follow this step if the password to the HCM databases you installed is different from the password on the database server you point to.

<pre>kubectl exec -it &lt;any-hcm-pod&gt; -n &lt;hcm-namespace&gt; -c &lt;container&gt; -- bash
update_secret NEW_DB_PASSWORD &lt;new-database-password&gt;</pre>

<h3><a id="Migrate Autopass" name="Migrate Autopass"></a>Migrate Autopass</h3>
Update <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-autopass.yaml </code>database configurations to point to the actual HCM database used with the old version. The properties that need to be updated are – DBHOST, DBPORT, DBNAME, DBUSER, DBPASSWORD_KEY, DBSYSTEMID.<br>
Delete and create Autopass pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-autopass.yaml
kubectl create -f hcm-autopass.yaml</code></pre>

<h3><a id="Migrate CSA" name="Migrate CSA"></a>Migrate CSA</h3>

<h4>Copy the CSA encryption folder from the old version of HCM to 2020.05</h4>
Mount the NFS on the HCM 2020.05 master node, execute the following commands:

<pre>cd &lt;hcm-vol&gt;/shared/
mv encryption/ encryption_2020_05
mkdir encryption
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/shared/encryption/*.* encryption/
chown -R itom:itom *</pre>

<h4>Copy the Resource Alias and JSPs from the old version of HCM to 2020.05</h4>

<p>In the old version of HCM if the <code>resource-alias.xml</code> and JSPs are configured then you must copy these customizations to the HCM 2020.05. Please check the resource-alias.xml and JSPs in the following path <code>&lt;hcm-vol&gt;/sync/csa/jboss-as/standalone/deployments/csa.war/propertysources</code></p>

<h4>Update the YAML with the database configuration details</h4>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-csa.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties that need to be updated are – CSA_DB_HOST, CSA_DB_PORT, CSA_DB_USER, CSA_DB_PASSWORD_KEY, CSA_DB_NAME, CSA_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt> in the <code>hcm-csa.yaml</code><br>
<br>
Delete and create CSA pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-csa.yaml
kubectl create -f hcm-csa.yaml</code></pre>

<h3><a id="Migrate OO" name="Migrate OO"></a>Migrate OO</h3>

<h4>Copy the OO encryption folder from old version of HCM to 2020.05</h4>
Mount the NFS on the HCM 2020.05 master node, execute the following commands:

<pre>cd &lt;hcm-vol&gt;/conf/
mv oo oo_2020_05
mkdir oo
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oo/* oo/.
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-oo.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_CENTRAL_DB_HOST, OO_CENTRAL_DB_PORT, OO_CENTRAL_DB_USERNAME, OO_CENTRAL_DB_PASSWORD_KEY, OO_CENTRAL_DB_NAME, OO_CENTRAL_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt> in the <code>hcm-oo.yaml</code><br>
<br>
Delete and create OO pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-oo.yaml
kubectl create -f hcm-oo.yaml</code></pre>

<h3><a id="Migrate OO Designer" name="Migrate OO Designer"></a>Migrate OO Designer</h3>

<h4>Copy the OO Designer encryption folder from the old version of HCM to 2020.05</h4>
Mount the NFS on the HCM 2020.05 master node, execute the following commands:

<pre>cd &lt;hcm-vol&gt;/conf/
mv oodesigner oodesigner_2020_05
mkdir oodesigner
scp &lt;old-hcm-nfs&gt;:&lt;hcm-vol&gt;/conf/oodesigner/* oodesigner/.
chown -R itom:itom *</pre>

<h4>Update the YAML with the database configuration details</h4>
Update the &lt;<code>core-vol&gt;/suite-install/hcm/output/hcm-oodesigner.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – OO_DESIGNER_DB_HOST, OO_DESIGNER_DB_PORT, OO_DESIGNER_DB_USERNAME, OO_DESIGNER_DB_PASSWORD_KEY, OO_DESIGNER_DB_NAME, OO_DESIGNER_DB_SERVICENAME.<br>
<br>
Change the <code>DEPLOYMENT_TYPE</code> to <tt><strong>migrate</strong></tt> in the <code>hcm-oodesigner.yaml</code><br>
<br>
Delete and create OO Designer pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-oodesigner.yaml
kubectl create -f hcm-oodesigner.yaml</code></pre>

<h3><a id="Migrate UCMDB" name="Migrate UCMDB"></a>Migrate UCMDB</h3>
Update the <code>&lt;core-vol&gt;/suite-install/hcm/output/hcm-ucmdb.yaml</code> database configurations to point to the actual HCM database used with the old version. The properties needs to be updated are – DB_HOST, DB_PORT, DB_USER, DB_SCHEMA, DB_PASSWORD_VAULT_KEY.<br>
Delete and create UCMDB pods using the following commands;
<pre><code class="language-vim">kubectl delete -f hcm-ucmdb.yaml
kubectl create -f hcm-ucmdb.yaml</code></pre>

<h3><a id="Upgrading Vertica components" name="Upgrading Vertica components"></a>Upgrading Vertica components</h3>
Ensure you have the original version of HCM stopped and its not accessing the vertica database. Follow the process to upgrade Vertica if you are using any of the Cloud Service Brokering and Governance features.

<ol>
	<li>Upgrade Vertica to the required version level as per the documented process.<br>
	Upgrade to Vertica 9.2.0-7 version. For help, see <a href="https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/InstallationGuide/Upgrade/UpgradingVertica.htm?tocpath=Installing%20Vertica%7CUpgrading%20Vertica%7C_____0" rel="nofollow" target="1">Upgrading Vertica</a> in the Vertica documentation.</li>
	<li>Copy Vertica certificate to the HCM 2020.05<br>
	The certificate has to be copied to <code>&lt;hcm-vol&gt;/certs/ca</code> folder</li>
	<li>Clean up the following in the di folder <code>&lt;hcm-vol&gt;/di</code>
	<pre><code class="language-vim">mv vertica_ingestion vertica_ingestion_old
mv administration administration_old</code></pre>
	</li>
	<li>Delete and create <code>hcm-zookeeper.yaml, hcm-kafka.yaml</code>, wait till they are up and running
	<pre><code class="language-vim">kubectl delete -f hcm-zookeeper.yaml
kubectl delete -f hcm-kafka.yaml

kubectl create -f hcm-zookeeper.yaml
kubectl create -f hcm-kafka.yaml</code></pre>
	</li>
	<li>Delete and create <code>hcm-coso-config-data-job.yaml</code>, wait till it's completed
	<pre><code class="language-vim">kubectl delete -f hcm-coso-config-data-job.yaml
kubectl create -f hcm-coso-config-data-job.yaml</code></pre>
	</li>
	<li>Connect to the Vertica database and execute the below query<br>
	<code>drop schema csac cascade;</code></li>
	<li>Update the following yaml with Vertica server details and database details
	<ol>
		<li>hcm-coso-data-ingestion.yaml</li>
		<li>hcm-coso-data-processor-worker.yaml</li>
		<li>hcm-csa-collector.yaml</li>
		<li>hcm-showback.yaml</li>
		<li>hcm-co-optimizer.yaml</li>
		<li>hcm-boost-cm.yaml</li>
	</ol>
	</li>
	<li>Apply the Vertica server changes<br>
	Use <code>kubectl delete -f &lt;filename&gt;</code> and <code>kubectl create -f &lt;filename&gt;</code> for each of the following files. This would recreate all these yamls and apply Vertica server changes
	<pre><code class="language-vim">kubectl delete -f hcm-coso-data-ingestion.yaml
kubectl delete -f hcm-coso-data-administration.yaml
kubectl delete -f hcm-coso-data-processor-job-submitter.yaml
kubectl delete -f hcm-coso-data-processor-master.yaml
kubectl delete -f hcm-coso-data-receiver.yaml
kubectl delete -f hcm-coso-data-processor-worker.yaml
kubectl delete -f hcm-boost-cm.yaml
kubectl delete -f hcm-csa-collector.yaml
kubectl delete -f hcm-showback.yaml

kubectl create -f hcm-coso-data-ingestion.yaml
kubectl create -f hcm-coso-data-administration.yaml
kubectl create -f hcm-coso-data-processor-job-submitter.yaml
kubectl create -f hcm-coso-data-processor-master.yaml
kubectl create -f hcm-coso-data-receiver.yaml
kubectl create -f hcm-coso-data-processor-worker.yaml
kubectl create -f hcm-boost-cm.yaml
kubectl create -f hcm-csa-collector.yaml
kubectl create -f hcm-showback.yaml</code></pre>
	Wait till all the pods are up and running before you go to the next step.</li>
	<li>Copy <code>&lt;hcm-vol&gt;/conf/co</code> from the old version of HCM NFS to HCM 2020.05, then update Vertica details in <code>hcm-co-optimizer.yaml</code> and apply change
	<ol>
		<li>Once the Cloud Optimizer pod is running, update CSA End Point in the Cloud Optimizer UI to use the current CSA URL</li>
	</ol>
	</li>
	<li>Copy <code>&lt;hcm-vol&gt;/data/csb/elasticsearch</code> from the old version of HCM NFS to HCM 2020.05, (remove the existing folder if exists), then start the <code>hcm-elasticsearch.yaml</code></li>
</ol>

<h3>Update HCM URLs</h3>

<ol>
	<li>Update the OO URLs from old host to new HCM endpoint in <code>access_point_table</code> in CSA database</li>
	<li>Update the access point of OO, CO providers to new HCM endpoint in CSA Provider UI</li>
	<li>In OO, update the CSA End Points in Content Management &gt; Configuration Items</li>
	<li>Update ooInboundUser password in OO central with HCM_IDM_PROVIDER_ACCOUNT_OOINBOUNDUSER_PASSWORD. Use the below command to get the password<br>
	<code>kubectl exec -it &lt;hcm-oo-pod&gt; -n &lt;hcm-namespace&gt; -c hcm-oo -- get_secret HCM_IDM_PROVIDER_ACCOUNT_OOINBOUNDUSER_PASSWORD</code></li>
	<li>Run content job container to get the latest OOTB content - <code>hcm-content-job.yaml</code></li>
</ol>

<h3><a id="Bring up all rest of the pods" name="Bring up all rest of the pods"></a>Bring up all the rest of HCM components</h3>

<pre>kubectl get ns
$K8S_HOME/scripts/cdfctl.sh runlevel set -l UP -n &lt;hcm-namespace&gt;</pre>

<h3><a id="Changing VIP" name="Changing VIP"></a>Changing VIP or Load Balancer</h3>
If you have installed the HCM 2020.05 using VIP which is different than the previous version of HCM and you want to reconfigure to use the same VIP as the original HCM version, then you have to change the VIP on HCM 2020.05. To change the VIP follow the steps in the below documents.<br>
<a href="https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange">https://docs.microfocus.com/itom/SMAX:2020.05/VIPchange</a><br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname</a><br>
<br>
If you have installed the HCM 2020.05 using a temporary load balancer and now want to reconfigure it to the actual External Load Balancer, then follow the steps in the below document.<br>
<a href="https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname">https://docs.microfocus.com/itom/Hybrid_Cloud_Management:2020.05/ChangeExternalAccessHostname</a><br>
<br>
Once the changes are done following the above documentation, the CDF would have been reconfigured to the new VIP or Load Balancer.<br>
<br>
You need to re-configure the HCM yamls to use the new VIP or External Load Balancer. Find and replace the existing external hostname in the yamls with the actual external hostname. Then apply the changes using kubectl delete -f and kubectl create -f for all the HCM yamls in <code>&lt;core-vol&gt;/suite-install/hcm/output</code> and wait for all the pods in HCM namespace to come up correctly.<br>
<br>
Sample command to replace the hostname and apply changes. Backup the files before changing them.
<pre>cd &lt;core-vol&gt;/suite-install/hcm/output
sed -i 's/oldhcm-host.domain/newhcm-host.domain/g' *.yaml
ls -p *.yaml | grep -v / | tr '\n' ','
kubectl delete -f &lt;comma separated files&gt;
kubectl create -f &lt;comma separated files&gt;</pre>
<br>
The HCM side-by-side upgrade to 2020.05 is now complete. You can decommission the original version of HCM.</html>