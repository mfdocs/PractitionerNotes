<div>
<p>&nbsp;</p>

<h2>Introduction</h2>

<p>In order to support mission-critical systems, the Service Management Automation suite (SMA) is capable of running in a High Availability (HA) configuration. When running in HA mode, SMA is able to maintain end-user connections if the master or worker nodes become inactive. To run in HA mode, additional resources are required beyond the minimum resources in the sizing guides. Please note that the recommendations in this section do not cover the NFS and database implementations.</p>

<h2>Use Case</h2>

<p>Company ABC wants to implement HA for their SMA instance. They will be using the fully containerized SMA mode and use Service Management, the Service Portal, UCMDB, Smart Analytics, and chat. Company ABC estimates that they will have no more than 400 concurrent Service Portal or Agents using the system at any time.</p>

<h3>Configuration Steps - Summary</h3>

<ol>
	<li>Determine the appropriate hardware resources</li>
	<li>Prepare the environment for installation</li>
	<li>Obtain the virtual IP and DNS name</li>
	<li>Install the first master node</li>
	<li>Install the second master node</li>
	<li>Install the third master node</li>
	<li>Test that the master nodes are working together</li>
	<li>Deploy the worker nodes</li>
	<li>Upload the SMA images to master 1</li>
	<li>Install SMA suite</li>
</ol>

<p>&nbsp;</p>

<h3>Configuration Steps - Detail</h3>

<h4>1 — Determine the appropriate hardware resources</h4>

<p>The sizing guide is available on the help center and recommends the minimum hardware required to run the SMA suite based on the concurrent user count. The sizing guide is available here for the Service Manager version of SMA. Company ABC concludes that since they will not exceed 500 concurrent users that they will use the Medium installation configuration.</p>

<p><img alt="HA 1.png" src="/mediawiki/images/f/f2/HA_1.png" style="height:122px; width:900px"></p>

<p>Production installations require an external NFS server and database servers. The NFS and databases need to be configured according to the internal IT policies of the customers that provide an HA environment. The process will be transparent to the SMA installation.<img alt="HA 2.png" src="/mediawiki/images/5/59/HA_2.png" style="height:135px; width:975px"></p>

<p>The HA configuration requires an additional 2 Master nodes so 3 in total. In all, there will be 11 images for the SMA installation as well as the database and NFS servers.</p>

<p>It is important to note that the number of master nodes cannot be changed after the system is fully installed. The desired number of master nodes must be determined and installed before the worker nodes are added.</p>

<h4>2 — Prepare the environment for installation</h4>

<p>A number of pre-requisites required on the master and worker nodes are listed on the help center here. It is important to not miss the step that creates the <strong>no_proxy</strong> environment variable on each machine.</p>

<p>&nbsp;</p>

<h4>3 — Obtain the Virtual IP address</h4>

<p>When using HA mode with multiple master nodes, a static IP and DNS name are required. The IP will not be assigned to any specific machine. Instead, the IP address will be used as a virtual IP and will be managed by the three master nodes. If any node goes down, end users will not experience an interruption in their connection. The embedded technology of the master nodes manages the IP address, and no additional network configuration is required. It is recommended that the Master IP addresses and the IP address used as the virtual be on the same subnet.</p>

<p>&nbsp;</p>

<h4>4 — Install the first master node</h4>

<p>Assuming the pre-requisites and the NFS partitions are created as defined in the help center, the first master node is ready to be installed. To install the master nodes, the following information is required:</p>

<ul>
	<li>IP addresses of the 3 master nodes</li>
	<li>IP address of the virtual IP</li>
	<li>The external access Hostname — when using HA this is the DNS name assigned to the virtual IP address</li>
	<li>The NFS server IP and path</li>
</ul>

<p><br>
The information required for the first master nodes install.properties file is:</p>

<ul>
	<li>MASTER_NODES="16.103.28.149 16.103.28.151 16.103.28.153"</li>
	<li>EXTERNAL_ACCESS_HOST=svssawha.mycompany.net</li>
	<li>HA_VIRTUAL_IP=16.103.28.1</li>
	<li>NFS_SERVER=16.103.28.149</li>
	<li>NFS_FOLDER=/var/vols/itom/core</li>
	<li>DEFAULT_DB_TYPE=EMBEDDED</li>
</ul>

<p>It is best practice to install the worker nodes using the admin UI once the three master nodes have been configured. In this example, we are using the embedded Postgres for the CDF database. This is not the Service Manager database, but rather the database that the CDF uses to manage the master and worker nodes. In production mode, an external Postgres database should be used. Instructions for using an external Postgres are available in the help center.</p>

<p>After editing the ‘install.properties’ file with the above values and settings, the first master may be installed.</p>

<p>&nbsp;</p>

<h4>5 — Install the second master node</h4>

<p>The process to install the second master node is essentially the same as the first master node with a few extra steps. After completing the pre-requisites and ensuring the no_proxy environment variable is set, copy the installation files to the second machine. From the <em>/opt/kubernetes/ssl</em> directory, copy the ‘ca.crt’, ‘server.crt’ and ‘server.key’ files to the <em>/tmp</em> directory so that the installation can use them while installing the node. Edit the ‘install.properties’ file using the same parameters as the first master node, but add the additional three variables for the crt files:</p>

<ul>
	<li>MASTER_NODES="16.103.28.149 16.103.28.151 16.103.28.153"</li>
	<li>EXTERNAL_ACCESS_HOST=svssawha.mycompany.net</li>
	<li>HA_VIRTUAL_IP=16.103.28.1</li>
	<li>NFS_SERVER=16.103.28.149</li>
	<li>NFS_FOLDER=/var/vols/itom/core</li>
	<li>DEFAULT_DB_TYPE=EMBEDDED</li>
	<li>PEER_CA_FILE=/tmp/ca.crt</li>
	<li>PEER_CERT_FILE=/tmp/server.crt</li>
	<li>PEER_KEY_FILE=/tmp/server.key</li>
</ul>

<p>&nbsp;</p>

<h4>6 — Install the third master node</h4>

<p>The process to install the third master node is the same as the 2nd master node. After completing the pre-requisites and ensuring the <strong>no_proxy</strong> environment variable is set, copy the installation files to the third machine. From the <em>/opt/kubernetes/ssl</em> directory copy the ‘ca.crt’, ‘server.crt’ and ‘server.key’ files to the /tmp directory so that the installation can use them while installing the node.</p>

<p>Edit the ‘install.properties’ file using the same parameters as the first master node but add the additional three variables for the crt files:</p>

<ul>
	<li>MASTER_NODES="16.103.28.149 16.103.28.151 16.103.28.153"</li>
	<li>EXTERNAL_ACCESS_HOST=svssawha.mycompany.net</li>
	<li>HA_VIRTUAL_IP=16.103.28.1</li>
	<li>NFS_SERVER=16.103.28.149</li>
	<li>NFS_FOLDER=/var/vols/itom/core</li>
	<li>DEFAULT_DB_TYPE=EMBEDDED</li>
	<li>PEER_CA_FILE=/tmp/ca.crt</li>
	<li>PEER_CERT_FILE=/tmp/server.crt</li>
	<li>PEER_KEY_FILE=/tmp/server.key</li>
</ul>

<p>&nbsp;</p>

<h4>7 — Test that the master nodes are working together</h4>

<p>After the three master nodes have been installed and configured there are a few basic commands and tests that will confirm functionality. Use the command “kubectl get nodes –show-labels” to view the master nodes and confirm the three appear with the master=true parameter.<img alt="HA 3.png" src="/mediawiki/images/9/9c/HA_3.png" style="height:70px; width:973px"></p>

<p>Use the “kubectl get pods –all-namespaces –o wide” command to view that the pods are running on all three of the master nodes. There should be an apiserver, controller, kube-proxy etc for each of the masters.</p>

<p><img alt="HA 4.png" src="/mediawiki/images/1/1b/HA_4.png" style="height:482px; width:793px"></p>

<p>&nbsp;</p>

<h4>8 — Deploy the worker nodes</h4>

<p>Connect to the CDF UI using the URL <strong>https://&lt;master_node_url&gt;:5443/#/workload?namespace=core</strong>. Note that the external host access will be the DNS name of the virtual IP assigned to SMA. When clicking on <strong>administration-&gt; nodes</strong>, the three master nodes will be visible. After running through the pre-requisite list, deploy the worker nodes using the UI.</p>

<p><img alt="HA 5b.png" src="/mediawiki/images/6/6f/HA_5b.png" style="height:403px; width:618px"></p>

<p>When complete, the three master nodes and the newly added workers work should be listed.</p>

<p><img alt="HA 6.png" src="/mediawiki/images/b/bc/HA_6.png" style="height:254px; width:470px"></p>

<h4>9 — Upload SMA suite images to the first master node</h4>

<p>The installation of SMA requires the SMA images to be downloaded from the Docker hub site and then uploaded to the local registry of the master node. The instructions for this can be found in the help center <a href="https://docs.microfocus.com/">here</a>. The SMA images need to be uploaded on the current active master node. The active master node can be determined by running “kubectl get pods –all-namespaces –o wide” and looking for the IP address or DNS name of the machine running the suite-db and suite-installer pods. The download images and upload images scripts described in the help should be run from this machine.</p>

<h4>10 — Install SMA suite</h4>

<p>Once these steps are completed, the SMA installation can be performed.</p>
<!-- 
NewPP limit report
Cached time: 20200217094525
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.040 seconds
Real time usage: 0.178 seconds
Preprocessor visited node count: 18/1000000
Preprocessor generated node count: 46/1000000
Post‐expand include size: 8753/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 0/5000000 bytes
--><!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
--><!-- Saved in parser cache with key docops_wiki:pcache:idhash:875821-0!canonical and timestamp 20200217094525 and revision id 1577592
 --></div>
