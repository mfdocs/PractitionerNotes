<html><p>This topic describes how you can use Velero,&nbsp;an open-source tool, to back up and restore Kubernetes cluster resources and persistent volumes in an on-premises environment. This is helpful when you have destroyed some Kubernetes resources for whatever reason, for example, when you have deleted the suite namespace accidentally.&nbsp;</p>

<div class="Admonition_Important"><span class="autonumber">Note</span> This tool does not back up database data and NFS data.&nbsp;</div>

<p>Perform the following steps to set up Velero on OpenShift.&nbsp;</p>

<h2>Set up Velero</h2>

<p>Perform the following steps to set up Velero in your environment.</p>

<h3>Download a&nbsp;Minio&nbsp;image</h3>

<p>If your control plane node has Internet access, download the image from the control plane&nbsp;node; otherwise, download the image from another Linux machine that has Internet access and then transfer the image to the control plane node.</p>

<ol>
	<li>On the download machine, navigate to a directory where you want to download the images, and then run the following commands:
	<pre><code>docker pull minio/minio:latest&nbsp;
docker pull minio/mc:latest&nbsp;</code></pre>
	</li>
	<li lang="en-US">If the control plane node has no Internet access, transfer&nbsp;the images to the control plane node.&nbsp;</li>
	<li lang="en-US">Obtain the image IDs.
	<ol start="1" style="list-style-type: lower-alpha;">
		<li>Run the following command:&nbsp;
		<pre><code>docker images |grep minio</code></pre>
		</li>
		<li>In the output, find the&nbsp;IDs of the&nbsp;images. In the following example, it's&nbsp;8dbf9ff992d5.
		<pre><code>docker.io/minio/minio latest 8dbf9ff992d5   30 hours ago 183 MB
docker.io/minio/mc     latest  cb749294c71f  6 days ago     134MB</code></pre>
		</li>
	</ol>
	</li>
	<li lang="en-US">Run the following command to tag the&nbsp;images:
	<pre><code>docker tag &lt;image ID&gt; &lt;image registry URL&gt;/&lt;organization name&gt;/minio:&lt;tag&gt;
docker tag &lt;image ID&gt; &lt;image registry URL&gt;/&lt;organization name&gt;/mc:&lt;tag&gt;</code></pre>

	<ul>
		<li>
		<p>&lt;image ID&gt;: the image IDs&nbsp;you obtained in the previous step.</p>
		</li>
		<li>
		<p>&lt;image registry URL&gt;/&lt;organizaition name&gt;: your image registry URL/organization name. If using the local registry, it's localhost:5000/hpeswitom; if using an external registry, ask your registry administrator for it.</p>
		</li>
		<li>&lt;tag&gt;: specify any value you like.&nbsp;</li>
	</ul>

	<p>The following is an example:</p>

	<pre><code>docker tag 8dbf9ff992d5 myregistry.azurecr.io/sandbox/minio:test
docker tag cb749294c71f myregistry.azurecr.io/sandbox/mc:test</code></pre>
	</li>
	<li lang="en-US">Push the images into your image registry:
	<pre><code>docker push &lt;image registry URL&gt;/&lt;organization name&gt;/minio:&lt;tag&gt;</code></pre>

	<p>Example 1:</p>

	<pre><code>docker push myregistry.azurecr.io/sandbox/minio:test
docker push myregistry.azurecr.io/sandbox/mc:test</code></pre>

	<p>Example 2:</p>

	<pre><code>docker push localhost:5000/hpeswitom/minio:test
docker push localhost:5000/hpeswitom/mc:test</code></pre>
	</li>
</ol>

<h3>Create an NFS volume for Minio</h3>

<ol>
	<li>On the NFS server, create a directory for Minio. For example:
	<pre><code>sudo mkdir -p /var/vols/itom/itsma/minio</code></pre>
	</li>
	<li>Change the ownership of the directory:
	<pre><code>sudo&nbsp;chown&nbsp;-R&nbsp;$uid:$gid&nbsp;/var/vols/itom/itsma/minio&nbsp;</code></pre>
	</li>
	<li>If your NFS configuration used no_root_squash, run two more commands:
	<pre><code>sudo&nbsp;chmod&nbsp;g+w&nbsp;/var/vols/itom/itsma/minio&nbsp;
sudo&nbsp;chmod&nbsp;g+s&nbsp;/var/vols/itom/itsma/minio</code></pre>
	</li>
</ol>

<h3 paraeid="{47265f47-a254-414e-9f16-6d5e2ff3447b}{160}" paraid="1310021110">Install Minio</h3>

<p>Perform the following steps on the control plane node.</p>

<ol>
	<li>Create a&nbsp;minio.yaml file with the content below.&nbsp;</li>
	<li>Search the file for the following comments and make the required changes to the file:
	<ul>
		<li># Update the path to your minio NFS volume path</li>
		<li># Update &lt;NFS server host&gt; to your NFS server FQDN or IP address</li>
		<li># Change {username} to a username you want to use</li>
		<li># Change {password} &nbsp;to a password you want to use for the user</li>
		<li># Replace {GID} with your GID&nbsp;</li>
		<li># Replace {UID} with your UID</li>
		<li># Change &lt;tag&gt; to the &lt;tag&gt; value you specify in the "Download a Minio image" step</li>
	</ul>
	</li>
	<li>Install Minio:&nbsp;
	<pre><code>kubectl create -f minio.yaml</code></pre>
	</li>
	<li>Wait until all pods are running:
	<pre><code>kubectl get pods -n velero</code></pre>
	</li>
</ol>

<p>The following is the content of the&nbsp;minio.yaml file.&nbsp;</p>

<pre><code>--- 

apiVersion: v1 
kind: Namespace 
metadata: 
   name: velero 
--- 
apiVersion: v1 
kind: PersistentVolume 
metadata: 
  name: itom-dr-minio-pv 
  namespace: velero 
spec: 
  capacity: 
    storage: 2Gi                                                                         
  volumeMode: Filesystem 
  accessModes: 
    - ReadWriteMany 
  persistentVolumeReclaimPolicy: Retain 
  nfs: 
    path: /var/vols/itom/itsma/minio                                          # Update the path to your minio NFS volume path
    server: &lt;NFS server host&gt;         # Update &lt;NFS server host&gt; to your NFS server FQDN or IP address
  storageClassName: cdf-default 

--- 
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: itom-dr-minio-pvc 
  namespace: velero 
spec: 
  accessModes: 
    - ReadWriteMany 
  resources: 
    requests: 
      storage: 2Gi                                                                                  
  volumeName: itom-dr-minio-pv 
  storageClassName: cdf-default 

 --- 
apiVersion: v1 
kind: Secret 
metadata: 
  name: itom-dr-secret-minio
  namespace: velero
type: Opaque 
stringData: 
  username: {minio username}                # Change {minio username} to a username you want to use. You will use this user to install Velero.
  password: {minio password}                # Change {minio password}  to a password you want to use for the user

 --- 

apiVersion: apps/v1 
kind: Deployment 
metadata: 
  namespace: velero 
  name: minio 
  labels: 
    component: minio 
spec: 
  strategy: 
    type: Recreate 
  selector: 
    matchLabels: 
      component: minio 
  template: 
    metadata: 
      labels: 
        component: minio 
    spec: 
      securityContext: 
        runAsGroup: {GID}           # Replace {GID} with your GID 
        runAsUser: {UID}           # Replace {UID} with your UID 
      volumes: 
      - name: storage 
        persistentVolumeClaim: 
          claimName: itom-dr-minio-pvc 
      containers: 
      - name: minio 
        image: minio/minio:&lt;tag&gt;   # change &lt;tag&gt; to the &lt;tag&gt; value you specify in the "Download a&nbsp;minio&nbsp;image" step
        imagePullPolicy: IfNotPresent 
        args: 
        - server 
        - /storage 
        - --config-dir=/config 
        env: 
        - name: MINIO_ACCESS_KEY                     
          valueFrom: 
            secretKeyRef: 
              name: itom-dr-secret-minio 
              key: username 
        - name: MINIO_SECRET_KEY 
          valueFrom: 
            secretKeyRef: 
              name: itom-dr-secret-minio
              key: password                                
        ports: 
        - containerPort: 9000 
        volumeMounts: 
        - name: storage 
          mountPath: "/storage" 

--- 

apiVersion: v1 
kind: Service 
metadata: 
  namespace: velero 
  name: minio 
  labels: 
    component: minio 
spec: 
  # ClusterIP is recommended for production environments. 
  # Change to NodePort if needed per documentation, 
  # but only if you run Minio in a test/trial environment, for example with Minikube. 
  type: ClusterIP 
  ports: 
    - port: 9000 
      targetPort: 9000 
      protocol: TCP 
  selector: 
    component: minio 

--- 
apiVersion: batch/v1
kind: Job
metadata:
  namespace: velero
  name: minio-setup
  labels:
    component: minio
spec:
  template:
    metadata:
      name: minio-setup
    spec:
      restartPolicy: OnFailure
      volumes:
      - name: config
        emptyDir: {}
      containers:
      - name: mc
        image: minio/mc:&lt;tag&gt;   # change &lt;tag&gt; to the &lt;tag&gt; value you specify in the "Download a&nbsp;minio&nbsp;image" step
        imagePullPolicy: IfNotPresent
        command:
        - /bin/sh
        - -c
        - "mc --config-dir=/config config host add velero http://minio:9000 minio minio123 &amp;&amp; mc --config-dir=/config mb -p velero/velero"
        volumeMounts:
        - name: config
          mountPath: "/config"
</code></pre>

<h3>Install Velero</h3>

<ol>
	<li>Download Velero v1.4 from the <a href="https://github.com/vmware-tanzu/velero/releases">Velero download site</a>&nbsp;to a temporary directory on the control plane node. For example: <strong>/tmp/velero</strong>. Note that the installation package&nbsp;contains the velero command-line client.</li>
	<li>Extract the installation package:
	<pre><code>tar -xvf &lt;RELEASE-TARBALL-NAME&gt;.tar.gz -C /tmp/velero</code></pre>
	</li>
	<li>Copy the <strong>velero</strong> file in the package to the <strong>/usr/local/bin/</strong>&nbsp;folder.&nbsp;</li>
	<li>Create a Velero-specific credentials file (<strong>credentials-velero</strong>) in the&nbsp;<strong>/usr/local/bin/</strong>&nbsp;directory:
	<pre><code>[default]
aws_access_key_id = &lt;minio username&gt;
aws_secret_access_key = &lt;minio password&gt;</code></pre>

	<p>Where: &lt;minio username&gt; and &lt;minio password&gt; are the minio credentials that you have specified&nbsp;in the minio.yaml file.&nbsp;</p>
	</li>
	<li>Install Velero:
	<pre><code>velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.0.0 \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://minio.velero.svc:9000</code></pre>
	</li>
	<li>Check to see that both the Velero deployment&nbsp;has been&nbsp;successfully created:
	<pre><code>kubectl get deployments -l component=velero --namespace=velero</code></pre>
	</li>
</ol>

<h2>Back up and restore k8s objects</h2>

<p>Perform the following steps by using Velero.</p>

<h3>Backup</h3>

<p>To back up objects, run the following command on the control plane node on which Velero is installed:</p>

<pre><code>velero backup create &lt;backup name&gt; --include-namespaces &lt;namespace&gt; --wait</code></pre>

<p>The following are examples to back up objects for the suite and CDF, respectively:</p>

<pre><code>velero backup create itsma-backup --include-namespaces itsma-fghnd --wait
velero backup create core-backup --include-namespaces core --wait
</code></pre>

<h3>Restore</h3>

<p>The following procedures assume that you have removed the suite namespace or the CDF core namespace. To test the procedure, you can run&nbsp;<strong>kubectl delete ns &lt;namespace&gt;</strong> to delete a namespace.</p>

<h4>Restore the objects for the suite</h4>

<p>Perform the following steps:</p>

<ol>
	<li>Once the suite namespace is deleted, the associated PVs are released. Before the restore, delete the released PVs by running the following command on the control plane node:
	<pre><code>kubectl get pv |grep -i release | awk '{print $1}' | xargs kubectl delete pv</code></pre>
	</li>
	<li>Run the following command to restore:
	<pre><code>velero restore create --from-backup &lt;suite backup name&gt; --wait</code></pre>

	<p>For example:</p>

	<pre><code>velero restore create --from-backup itsma-backup --wait</code></pre>
	</li>
	<li>Change the nodePort in&nbsp;itom-nginx-ingress-svc to 443:
	<ol start="1" style="list-style-type: lower-alpha;">
		<li>Run the following command:
		<pre><code>kubectl edit svc itom-nginx-ingress-svc -n &lt;suite namespace&gt;</code></pre>
		</li>
		<li>Change nodePort to 443 as shown below:
		<pre><code>- name: https-port
    nodePort: 443
    port: 443
    protocol: TCP
    targetPort: 443
</code></pre>
		</li>
	</ol>
	</li>
</ol>

<h4>Restore the objects for CDF</h4>

<ol>
	<li>Delete the PVs associated with the core namespace:
	<pre><code>kubectl delete pv db-single itom-logging itom-vol</code></pre>
	</li>
	<li>Restore with Velero:
	<pre><code>velero restore create --from-backup &lt;core backup&gt; –wait</code></pre>

	<p>For example:</p>

	<pre><code>velero restore create --from-backup core-backup –wait</code></pre>
	</li>
</ol>
</html>