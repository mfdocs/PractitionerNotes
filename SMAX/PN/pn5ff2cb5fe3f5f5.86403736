<html><p>This topic describes how you can use Velero,&nbsp;an open-source tool, to back up and restore Kubernetes cluster resources and persistent volumes in an on-premises environment. This is helpful when you destroyed some Kubernetes resources for whatever reason, for example, when you delete the suite namespace accidentally.&nbsp;</p>

<div class="Admonition_Important"><span class="autonumber">Note</span> This tool does not back up database data and NFS data.&nbsp;</div>

<h2>Export an NFS directory</h2>

<p>On your&nbsp;NFS server, export one NFS directory.</p>

<ol>
	<li>Create one directory under the base directory. For example, if your existing NFS directories share the base directory /var/vols/itom, run the following command:
	<pre><code>mkdir -p  /var/vols/itom/minio</code></pre>
	</li>
	<li>Change the permission of the directory:
	<pre><code>chmod -R 755 /var/vols/itom/minio</code></pre>
	</li>
	<li>Change the ownership of the directory (change 1999:1999 to your own UID:GID if you use custom values):
	<pre><code>chown -R 1999:1999 /var/vols/itom/minio</code></pre>
	</li>
	<li><span style="font-size:12.0pt"><span style="font-family:&quot;Calibri&quot;,sans-serif">In the /etc/exports file, export the NFS directory by adding one line&nbsp;</span></span>(change 1999 to your own UID or GID if you use a custom value for them):&nbsp;
	<pre><code>/var/vols/itom/minio *(rw,sync,anonuid=1999,anongid=1999,root_squash)</code></pre>
	</li>
	<li>Run the following command:
	<pre><code>exportfs -ra</code></pre>
	</li>
	<li>Run the following command to check that the directory is exported:
	<pre><code>showmount -e | grep minio</code></pre>
	</li>
</ol>

<h2>Download the minio&nbsp;images</h2>

<p>If your control plane nodes (formerly known as "master nodes") have Internet access, download the image from a control plane&nbsp;node; otherwise, download the image from another Linux machine that has Internet access and then transfer the image to the control plane node.</p>

<ol>
	<li>On the download machine, navigate to a directory where you want to download the images, and then run the following commands&nbsp;:
	<pre><code>docker pull minio/minio:latest&nbsp;
ocker pull  minio/mc:latest</code></pre>
	</li>
	<li lang="en-US">If the control plane node has no Internet access, transfer&nbsp;the images to the control plane node.&nbsp;</li>
	<li lang="en-US">Obtain the image IDs.
	<ol start="1" style="list-style-type: lower-alpha;">
		<li>Run the following command:&nbsp;
		<pre><code>docker images |grep minio</code></pre>
		</li>
		<li>In the output, find the&nbsp;IDs of the&nbsp;images. In the following example, it's&nbsp;8dbf9ff992d5.
		<pre><code>docker.io/minio/minio latest 8dbf9ff992d5 30 hours ago 183 MB</code></pre>
		</li>
	</ol>
	</li>
	<li lang="en-US">Run the following command to tag one&nbsp;image:
	<pre><code>docker tag &lt;image ID&gt; &lt;image registry URL&gt;/&lt;organization name&gt;/minio:&lt;tag&gt;</code></pre>

	<p>The following are two examples:</p>

	<pre><code>docker tag 8dbf9ff992d5 myregistry.azurecr.io/sandbox/minio:test
docker tag 8dbf9ff992d5 localhost:5000/hpeswitom/minio:test</code></pre>

	<ul>
		<li>
		<p>&lt;image ID&gt;: the image ID you obtained in the previous step.</p>
		</li>
		<li>
		<p>&lt;image registry URL&gt;/&lt;organizaition name&gt;: your image registry URL/organization name. If using the local registry, it's localhost:5000/hpeswitom; if using an external registry, ask your registry administrator for it.</p>
		</li>
		<li>&lt;tag&gt;: specify any value you like.&nbsp;</li>
	</ul>
	</li>
	<li lang="en-US">Repeat the step above&nbsp;to tag the other image (minio/mc:latest) into your image registry.&nbsp;</li>
	<li lang="en-US">Push the images into your image registry:
	<pre><code>docker push &lt;image registry URL&gt;/&lt;organization name&gt;/minio:&lt;tag&gt;
docker push &lt;image registry URL&gt;/&lt;organization name&gt;/mc:&lt;tag&gt;</code></pre>

	<p>Example 1:</p>

	<pre><code>docker push myregistry.azurecr.io/sandbox/minio:test
docker push myregistry.azurecr.io/sandbox/mc:test

</code></pre>

	<p>Example 2:</p>

	<pre><code>docker push localhost:5000/hpeswitom/minio:test
docker push localhost:5000/hpeswitom/mc:test</code></pre>
	</li>
</ol>

<h2>Install Velero on a control plane node</h2>

<p>Perform the following steps to install Velero on the control plane node.</p>

<ol>
	<li>Download the tarball of the latest Velero release to a temporary directory on the control plane node. The download URL is&nbsp;https://github.com/vmware-tanzu/velero/releases/.</li>
	<li>Extract the package:
	<pre><code>tar -xvf &lt;release-tarball-name&gt;.tar.gz</code></pre>

	<p>The directory you extracted is called the “Velero directory” in subsequent steps.&nbsp;</p>
	</li>
	<li>Move the Velero binary from the Velero directory to somewhere in your&nbsp;PATH. For example:
	<pre><code>cp velero /usr/local/bin/</code></pre>
	</li>
	<li>Create a Velero-specific credentials file in your local directory. For example, in the Velero directory:
	<pre><code>cat &lt;&lt;ENDFILE &gt; ./credentials-velero
[default]
aws_access_key_id = minio
aws_secret_access_key = minio123
ENDFILE
</code></pre>
	</li>
	<li>Navigate to the Velero directory, and create a backup copy of the&nbsp;examples/minio/00-minio-deployment.yaml file. This is because in the next steps, you will need to edit this file.&nbsp;</li>
	<li>Edit the 00-minio-deployment.yaml file as follows:
	<ol start="1" style="list-style-type: lower-alpha;">
		<li>Add PVs/PVCs to the 00-minio-deployment.yaml file by appending the following code lines to the end of the file (replace $minio-NFS-path and $NFS-server-FQDN with the exported NFS folder path and hostname of the NFS server):
		<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: minio-pv-claim
  namespace: velero
spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: $minio-NFS-path
    server: $NFS-server-FQDN
   
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-pv-claim
  namespace: velero
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  volumeName: minio-pv-claim
</code></pre>
		</li>
		<li>In the Deployment section, make the following change:
		<p>From:</p>

		<pre><code>volumes:
      - name: storage
        emptyDir: {}
      - name: config
        emptyDir: {}
</code></pre>

		<p>To:</p>

		<pre><code>volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: minio-pv-claim
</code></pre>
		</li>
		<li>Remove the last two lines in the Deployment section below:
		<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: velero
  name: minio
  labels:
    component: minio
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      component: minio
  template:
    metadata:
      labels:
        component: minio
    spec:
      volumes:
      - name: storage
        emptyDir: {}
      - name: config
        emptyDir: {}
      containers:
      - name: minio
        image: minio/minio:latest
        imagePullPolicy: IfNotPresent
        args:
        - server
        - /storage
        - --config-dir=/config
        env:
        - name: MINIO_ACCESS_KEY
          value: "minio"
        - name: MINIO_SECRET_KEY
          value: "minio123"
        ports:
        - containerPort: 9000
        volumeMounts:
        - name: storage
          mountPath: "/storage"
        - name: config
          mountPath: "/config"</code></pre>
		</li>
		<li>Replace the image values in the following lines with the images in your image registry:
		<pre><code>image: minio/minio:latest 
image: minio/mc:latest </code></pre>
		For example, change them as shown below (this example uses an external registry):

		<pre><code>image: myregistry.azurecr.io/sandbox/minio:test 
image: myregistry.azurecr.io/sandbox/mc:test</code></pre>
		</li>
	</ol>
	</li>
	<li>Run the following commands:
	<pre><code>kubectl apply -f examples/minio/00-minio-deployment.yaml
velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.0.0 \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://minio.velero.svc:9000
</code></pre>
	</li>
</ol>

<h2>Back up and restore k8s objects</h2>

<p>Perform the following steps by using Velero.</p>

<h3>Backup</h3>

<p>To back up objects, run the following command on the control plane node on which Velero is installed:</p>

<pre><code>velero backup create &lt;backup name&gt; --include-namespaces &lt;namespace&gt; --wait</code></pre>

<p>The following are examples to back up objects for the suite and CDF, respectively:</p>

<pre><code>velero backup create itsma-backup --include-namespaces itsma-fghnd --wait
velero backup create core-backup --include-namespaces core --wait
</code></pre>

<h3>Restore</h3>

<p>The following procedures assume that you have removed the suite namespace or the CDF core namespace. To test the procedure, you can run&nbsp;<strong>kubectl delete ns &lt;namespace&gt;</strong> to delete a namespace.</p>

<h4>Restore the objects for the suite</h4>

<p>Perform the following steps:</p>

<ol>
	<li>Once the suite namespace is deleted, the associated PVs are released. Before the restore, delete the released PVs by running the following command on the control plane node:
	<pre><code>kubectl get pv |grep -i release | awk '{print $1}' | xargs kubectl delete pv</code></pre>
	</li>
	<li>Run the following command to restore:
	<pre><code>velero restore create --from-backup &lt;suite backup name&gt; --wait</code></pre>

	<p>For example:</p>

	<pre><code>velero restore create --from-backup itsma-backup --wait</code></pre>
	</li>
	<li>Change the nodePort in&nbsp;itom-nginx-ingress-svc to 443:
	<ol start="1" style="list-style-type: lower-alpha;">
		<li>Run the following command:
		<pre><code>kubectl edit svc itom-nginx-ingress-svc -n &lt;suite namespace&gt;</code></pre>
		</li>
		<li>Change nodePort to 443 as shown below:
		<pre><code>- name: https-port
    nodePort: 443
    port: 443
    protocol: TCP
    targetPort: 443
</code></pre>
		</li>
	</ol>
	</li>
</ol>

<h4>Restore the objects for CDF</h4>

<ol>
	<li>Delete the PVs associated with the core namespace:
	<pre><code>kubectl delete pv db-single itom-logging itom-vol</code></pre>
	</li>
	<li>Restore with Velero:
	<pre><code>velero restore create --from-backup &lt;core backup&gt; –wait</code></pre>

	<p>For example:</p>

	<pre><code>velero restore create --from-backup core-backup –wait</code></pre>
	</li>
</ol>
</html>