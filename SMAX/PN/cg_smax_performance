<div>
<p><br>
In order to keep your SMA system running at peak potential, or to correct a slow or unstable system, there are many methods available to track and improve performance. Such methods include:</p>

<ul>
	<li>Monitoring the Operating System (OS)</li>
	<li>Monitoring the Postgres database</li>
	<li>Applying the latest SMA patch</li>
	<li>Using the thinpool</li>
	<li>Tuning the database (must be done for each tenant)</li>
	<li>Tuning the pods</li>
	<li>Rebalancing the pods regularly</li>
	<li>Adding more workers</li>
</ul>

<p>This document examines each method and gives examples and suggestions for maintaining peak application performance.</p>

<div>
<div>&nbsp;</div>
</div>

<h2>Monitoring the Operating System</h2>

<h3>Setting a baseline</h3>

<p>Before making any changes, you need to set up a baseline in order to measure the effect of each change.</p>

<p>To set up the baseline, you must monitor the system for a few days and save the results. When you encounter any new issue or apply a change to the system, you can use this baseline to compare the before/after results and understand the impact to the system.</p>

<p>There are several layers to monitor:</p>

<ul>
	<li>Operating System</li>
	<li>Kubernetes Cluster</li>
	<li>Postgres Database</li>
	<li>SMA Application</li>
</ul>

<h3>Monitoring Tools</h3>

<p>There are many monitoring tools that you may use to monitor your SMAX system, such as the Micro Focus OpsB suite. If you do not have a monitoring tool, there are open source Linux tools that you may use.</p>

<p>The tool will need to monitor the Operating Systems for the master nodes, worker nodes, nfs, and Postgres.</p>

<p>If necessary, Install the open source Linux monitoring tools on these systems:</p>

<pre>yum install -y sysstat
</pre>

<p>Change the data collection time from 10 minutes to 1 minute. To do this, edit the <strong>sysstat</strong> configuration file:</p>

<pre>vi /etc/cron.d/sysstat
</pre>

<p>Change the line that reads</p>

<div>
<pre>*/10 * * * * root /usr/lib64/sa/sa1 1 1
53 23 * * * root /usr/lib64/sa/sa2 -A
</pre>
</div>

<p>to</p>

<div>
<pre>*/1 * * * * root /usr/lib64/sa/sa1 1 1
53 23 * * * root LANG=en_US; /usr/lib64/sa/sa2 -A -p
</pre>
</div>

<p>By default, the logs will be written to the <em>/var/log/sa</em> directory, and the tool will collect the last 31 days of system information. To save system disk space, change the compress parameter with the following command:</p>

<pre>vi /etc/sysconfig/sysstat
</pre>

<p>Change the line that reads</p>

<div>
<pre>COMPRESSAFTER=31
ZIP="bzip2"
</pre>
</div>

<p>to</p>

<div>
<pre>COMPRESSAFTER=7
ZIP="gzip"
</pre>
</div>

<p>To help in the analysis of the monitoring result, you may wish to combine all the monitoring results into a single file with the following command:</p>

<pre>cat /var/log/sa/sar?? &gt; /tmp/sar.all
</pre>

<p>You can send the sar.all file to Micro Focus support, who can help you in the analysis. You can also analyze the file yourself with the <strong>ksar</strong> application (an open source graphing tool). You can download <strong>ksar</strong> 5.2.4 or later from <a href="https://github.com/vlsi/ksar/releases" target="1">https://github.com/vlsi/ksar/releases</a>: <a href="https://github.com/vlsi/ksar/releases" target="1">https://github.com/vlsi/ksar/releases</a> To use ksar, run using java with the following command (please use 64bit Java):</p>

<pre>java -jar ksar*.jar
</pre>

<p>and select sar.all from the GUI for analysis.</p>

<p>In the report, check for the following indicators:</p>

<ul>
	<li>CPU -&gt; all &lt; 95%, the lower the better, high CPU usage indicates not enough CPU resource or not balanced cluster;</li>
	<li>Devices -&gt; avgqu-sz &lt; 20, the lower the better, high disk queue size indicates disk IO may be the bottleneck;</li>
	<li>Devices -&gt; await &lt; 50 ms, the lower the better, high disk wait time indicates disk IO may be the bottleneck;</li>
	<li>Load -&gt; runq-sz &lt; No. cpu * 2, the lower the better, high run queue indicates not enough CPU resource;</li>
	<li>Swap -&gt; pswpout/s &lt; 100, the lower the better, high page out indicates not enough RAM;</li>
	<li>Swap -&gt; pswpin/s &lt; 100, the lower the better, high page in indicatss not enough RAM.</li>
</ul>

<p>As the default sysstat cannot monitor the available memory, you need to monitor the available memory by adding the <strong>free</strong> command into the cron job as below. You may need to clean up the free.log every month or so.</p>

<pre>*/1 * * * * date +"\%F \%X" &gt;&gt; /tmp/free.log&nbsp;; free -m -w &gt;&gt; /tmp/free.log
</pre>

<p>This will collect the available memory every minute. The "free" memory reported by free command may be lower than 100MB. This is normal. However, if the "available" memory is lower than 100 MB it may indicate the system does not have enough RAM. You can also double check the swap in (pswpin/s) and swap out (pswpout/s) collected by sysstat to make sure the RAM is the bottleneck.</p>

<h2>Monitoring the Kubernetes cluster</h2>

<p>Micro Focus has a script for checking the resources within the Kubernetes cluster named check_resource.py. This script can highlight areas of low resources or failing pods, making it easier to determine if more resources are needed or if specific pods need to be moved to another node. This script can be run every 15 minutes. You can download the check_resource.py from <a href="https://marketplace.microfocus.com/itom/content/service-management-automation-2019-02" target="1">Micro Focus ITOM Marketplace</a> Diagnostic Toolkit.</p>

<p>Before using the check_resource.py, it is recommended that you review the <a href="./itom/SMAX:2020.02/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube">Monitoring and tuning resource utilization in a Kubernetes cluster</a> document. This document contains a great deal of information regarding how to use the script to help check, tune, and monitor the resources in the Kubernetes cluster.</p>

<h2>Monitoring Postgres</h2>

<p>Postgres has built-in tools for performance monitoring. To enable this functionality, change the following parameters in the postgres configuration file postgresql.conf:</p>

<div>
<pre>shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
</pre>
</div>

<p>After making these changes, it is necessary to reboot Postgres to activate the monitoring. In the psql command line interface, type the following command:</p>

<div>
<pre>CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
\x auto
\pset format wrapped
</pre>
</div>

<p>In order to view the top queries related to CPU, execute the following:</p>

<div>
<pre>SELECT * FROM pg_stat_statements ORDER BY total_time  DESC LIMIT 20;
</pre>
</div>

<p>To check the top queries related to disk:</p>

<div>
<pre>SELECT * FROM pg_stat_statements ORDER BY shared_blks_dirtied  DESC LIMIT 20;
</pre>
</div>

<p>After the monitoring has been enabled, you can use the following psql commands to view the results. To check the current running queries:</p>

<div>
<pre>SELECT now() - query_start as "runtime", usename, datname, state, query
  FROM  pg_stat_activity
  WHERE state='active'
 ORDER BY runtime DESC;
</pre>
</div>

<p>To check the database connections:</p>

<div>
<pre>SELECT datname, count(1) FROM  pg_stat_activity group by datname order by 2 desc;
</pre>
</div>

<p>To check the dead tuples, use the following command. You need to check for dead tuples in different databases including <strong>xservices_ems</strong> and <strong>xservices_rms</strong>.</p>

<div>
<pre>select relname, n_dead_tup, last_vacuum, last_autovacuum from
pg_catalog.pg_stat_all_tables
where n_dead_tup &gt; 0 order by n_dead_tup desc;
</pre>
</div>

<p>If you found some tables have more than 1M dead tuples, you should run the full vacuum on it. Having dead tuples in the database is similar to having a fragmented hard disk. The full vacuum is like a full disk defragment, it can release the disk usage and improve performance.</p>

<div>
<pre>VACUUM FULL FREEZE VERBOSE &lt;yourTableName&gt;;
ANALYZE &lt;yourTableName&gt;;
</pre>
</div>

<p>Then check the dead tuples again with previous query on pg_stat_all_tables. If you find slt_targets_&lt;yourtenantid&gt; table has lots of dead tuples, run the cluster command every night. You can find the commands in the Database tuning section.</p>

<p>To check the disk space usage (again check the different databases like <strong>xservices_ems</strong> and <strong>xservices_rms</strong>):</p>

<div>
<pre>SELECT *, pg_size_pretty(total_bytes) AS total
    , pg_size_pretty(index_bytes) AS INDEX
    , pg_size_pretty(toast_bytes) AS toast
    , pg_size_pretty(table_bytes) AS TABLE
  FROM (
  SELECT *, total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes FROM (
      SELECT c.oid,nspname AS table_schema, relname AS TABLE_NAME
              , c.reltuples AS row_estimate
              , pg_total_relation_size(c.oid) AS total_bytes
              , pg_indexes_size(c.oid) AS index_bytes
              , pg_total_relation_size(reltoastrelid) AS toast_bytes
          FROM pg_class c
          LEFT JOIN pg_namespace n ON n.oid = c.relnamespace
          WHERE relkind = 'r' and nspname NOT IN ('pg_catalog', 'information_schema')
  ) a
) a order by row_estimate desc;
</pre>
</div>

<h2>Monitoring the application</h2>

<p>Simulating common actions using a monitoring tool or a script will allow you to collect a baseline response time. Over time this baseline can be used to compare to the same actions, and warn you when the system is not performing well.</p>

<h2>Tuning tips</h2>

<p>To have a more stable and better performing system, please check the following.</p>

<h3>Applying the latest SMA Patch</h3>

<p>It is highly recommended that customers keep their implementation running on the most current version available. Using the latest version will help to ensure reliability of the product and improve the support process when issues do arise. Most importantly, always apply the latest patch for your release.</p>

<h3>OS tuning</h3>

<p>Make sure to run the ostuning script documented here:</p>

<p><a href="https://docs.microfocus.com/itom/SMAX:2019.11/AWSOSTuningScript" target="1">https://docs.microfocus.com/itom/SMAX:2019.11/AWSOSTuningScript</a></p>

<p>And if the CPU usage is higher than 80% or runq-sz is higher than 2 * cpu count, you may need to increase the CPU.</p>

<p>If avgqu-sz or await is higher than we mentioned in the monitoring section, you may increase the IOPS of your disk. A SSD is highly recommended for the Postgres server.</p>

<p>If the available memory reported by "free -h" is too low like less than 100MB or pswpout/s, pswpin/s is higher than we mentioned in the monitoring section, you may increase the RAM.</p>

<h3>Pod tuning</h3>

<p>Check the pod resource usage with the check_resource.py script mentioned earlier. If some pods have very high CPU or RAM usage (more than 85%), you may need to increase the pod resource limits. The following commands are commonly used in some customer environments to increase specific pod resource limits. Depending on the pod that is running high in CPU or RAM usage, you may need to change additional pods settings not listed here. Another indicator that pod settings may need to change is if users are experiencing http 504/502/500 errors when using the application.</p>

<p><strong>Note:</strong> For following commands, you need to change the namespace <strong>itsma1</strong> to your own namespace in your environment.</p>

<h4>Gateway pod</h4>

<p>Increase the resource limit of gateway pod: 2GB -&gt; 4GB, increase the replicas 50%:</p>

<div>
<pre>kubectl patch deploy/itom-xruntime-gateway -n itsma1 -p '{"spec": {"template": {"spec": {"containers": [{"name": "gateway","resources": {"limits": {"memory": "4096M"}}}]}}}}'
</pre>
</div>

<p>For version before 2019.02.001, reduce the tomcat connection timeout for gateway from 180s to 20s,</p>

<div>
<pre>kubectl patch deploy/itom-xruntime-gateway -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"gateway","env":[{"name":"TOMCAT_CONNECTOR_TIMEOUT","valueFrom":null,"value":"20000"}]}]}}}}'
</pre>
</div>

<h4>Platform pods</h4>

<p>Increase the resource limit of platform pod: 4GB -&gt; 8GB:</p>

<div>
<pre>kubectl patch deploy/itom-xruntime-platform -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"itom-xruntime-platform","resources": {"limits": {"memory": "8192M"}}}]}}}}'

kubectl patch deploy/itom-xruntime-platform-offline -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"itom-xruntime-platform","resources": {"limits": {"memory": "8192M"}}}]}}}}'
</pre>
</div>

<h4>Idm/bo-login pods</h4>

<p>If you find idm and/or bo-login pods are not stable: Increase the resource limit of idm pod: 2CPU -&gt; 4CPU, 2GB -&gt; 6GB, JVM 1.5/2GB -&gt; 5GB"</p>

<div>
<pre>kubectl patch deploy/idm -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"idm","resources": {"limits": {"cpu": "4","memory": "6144M"}}}]}}}}'

kubectl patch deploy/idm -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"idm","env":[{"name":"JVM_MAX_SIZE","value":"5120M"},{"name":"JVM_MIN_SIZE","value":"5120M"}]}]}}}}'
</pre>
</div>

<p>Increase the resource limit of bo-login pod: 1.5GB -&gt; 4GB</p>

<div>
<pre>kubectl patch deploy/itom-bo-login-deployment -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"itom-bo-login","resources": {"limits": {"memory": "4096M"}}}]}}}}'
</pre>
</div>

<h4>Rabbitmq pod</h4>

<p>Increase the resource limit of rabbitmq pod: 1.5CPU -&gt; 2CPU, 30GB -&gt; 4GB; and change the replicas to 1</p>

<div>
<pre>kubectl patch statefulsets/infra-rabbitmq -n itsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"itom-xruntime-rabbitmq","resources": {"limits": {"cpu": "2", "memory": "4096M"}}}]}}}}'
kubectl patch statefulsets/infra-rabbitmq -n itsma1 -p '{"spec":{"replicas": 1}}'
</pre>
</div>

<p><br>
<strong>Note:</strong> The above "kubectl patch" commands are used as an easier way to change the settings, but you can always manually change the deployment settings by using the "kubectl edit ..." command. For example:</p>

<pre>kubectl edit deploy/itom-xruntime-platform -n itsma1
</pre>

<p>And in the configuration, manually change the RAM from 4096M to 8192M.</p>

<h3>Database tuning</h3>

<p>The following section contains information for tuning the Postgres database. <strong>Important:</strong> These steps should be performed for <strong>each tenant</strong>. As you add more tenants into the environment, you need to perform these steps again for the each new tenant.</p>

<h4>Disk speed</h4>

<p>Make sure your Postgres server runs on a dedicate SSD with at least 1500 IOPS. For the best customer experience, 3000 IOPS is recommended.</p>

<h4>Auto vacuum</h4>

<p>Make sure the auto vacuum is enabled for all the tables except slt_targetes_&lt;yourtenantid&gt;, following command should return setting = on:</p>

<div>
<pre>SELECT name, setting from pg_settings WHERE name like 'autovacuum';
</pre>
</div>

<h4>Indexing the OpbTaskDataJSON table</h4>

<p>If in your top queries, you find queries like:</p>

<div>
<pre>SELECT body FROM "OpbTaskDataJSON_&lt;yourtenantid&gt;" WHERE (((body-&gt;'EndPoint'@&gt;'{"AgentId":"2790231d052b3691c0cc05da"}') AND (body-&gt;'Task'@&gt;'{"IsHandled":false}')) AND ((body-&gt;'Task'-&gt;'EndPointTypeId' != '"platform"') OR (body-&gt;'Task'-&gt;'EndPointTypeId' IS NULL))) ORDER BY body-&gt;'Task'-&gt;'Configuration'-&gt;'Priority' ASC,body-&gt;'Task'-&gt;'SubmitTime' ASC LIMIT 2000;
</pre>
</div>

<p>Create a new index for the OpbTaskDataJSON with the following command:</p>

<div>
<pre>\c xservices_rms
create index   "mf_opbindex" on "OpbTaskDataJSON_&lt;yourtenantid&gt;" using btree (((body -&gt; 'Task'::text) -&gt; 'AgentId'::text)) WHERE (body -&gt; 'Task'::text) @&gt; '{"IsHandled": false}'::jsonb;
</pre>
</div>

<p>To ensure the new index works, run following command:</p>

<div>
<pre>explain analyze SELECT body FROM "OpbTaskDataJSON_&lt;yourtenantid&gt;" WHERE (((body-&gt;'EndPoint'@&gt;'{"AgentId":"2790231d052b3691c0cc05da"}') AND (body-&gt;'Task'@&gt;'{"IsHandled":false}')) AND ((body-&gt;'Task'-&gt;'EndPointTypeId' != '"platform"') OR (body-&gt;'Task'-&gt;'EndPointTypeId' IS NULL))) ORDER BY body-&gt;'Task'-&gt;'Configuration'-&gt;'Priority' ASC,body-&gt;'Task'-&gt;'SubmitTime' ASC LIMIT 2000;
</pre>
</div>

<p>it should return:</p>

<pre>… Index Scan using mf_opbindex …
</pre>

<h4>Tuning the slt_targets related queries</h4>

<p>If in your top queries, you found queries like below,</p>

<div>
<pre>UPDATE slt_targets_&lt;yourtenantid&gt; SET elapsed_duration=....
</pre>
</div>

<p>And your version is before 2019.05, you can do the following,</p>

<h5>1. Schedule below command regularly via cron job:[<a href="/mediawiki/index.php?title=SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_practices_troubleshooting/cg_smax_performance&amp;veaction=edit&amp;section=20">edit</a> | <a href="/mediawiki/index.php?title=SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_practices_troubleshooting/cg_smax_performance&amp;action=edit&amp;section=20">edit source</a>]</h5>

<p>It is recommended that you run it once for all your tenants and then schedule the commands to run every night.</p>

<div>
<pre>\c xservices_ems
cluster slt_targets_&lt;yourtenantid&gt; using slt_targets_&lt;yourtenantid&gt;_sltd_id_idx;
analyze slt_targets_&lt;yourtenantid&gt;
</pre>
</div>

<h5>2. Add indexes:[<a href="/mediawiki/index.php?title=SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_practices_troubleshooting/cg_smax_performance&amp;veaction=edit&amp;section=21">edit</a> | <a href="/mediawiki/index.php?title=SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_practices_troubleshooting/cg_smax_performance&amp;action=edit&amp;section=21">edit source</a>]</h5>

<div>
<pre>\c xservices_ems
create index mf_slt_targets_&lt;yourtenantid&gt;_slt_update2 on public.slt_targets_&lt;yourtenantid&gt; (should_update, is_closed,is_deleted, active);
create index mf_slt_targets_&lt;yourtenantid&gt;_slt_update on public.slt_targets_&lt;yourtenantid&gt; (is_closed,is_deleted, effective_target);
CREATE INDEX mf_slt_tud_&lt;yourtenantid&gt; ON slt_tud_&lt;yourtenantid&gt; USING gist (tud_range);
CREATE INDEX mf_slt_tud_&lt;yourtenantid&gt;_start ON slt_tud_&lt;yourtenantid&gt;  (start_range, range_cost);
CREATE INDEX mf_slt_tud_&lt;yourtenantid&gt;_end ON slt_tud_&lt;yourtenantid&gt;  (end_range, range_cost);
analyze slt_targets_&lt;yourtenantid&gt;;
analyze slt_tud_&lt;yourtenantid&gt;
</pre>
</div>

<h5>3. turn off auto vacuum on slt_targets table:[<a href="/mediawiki/index.php?title=SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_practices_troubleshooting/cg_smax_performance&amp;veaction=edit&amp;section=22">edit</a> | <a href="/mediawiki/index.php?title=SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_practices_troubleshooting/cg_smax_performance&amp;action=edit&amp;section=22">edit source</a>]</h5>

<div>
<pre>\c xservices_ems
ALTER TABLE slt_targets_&lt;yourtenantid&gt; SET (autovacuum_enabled = false);
</pre>
</div>

<h4>Handling other top queries</h4>

<p>If you found top queries like:</p>

<div>
<pre>SELECT body FROM "FileInfoTenantEntity_&lt;yourtenantid&gt;" ...

SELECT ... FROM "ReportConfig_&lt;yourtenantid&gt;" ...

... entities_&lt;yourtenantid&gt; Request INNER JOIN slt_record_status_&lt;yourtenantid&gt; SLT ...
</pre>
</div>

<p>For 2018.05.x, you can run following tuning queries:</p>

<div>
<pre>\c xservices_rms
CREATE OR REPLACE FUNCTION drop_gin_indexes_onbody() RETURNS INTEGER AS $$
DECLARE
  i RECORD;
  error varchar;
BEGIN
  FOR i IN
    (SELECT relname FROM pg_class
       -- exclude all pkey, exclude system catalog which starts with 'pg_' and end with 'gin'
      WHERE relkind = 'i' AND relname NOT LIKE '%_pkey%' AND relname NOT LIKE 'pg_%' and relname like '%_gin')
  LOOP
    BEGIN
        RAISE INFO 'DROP INDEX %;', i.relname;
        EXECUTE 'DROP INDEX ' || i.relname;
    EXCEPTION WHEN OTHERS THEN
        GET STACKED DIAGNOSTICS error = MESSAGE_TEXT;  
        RAISE NOTICE '%', error;
    END;
  END LOOP;
RETURN 1;
END;
$$ LANGUAGE plpgsql;


CREATE OR REPLACE FUNCTION create_gin_index() RETURNS INTEGER AS $$
DECLARE
  i RECORD;
  statement varchar;
  error varchar;
BEGIN
  FOR i IN
    (select indexdef from pg_indexes where schemaname = 'public' and indexdef like 'CREATE UNIQUE INDEX%btree%body%')
  LOOP
  --build the new index statement
    BEGIN
        statement=replace(i.indexdef, 'CREATE UNIQUE INDEX ', 'CREATE INDEX if not exists mf_');
        statement=replace(statement, 'btree', 'gin');
        RAISE INFO '%', statement;
        EXECUTE statement;
    EXCEPTION WHEN OTHERS THEN
        GET STACKED DIAGNOSTICS error = MESSAGE_TEXT;  
        RAISE NOTICE '%', error;
    END;
  END LOOP;
RETURN 1;
END;
$$ LANGUAGE plpgsql;

Select drop_gin_indexes_onbody();
Select create_gin_index();

\c xservices_ems

create index mf_entities_&lt;yourtenantid&gt;_sbigint0 on public.entities_&lt;yourtenantid&gt;(upper(schar8), entity_type_id, sbigint0, is_deleted );

create index mf_entities_&lt;yourtenantid&gt;_schar9_sbigint5 on public.entities_&lt;yourtenantid&gt;(upper(schar9), entity_type_id, sbigint5, is_deleted );

create unique index mf_slt_record_status_&lt;yourtenantid&gt;_slt on public.slt_record_status_&lt;yourtenantid&gt;( status,is_deleted, entity_id);

create unique index mf_long_text_&lt;yourtenantid&gt;_entity_id_field_id_idx on long_text_&lt;yourtenantid&gt;  (entity_id, field_id);

analyze long_text_&lt;yourtenantid&gt;;

analyze entities_&lt;yourtenantid&gt;;

analyze slt_record_status_&lt;yourtenantid&gt;;
</pre>
</div>

<p>For 2018.08/11, you can run following tuning queries:</p>

<div>
<pre>\c xservices_ems

create index mf_entities_&lt;yourtenantid&gt;_sbigint0 on public.entities_&lt;yourtenantid&gt;(upper(schar8), entity_type_id, sbigint0, is_deleted );

create index mf_entities_&lt;yourtenantid&gt;_schar9_sbigint5 on public.entities_&lt;yourtenantid&gt;(upper(schar9), entity_type_id, sbigint5, is_deleted );

analyze long_text_&lt;yourtenantid&gt;;

analyze entities_&lt;yourtenantid&gt;;
</pre>
</div>

<h4>Tune Audit queries</h4>

<p>If you found top queries like:</p>

<div>
<pre>INSERT INTO "Audit_&lt;yourtenantid&gt;" (id,body) ...
</pre>
</div>

<p>Or if there are more than 100 active database connections are running above query. You can improve performance by truncating the Audit tables regularly (such as every week). The Audit tables record the detailed action histories, and you can safely remove all the records as the records in Audit table cannot be viewed in the GUI.</p>

<div>
<pre>\c xservices_rms
truncate table "Audit_&lt;yourtenantid&gt;";
</pre>
</div>

<h4>Tune reports</h4>

<p>If you want to have complex report with <a href="https://docs.microfocus.com/">Postgres Views</a>, make sure to run the reporting on a mirror database. This can help avoid overuse of the primary database server.</p>

<p>For analytics reports in the system, if you have more than 20 hourly or daily reports, you need to tune the queries for your reports. You can find the top queries and add the indexes accordingly.</p>

<p>Please also carefully review the reports to determine if you can reduce the frequency (from hourly to daily or from daily to monthly). You should also disable unused reports. For example, if you don't use the Idea module, disable the reports of that module.</p>

<p>You can change the running time of the report to a time where the system is not busy (such as late at night). First, you need to query of the current report execution time as shown below. The time will be returned in UTC time.</p>

<div>
<pre>select extract(hour from to_timestamp(to_number(r.body-&gt;&gt; 'NextCalculationTime','99999999999999')/1000)) "runtimeInUTC", count(1) from "ReportRuntime_&lt;yourtenantid&gt;" r, "ReportConfig_&lt;yourtenantid&gt;" c where r.body-&gt;&gt;'ConfigId' =c.id and r.body-&gt;'IsScheduled' @&gt; 'true' and r.body-&gt;'Status' @&gt; '"READY"' group by extract(hour from to_timestamp(to_number(r.body-&gt;&gt; 'NextCalculationTime','99999999999999')/1000)) order by 1;
</pre>
</div>

<p>Update the report next execution time to a time when the system is under light load. The following queries are an example for a UTC environment. For your timezone, you need to specify the correct execution time:</p>

<div>
<pre>update  "ReportRuntime_&lt;yourtenantid&gt;" set body = jsonb_set(body, '{"NextCalculationTime"}', (to_number(body-&gt;&gt; 'NextCalculationTime','99999999999999')+54000000)::text::jsonb) where body-&gt;'IsScheduled' @&gt; 'true' and body-&gt;'Status' @&gt; '"READY"' and extract(hour from to_timestamp(to_number(body-&gt;&gt; 'NextCalculationTime','99999999999999')/1000)) in (2,3,4,5,6,7,8); 

update  "ReportRuntime_&lt;yourtenantid&gt;" set body = jsonb_set(body, '{"NextCalculationTime"}', (to_number(body-&gt;&gt; 'NextCalculationTime','99999999999999')+32400000)::text::jsonb) 
where body-&gt;'IsScheduled' @&gt; 'true' and body-&gt;'Status' @&gt; '"READY"' and extract(hour from to_timestamp(to_number(body-&gt;&gt; 'NextCalculationTime','99999999999999')/1000)) in (9,10,11,12,13,14); 

update  "ReportRuntime_&lt;yourtenantid&gt;" set body = jsonb_set(body, '{"NextCalculationTime"}', (to_number(body-&gt;&gt; 'NextCalculationTime','99999999999999')+18000000)::text::jsonb) 
where body-&gt;'IsScheduled' @&gt; 'true' and body-&gt;'Status' @&gt; '"READY"' and extract(hour from to_timestamp(to_number(body-&gt;&gt; 'NextCalculationTime','99999999999999')/1000)) in (15,16);
</pre>
</div>

<h4>Add more resources</h4>

<p>If, with all the tuning above, the system still runs out of CPU/memory, you may need to extend your CPU/RAM of your database.</p>

<h3>Rebalancing pods</h3>

<p>Run the check_resource.py script. If it finds that some workers are extremely busy while other workers are underutilized, you may rebalance the pods using the following commands:</p>

<div>
<pre>kubectl patch deploy/itom-xruntime-platform -nitsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"itom-xruntime-platform","env":[{"name":"RESTART_","value":"'$(date +%s)'"}]}]}}}}'

kubectl patch deploy/itom-xruntime-gateway -nitsma1 -p '{"spec":{"template":{"spec":{"containers":[{"name":"gateway","env":[{"name":"RESTART_","value":"'$(date +%s)'"}]}]}}}}'
</pre>
</div>

<p><strong>Important</strong>: the above commands will perform a rolling restart the platform pods and the gateway pods. The system will be available with less system capacity.</p>

<p>You may wish to create a cron job to run above command at a regular interval (every weekend for example).</p>

<h3>Tuning search performance</h3>

<p>If system searches perform poorly, you may try one or more of the following:</p>

<ul>
	<li>Add more Smart Analytics content servers through the GUI in bo</li>
	<li>Add more DAH (1 to 4) (see <a href="https://docs.microfocus.com/itom/SMAX:2018.08/Scale-out-DAH-server_21431430" target="1">https://docs.microfocus.com/itom/SMAX:2018.08/Scale-out-DAH-server_21431430</a>)</li>
	<li>Increase the cpu of content servers to 6</li>
	<li>Increase the RAM of content servers to 6GB</li>
	<li>Change the threads of content servers from 5 to 15 in Content.cfg</li>
	<li>Change the IndexCacheMaxSize of content servers to 1024000 in Content.cfg</li>
	<li>Change the TermCachePersistentKB of content servers to 1024000 in Content.cfgDatabase memory</li>
</ul>

<h3>Turn off the fluentd and logrotate for version 2018.08 ~ 2019.02</h3>

<p>Fluentd and Log Rotate combine the logs into a single location for future analysis. However, this requires extra disk space in /var/log folder. If you don't have enough disk space in /var/log, the system can be impacted. You can turn off this behavior using the following linux commands. You can still find the original logs in nfs.</p>

<div>
<pre>kubectl delete ds/fluentd -n core

kubectl delete ds/itom-logrotate -n core
</pre>
</div>

<h2>Other known issues</h2>

<p>There are memory leaks in Docker and Kubernetes. MicroFocus upgraded docker in 2019.02 to include fixes. However, there are still leaks in Kubernetes. Information about these issues can be found here:</p>

<p><a href="https://support.mesosphere.com/s/article/Critical-Issue-KMEM-MSPH-2018-0006" target="1">https://support.mesosphere.com/s/article/Critical-Issue-KMEM-MSPH-2018-0006</a></p>

<p><a href="https://github.com/docker/runc/pull/5" target="1">https://github.com/docker/runc/pull/5</a></p>

<p><a href="https://github.com/kubernetes/kubernetes/pull/72998" target="1">https://github.com/kubernetes/kubernetes/pull/72998</a></p>

<p>As a workaround, you can add more workers or restart the worker nodes regularly.</p>
<!-- 
NewPP limit report
Cached time: 20200217094441
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.160 seconds
Real time usage: 2.864 seconds
Preprocessor visited node count: 445/1000000
Preprocessor generated node count: 852/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 47999/5000000 bytes
--><!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
--><!-- Saved in parser cache with key docops_wiki:pcache:idhash:875813-0!canonical and timestamp 20200217094438 and revision id 1732704
 --></div>
