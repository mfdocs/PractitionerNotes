<div class="PNbanner"><div style="margin: auto 1% auto 0;"><i style="color:#0073e7;cursor:pointer;padding: 5%;" class="material-icons tooltip">info <span class="tooltiptext">The content has not been reviewed or tested by Micro Focus.</span></i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</div><div>This page was added by an ITOM practitioner. Join the practitioner group, contribute, and build a stronger knowledge network.</div></div><h1 style="margin-bottom:0px;">Monitoring and tuning resource utilization in a Kubernetes cluster</h1><div class="f14 metric" style="color:#656668;display:flex;"><img src="/assets/images/calendar.png" style="height:12px;margin-top:auto;margin-bottom:auto;"><span class="updateDate">&nbsp; Updated on Nov 8, 2019</span> &nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp; <img src="/assets/images/updated.png" style="height:12px;margin-top:auto;margin-bottom:auto;"><span class="readTime">&nbsp; 14 minutes to read</span>&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;<div>Rated 5&nbsp;<img src="/assets/images/star.png" style="height:12px;margin-top:auto;margin-bottom:auto;"></div> </div><br><div class="mw-parser-output"><div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr"><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Background"><span class="tocnumber">2</span> <span class="toctext">Background</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Resource_layers"><span class="tocnumber">2.1</span> <span class="toctext">Resource layers</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Case_example:_unbalanced_cluster"><span class="tocnumber">2.2</span> <span class="toctext">Case example: unbalanced cluster</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#How_to_Check_Resources"><span class="tocnumber">2.3</span> <span class="toctext">How to Check Resources</span></a>
<ul>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Running_the_check_resources.py_script"><span class="tocnumber">2.3.1</span> <span class="toctext">Running the check_resources.py script</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Interpreting_the_results"><span class="tocnumber">2.3.2</span> <span class="toctext">Interpreting the results</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#When_to_run_the_script"><span class="tocnumber">2.3.3</span> <span class="toctext">When to run the script</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#How_to_Tune_Resources"><span class="tocnumber">2.4</span> <span class="toctext">How to Tune Resources</span></a>
<ul>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Node_capability"><span class="tocnumber">2.4.1</span> <span class="toctext">Node capability</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Node_resources"><span class="tocnumber">2.4.2</span> <span class="toctext">Node resources</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Pod_resources"><span class="tocnumber">2.4.3</span> <span class="toctext">Pod resources</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Pod_restarts"><span class="tocnumber">2.4.4</span> <span class="toctext">Pod restarts</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#How_to_Monitor_Resources"><span class="tocnumber">2.5</span> <span class="toctext">How to Monitor Resources</span></a>
<ul>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#The_resources_report.csv"><span class="tocnumber">2.5.1</span> <span class="toctext">The resources_report.csv</span></a></li>
<li class="toclevel-3"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube#Charting_in_Excel"><span class="tocnumber">2.5.2</span> <span class="toctext">Charting in Excel</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

<h1><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=SMAX:2019.05/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube&amp;veaction=edit&amp;section=T-1" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=SMAX:2019.05/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube&amp;action=edit&amp;section=T-1" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>Resource checking, tuning and monitoring in a Kubernetes cluster is essential for both initial sizing and for subsequent ongoing environment stability. 
It is important to realize:
</p>
<ul><li>It is insufficient to only assign the amount of memory and cores specified in the supporting documentation.</li>
<li>Even assigning surplus memory and cores, with no active users in the system, may still lead to instability if the out-of-box tuning does not perfectly match the hardware architecture and no additional tuning steps are taken.</li></ul>
<p>This guide and the accompanying script help provide insight and resolution for resource management and consequently cluster stability. 
</p>
<h1><span class="mw-headline" id="Background">Background</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/mediawiki/index.php?title=SMAX:2019.05/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube&amp;veaction=edit&amp;section=T-2" class="mw-editsection-visualeditor" title="Edit section: ">edit</a><span class="mw-editsection-divider"> | </span><a href="/mediawiki/index.php?title=SMAX:2019.05/PractitionersNotes/SMAX_Config/cg_system_admin/cg_monitor_kube&amp;action=edit&amp;section=T-2" title="Edit section: ">edit source</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>Due to layered nature of a cluster’s resources, resource reservations and limits may need to be fine-tuned at the OS, node, pod or container level to enable a smoothly functioning suite.
</p>
<h2 class="mw-headline" id="Resource_layers">Resource layers</h2>
<p>The following diagram shows how memory assignment per pod adds up across nodes. 
<a onclick="javascript:loadingImage(this);" class="image"><img alt="CheckResources1.png" src="/mediawiki/images/b/b9/CheckResources1.png" width="1050" height="752" data-file-width="1050" data-file-height="752"></a>
</p>
<h2 class="mw-headline" id="Case_example:_unbalanced_cluster">Case example: unbalanced cluster</h2>
<p><a onclick="javascript:loadingImage(this);" class="image"><img alt="CheckResources2.png" src="/mediawiki/images/5/55/CheckResources2.png" width="1050" height="414" data-file-width="1050" data-file-height="414"></a>
The above cluster example shows an unhealthy cluster as Worker2 is running out of memory any may soon be evicted. This will result in all its pods being rescheduled on Worker1. Worker1 will then also become overwhelmed and be evicted. This is called a cascade failure. Detecting and rebalancing some of Worker2’s load onto Worker1 or a new 3rd worker would prevent this cascade and maintain stability.
</p>
<h2 class="mw-headline" id="How_to_Check_Resources">How to Check Resources</h2>
<p>Kubernetes provides a number of commands to check resource usage at the node or pod level, or to check maximum resource limits, or to check node/pod status. The check_resources.py script (version 2.0+) combines all these command outputs into one single report and maintains an average record of the values for monitoring.
</p>
<h3 class="mw-headline" id="Running_the_check_resources.py_script">Running the check_resources.py script</h3>
<p>The script, if not packaged with this documentation, should be accessible from the Micro Focus Marketplace website. 
It can be run by entering the command:   
</p>
<pre>python check_resources.py
</pre>
<p>Python 2.7.x must be installed. The script is not tested for Python 3.x.
</p>
<h3 class="mw-headline" id="Interpreting_the_results">Interpreting the results</h3>
<p>The output will show the following information;
<a onclick="javascript:loadingImage(this);" class="image"><img alt="CheckResources3.png" src="/mediawiki/images/3/37/CheckResources3.png" width="1050" height="512" data-file-width="1050" data-file-height="512"></a>
In more detail and for reference;
</p>
<ol><li>Nodes
<ol><li><b>Type</b> - master &amp;/ worker</li>
<li><b>Availability</b> to schedule pods: inaccessible | cordoned | ready</li>
<li><b>Node name</b> – usually the fqdn for the server on which the node is running</li>
<li><b>CPU cores</b>: percentage of available used, actual used, total available – color coded if over 80%</li>
<li><b>Memory</b>: percentage of available used, actual used, total available – color coded if over 80%</li>
<li><b>Architecture summary</b>: OS version, processor type, docker &amp; kubelet versions</li>
<li><b>Warning flags</b>: OutOfDisk | MemoryPressure | DiskPressure</li></ol></li>
<li>Pods
<ol><li><b>Namespace</b></li>
<li><b>Pod name</b> and total pods per node</li>
<li><b>CPU cores</b>:
<ol><li><b>Average</b> – calculated as mean of all previously recorded values. The Notes section will outline how many data points and data range were used to generate the average, and the file that stores these values.</li>
<li><b>Current</b> – the amount of cores the pod is currently using for all its containers</li>
<li><b>MAX</b> – this is the cpu ‘Limit’ value set for the deployment</li></ol></li>
<li><b>Memory</b>:
<ol><li><b>Average</b> – calculated as mean of all previously recorded current values</li>
<li><b>Current</b> – the amount of MB of memory the pod is currently using for all its containers</li>
<li><b>MAX</b> – this is the memory  ‘Limit’ value set for the deployment</li></ol></li>
<li><b>Status</b> of pod, e.g.; “Running”, “Terminating” and the number of containers that apply, e.g.; 1 of 2, 2 of 2</li>
<li><b>Restart count</b> – the number of times this pod has failed its probe check or experienced and error and been started over. This is color coded if it is statistically more than 2 times the standard deviation from the average (if over 10), to indicate an outlier with excessive restarts and warranting review.</li>
<li><b>Age</b> – the amount of time this pod, with its unique ID, has been scheduled</li></ol></li></ol>
<h3 class="mw-headline" id="When_to_run_the_script">When to run the script</h3>
<p>Important times to (re)check resources are;
</p>
<ul><li>after installation, patching, or updating</li>
<li>after any node outage or cluster restart</li>
<li>during load testing or at times of peak usage</li>
<li>after any addition or subtraction of nodes</li></ul>
<p>At these times some nodes may be over committed and pods need to be re-balanced across nodes, or the need for a new worker node is seen.
You can also schedule the script to be run periodically (15 to 30 mins recommended) using a cron job. E.G.;
</p>
<pre>crontab -l | { cat; echo "*/15 * * * * python /root/check_resources.py"; } | crontab -&nbsp;; crontab -l
</pre>
<h2 class="mw-headline" id="How_to_Tune_Resources">How to Tune Resources</h2>
<p>Tuning can also refer to other aspects of a Kubernetes cluster such as software config, database, network, file storage, etc. The following is tuning specific to memory and cpu for keeping pods running and balanced across nodes, and preventing node eviction.
</p><p>If enabled in the Check_resources.py settings, some Recommendations will be presented at the end of the output on the following areas.
</p><p>Note that these recommendations are algorithmically generated and may or may not be relevant to every scenario. If in doubt, or if it’s a Production environment, check the product documentation and with Support.
</p>
<h3 class="mw-headline" id="Node_capability">Node capability</h3>
<p>This is which nodes have issues that will impact scheduling. The node may be down, firewalled, or cordoned, preventing new pods being scheduled to such a node and consequently degrading cluster stability.
</p>
<ul><li>To recover a down node, restart the server and or run /opt/kubernetes/bin/kube-restart.sh on that server.</li>
<li>For inaccessible nodes, check the firewall and network path</li>
<li>For cordoned nodes, run the command #:  kubectl uncordon &lt;node-name&gt;</li></ul>
<h3 class="mw-headline" id="Node_resources">Node resources</h3>
<p>This is which nodes are low on resources. 
</p>
<ul><li>If over 70% resources, a warning to keep monitoring that node is shown.</li>
<li>If over 80%, suggestions are made to take action. This is because testing has shown that a node with a sustained resource load (memory in particular) over 80% will result in instability and performance issues. If it is a master, the cluster will startup and respond slower, and if on a worker there is a greater susceptibility to cascading crashes due to worker evictions.</li>
<li>The suggested action is to move pods from the overcommitted worker node to another (re-balancing) if there is a worker with free resources. This can be done by cordoning the busy node and deleting the pod; it will be rescheduled on an uncordoned node. If the pod is not part of a replicaset (there is only one in the cluster), the end-users will not have access to the functionality provided by that pod while it is restarting. To move a pod use these commands;</li></ul>
<pre>kubectl cordon &lt;busy-node&gt;  #repeat for all busy nodes 
kubectl delete pod &lt;pod-name&gt; -n&lt;ns&gt;  #repeat until balanced
</pre>
<ul><li>If there are no other nodes that are not also busy, the suggestion is to add more OS resources or another worker. The latter can be done in the CDF management page on the :5443 port.</li></ul>
<h3 class="mw-headline" id="Pod_resources">Pod resources</h3>
<p>If a pod is using more than 80% of the resources, the suggestion is made to increase the replica count, if it is part of a replicaset, or to increase the resource “limit”.
</p>
<ul><li>To restart a pod with more memory this command can be used;</li></ul>
<pre>kubectl patch deploy/&lt;deploy_name&gt; -n&lt;ns&gt; -p '{"spec":{"template":{"spec":{"containers":[{"name":"&lt;main_container&gt;","resources":{"limits":{"memory":"&lt;new val in Mi&gt;"}}}]}}}}'
</pre>
<p>This will restart the pods for that deployment in order to apply the new memory limit. If you have a saved yaml file for the deployment, keep in mind that the in-memory config is now different.
Continually increasing the resource limits is not recommended unless there are unique use cases. Increase it a few times by 1.5 until it stabilizes, otherwise contact Support to review the resource usage.
Remember that increasing pod resources could affect the total cluster resource footprint to the point that an additional node is needed.
</p>
<ul><li>To increase the replica count for a pod that is part of a replicaset this command can be used;</li></ul>
<pre>kubectl scale deploy/&lt;deploy_name&gt; -n&lt;ns&gt; --replicas=&lt;new replica value&gt;
</pre>
<p>Some pods are not designed to have multiple replicas – check the documentation.
Continually increasing the replica count is not recommended unless there are unique use cases. Add up to a couple additional replicas to handle additional load, otherwise contact Support to review the cluster sizing and unique usage patterns.
It is not recommended to completely resize the cluster in this manner as additional parameters are needed.
</p>
<h3 class="mw-headline" id="Pod_restarts">Pod restarts</h3>
<p>If a pod has a high restart count then the suggestion is made to review the pod logs for reasons why. It is triggered when the restart count is more than 2 times the standard deviation from the average. The pod log can be reviewed for possible causes, and if no issues other than a failed probe, then the probe timeout could be increased in the deployment configuration.
</p>
<ul><li>To increase a probe timeout you can review and edit the deployment configuration with the following command;</li></ul>
<pre>kubectl edit deploy/&lt;deploy_name&gt; -n&lt;ns&gt; -o yaml --save-config
</pre>
<p>This should be done carefully and per documentation or Support guidance.
</p>
<h2 class="mw-headline" id="How_to_Monitor_Resources">How to Monitor Resources</h2>
<p>There are number of more comprehensive kubernetes resource monitoring tools available, for example; the built in interface on the :5443 port, or a 3rd-party implementation of Prometheus, etc. The following is a simple no-install approach for admins to obtain and interact with memory and cpu utilization data that is related to stability.
</p>
<h3 class="mw-headline" id="The_resources_report.csv">The resources_report.csv</h3>
<p>Every time check_resources.py is run, either manually or via a scheduled cron job every 15 minutes for example, a spreadsheet is updated with the latest cpu and memory reading for each node and each pod. The contents may look like this excerpt;
</p>
<pre>[root@smax_worker1.net ~]# cat resources_report.csv
type,metric,name,Apr 20 2019 16:35:10,Apr 20 2019 16:35:47,Apr 20 2019 16:38:24
node,cpu,smax_worker1.net, .86, .86, .93
node,mem, smax_worker1.net,9628,9628,9631
pod,cpu,cdf-apiserver-6957864c67-p5spl,0,0,0
pod,mem,cdf-apiserver-6957864c67-p5spl,910,910,910
pod,cpu,idm-68c78b998d-9vt8d, .02, .02,0
pod,mem,idm-68c78b998d-9vt8d,541,541,520
</pre>
<p><i>The first line shows the header where is type is node/pod, metric is cpu/mem, and then the name of the node/pod. Following that are the list of dates the measurements were taken. There are two lines for each node/pod; one for the memory and one for the cpu. The above example shows smax_worker1.net’s resources slightly increased on the 3rd reading, and the idm pod’s memory dropped.</i>
</p>
<h3 class="mw-headline" id="Charting_in_Excel">Charting in Excel</h3>
<p>For multiple columns you can open the csv in Excel. By selecting all data, then clicking Insert &gt; Table, the data can be filtered down by type or metric.
</p><p>Chart the data with one of the line graphs;
</p>
<pre><a onclick="javascript:loadingImage(this);" class="image"><img alt="CheckResources4.png" src="/mediawiki/images/2/20/CheckResources4.png" width="1050" height="587" data-file-width="1050" data-file-height="587"></a>
</pre>
<p>Or add a column to the table and then insert a sparkline for charting a greater number of columns;
</p>
<pre><a onclick="javascript:loadingImage(this);" class="image"><img alt="CheckResources5.png" src="/mediawiki/images/6/68/CheckResources5.png" width="1050" height="600" data-file-width="1050" data-file-height="600"></a>
</pre>
<p>When a node or pod’s total resources exceeds 80%, review the Recommendations for that node. 
If pod resources start to climb over time, consider scheduling rebalances or restarts.
</p>
<!-- 
NewPP limit report
Cached time: 20200217094450
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.028 seconds
Real time usage: 0.126 seconds
Preprocessor visited node count: 20/1000000
Preprocessor generated node count: 50/1000000
Post‐expand include size: 11680/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 0/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key docops_wiki:pcache:idhash:875828-0!canonical and timestamp 20200217094450 and revision id 1577600
 -->
</div>