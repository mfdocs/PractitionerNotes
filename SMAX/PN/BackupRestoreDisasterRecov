<div class="mw-parser-output"><p><br>
Data backup is a prerequisite to disaster recovery.
</p><p>Accurately backing up all the data needed so it can be restored in case of a disaster is of paramount importance.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr"><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/BackupRestoreDisasterRecov#Backup_and_restore"><span class="tocnumber">1</span> <span class="toctext">Backup and restore</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/BackupRestoreDisasterRecov#Backup"><span class="tocnumber">1.1</span> <span class="toctext">Backup</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/BackupRestoreDisasterRecov#Restore"><span class="tocnumber">1.2</span> <span class="toctext">Restore</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/BackupRestoreDisasterRecov#Disaster_recovery"><span class="tocnumber">2</span> <span class="toctext">Disaster recovery</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/BackupRestoreDisasterRecov#A_few_pets_but_mostly_cattle"><span class="tocnumber">2.1</span> <span class="toctext">A few pets but mostly cattle</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/BackupRestoreDisasterRecov#Cattle_and_CDF_disaster_recovery"><span class="tocnumber">2.2</span> <span class="toctext">Cattle and CDF disaster recovery</span></a></li>
</ul>
</li>
</ul>
</div>

<h2 class="mw-headline" id="Backup_and_restore">Backup and restore</h2>
<h3 class="mw-headline" id="Backup">Backup</h3>
<p>Containers provide the edge over classic product installation because of the standardized way of building and running containerized applications.
</p><p>All application components running in containers share the same common runtime and a similar and known way and place to store external data.
</p><p>In case of the CDF and the Suite, backing up all of the data is simplified as it lives in only a handful known locations:
</p>
<a class="buttonfullscr" onclick="javascript:loadingTableButton(this);"><button class="flscrbtn"><i class="material-icons flscricn">open_in_new</i><div style="margin: auto 5px;">View Fullscreen</div></button></a><table>
<tbody><tr>
<th><b>Location</b>
</th>
<th><b>Use</b>
</th></tr>
<tr>
<td>Docker runtime data directories
</td>
<td>
<ul><li>Vault, flannel and Kubernetes</li>
<li>CDF core (management portal, suite installer)</li></ul>
</td></tr>
<tr>
<td>The attached NFS volumes
</td>
<td>
<ul><li>CDF NFS external volume (container persisted configuration and data)
<ul><li>Database files</li>
<li>Configuration data</li></ul></li>
<li>Suite NFS external volumes (container persisted configuration and data)
<ul><li>Database files</li>
<li>Configuration data</li></ul></li></ul>
</td></tr>
<tr>
<td>CDF installation configuration data
</td>
<td>
<ul><li>CDF core container specifications</li>
<li>CDF certificate CA and certificates</li>
<li>Docker daemons configuration</li>
<li>Autopass/IDM configuration</li>
<li>API server Nginx configuration</li></ul>
</td></tr></tbody></table>
<h3 class="mw-headline" id="Restore">Restore</h3>
<p>Restoring saved data to a fresh CDF cluster is supported.
</p><p>Refer to the suite documentation for detailed backup and restore instructions.
</p>
<h2 class="mw-headline" id="Disaster_recovery">Disaster recovery</h2>
<h3 class="mw-headline" id="A_few_pets_but_mostly_cattle">A few pets but mostly cattle</h3>
<p>Nodes, even master nodes, are much more like cattle than pets.
</p><p>Already in 2012, in a presentation given at CERN (CH, <a target="1" rel="nofollow" class="external free" href="http://www.slideshare.net/gmccance/cern-data-centre-evolution">http://www.slideshare.net/gmccance/cern-data-centre-evolution</a>) this new service model was explained.
</p><p>Instead of treating servers like an admin’s favorite (set of) unique pets, caring for them, nursing them back to health if they get sick, administrators simply treat servers as cattle.
</p><p>Servers are almost identical, loving care is limited and if they get sick, admins just kill it and get a fresh one.
</p><p>Virtualization removes the need to deal with physical machines. Servers can be provisioned in minutes.
</p><p>Why try to save a server in a CDF multi-node cluster if that cluster is resilient to node failure (workload is rescheduled elsewhere and/or is highly available) and the downed server can be replaced with a fresh new one and added back into the cluster in minutes?
</p>
<h3 class="mw-headline" id="Cattle_and_CDF_disaster_recovery">Cattle and CDF disaster recovery</h3>
<p>In a multi-master node with multiple worker nodes, the system can tolerate node outages up to a fairly severe level.
</p><p>On multi-master nodes setups the ETCD database is distributed. So it is essentially sufficient (worst case) to reduce the number of cluster members to one and then start adding them back in as new members.
</p><p>Workers nodes are totally like cattle. If a worker node crashes or dies, it can simply be reinstalled or replaced. The workload will be moved automatically by Kubernetes.
</p><p>So for disaster recovery, the Micro Focus strategy is to take a fresh look at traditional server management and treat all of the machines in a cluster like cattle and not like pets.
</p><p>If we take this approach to disaster recovery, things become much easier, but <b>only if</b> we take care of building resilient systems that revolve around the concepts of shared nothing, stateless file systems, configuration automation and repeatability.
</p>
<a class="buttonfullscr" onclick="javascript:loadingTableButton(this);"><button class="flscrbtn"><i class="material-icons flscricn">open_in_new</i><div style="margin: auto 5px;">View Fullscreen</div></button></a><table>
<tbody><tr>
<th><b>What</b>
</th>
<th><b>Type</b>
</th>
<th><b>Backup&nbsp;?</b>
</th>
<th><b>Restore&nbsp;?</b>
</th></tr>
<tr>
<td><b>1<sup>st</sup> master node</b>
</td>
<td>Pet
</td>
<td>Yes
</td>
<td>Yes
</td></tr>
<tr>
<td><b>NFS volume(s)</b>
</td>
<td>Pet
</td>
<td>Yes
</td>
<td>Yes
</td></tr>
<tr>
<td><b>External DB</b>
</td>
<td>Pet
</td>
<td>Yes
</td>
<td>Yes
</td></tr>
<tr>
<td>Secondary master node(s)
</td>
<td>Cattle
</td>
<td>No
</td>
<td>No&nbsp;; recreate or add new
</td></tr>
<tr>
<td>Worker nodes
</td>
<td>Cattle
</td>
<td>No
</td>
<td>No&nbsp;; recreate or add new
</td></tr></tbody></table>
<p>Refer to the suite documentation for detailed steps for disaster recovery.
</p>
<!-- 
NewPP limit report
Cached time: 20200217095613
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.012 seconds
Real time usage: 0.051 seconds
Preprocessor visited node count: 10/1000000
Preprocessor generated node count: 36/1000000
Post‐expand include size: 3888/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 0/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key docops_wiki:pcache:idhash:875857-0!canonical and timestamp 20200217095613 and revision id 1586172
 -->
</div>