<div class="mw-parser-output"><p><br>
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr"><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Overview"><span class="tocnumber">1</span> <span class="toctext">Overview</span></a></li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Containers"><span class="tocnumber">2</span> <span class="toctext">Containers</span></a></li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Docker_storage"><span class="tocnumber">3</span> <span class="toctext">Docker storage</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#The_devicemapper_storage_driver"><span class="tocnumber">3.1</span> <span class="toctext">The devicemapper storage driver</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Docker_image_cache"><span class="tocnumber">3.2</span> <span class="toctext">Docker image cache</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Host_storage"><span class="tocnumber">4</span> <span class="toctext">Host storage</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Docker_storage_driver"><span class="tocnumber">4.1</span> <span class="toctext">Docker storage driver</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#ETCD"><span class="tocnumber">4.2</span> <span class="toctext">ETCD</span></a></li>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Other_host_storage"><span class="tocnumber">4.3</span> <span class="toctext">Other host storage</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Container_Deployment_Foundation_storage"><span class="tocnumber">5</span> <span class="toctext">Container Deployment Foundation storage</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#CDF_external_storage"><span class="tocnumber">5.1</span> <span class="toctext">CDF external storage</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Suites_storage"><span class="tocnumber">6</span> <span class="toctext">Suites storage</span></a>
<ul>
<li class="toclevel-2"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Suite_external_storage"><span class="tocnumber">6.1</span> <span class="toctext">Suite external storage</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#How_CDF_and_suites_connect_to_external_storage"><span class="tocnumber">7</span> <span class="toctext">How CDF and suites connect to external storage</span></a></li>
<li class="toclevel-1"><a href="https://docs.microfocus.com/itom/SMAX:2019.11/Storage#Storage_performance"><span class="tocnumber">8</span> <span class="toctext">Storage performance</span></a></li>
</ul>
</div>

<h2 class="mw-headline" id="Overview">Overview</h2><p>
Local node storage considerations are necessary for both CDF installations with and without embedded Kubernetes. Guidance can be viewed from the details below that discuss CDF installation with embedded Kubernetes.</p><div class="Admonition_Note">
<p><span class="autonumber">Note</span><br>
&lt;CDF root&gt; is the CDF installation root directory, the default is /opt/kubernetes.
</p>
</div> 
<p>The following table provides an overview of the various storage locations and types.
</p>
<a class="buttonfullscr" onclick="javascript:loadingTableButton(this);"><button class="flscrbtn"><i class="material-icons flscricn">open_in_new</i><div style="margin: auto 5px;">View Fullscreen</div></button></a><table>
<tbody><tr>
<th><b>Component/process requiring storage</b>
</th>
<th><b>Backed</b>
</th>
<th><b>Path</b>
</th>
<th><b>Typical size or size range</b>
</th></tr>
<tr>
<td>CDF installation
</td>
<td>Node local
</td>
<td>&lt;CDF root&gt;
</td>
<td>+- 3GB
</td></tr>
<tr>
<td>CDF image download
</td>
<td>Node local
</td>
<td>Directory can be chosen.
</td>
<td>Between 5 GB and 80 GB. Check the Suite documentation for more detailed numbers.
</td></tr>
<tr>
<td>CDF external storage
</td>
<td>NFS
</td>
<td>/baseinfra-1.0 and /suite-install in the CDF external volume
</td>
<td>
<p>Between 5 GB and 80 GB
</p>
<div class="Admonition_Note">
<p><span class="autonumber">Note</span><br>
Includes CDF private registry.
</p>
</div> 
</td></tr>
<tr>
<td>Docker storage driver (devicemapper) for container runtime images
</td>
<td>Node local
</td>
<td>
<p>&lt;CDF root&gt;/data/docker
</p>
</td>
<td>Between 5 GB and 200GB
</td></tr>
<tr>
<td>CDF ETCD instance
</td>
<td>Node local
</td>
<td>&lt;CDF root&gt;/data/etcd
</td>
<td>Between 250 M and 2 GB
</td></tr>
<tr>
<td>CDF core logs
</td>
<td>Node local
</td>
<td>&lt;CDF root&gt;/log and /var/log/cdf
</td>
<td>Between 100 KB and 2 GB
</td></tr>
<tr>
<td>Docker image cache
</td>
<td>Node local
</td>
<td>
<p>&lt;CDF root&gt;/data/docker
</p>
</td>
<td>Between 5 GB and 80 GB
</td></tr>
<tr>
<td>CDF private registry
</td>
<td>NFS
</td>
<td>Below /baseinfra-1.0 in the CDF external volume
</td>
<td>Between 5 GB and 80 GB
</td></tr>
<tr>
<td>Suite external storage
</td>
<td>NFS
</td>
<td>Below the Suite external volume
</td>
<td>See Suite installation/administration guides
</td></tr></tbody></table>
<h2 class="mw-headline" id="Containers">Containers</h2>
<p>At runtime, container file systems are either completely read-only (RO) or read-write (RW).
</p><p>Files on-disk inside a container are ephemeral. When a container crashes, it will be restarted but the files will be lost. A container always starts with a clean file system.
</p><p>Changes to RW container file systems will not survive a container restart. Any changes made to a RW container file system will be gone if the container is restarted for whatever reason.
</p><p>Therefore, any container state that needs to survive a restart needs to be stored outside of the container on so called <b>external storage</b>.
</p><p>When running in Kubernetes (the scheduling unit being a Pod possibly consisting of multiple containers) it may also be necessary to share files between containers. This needs to be done by accessing a shared external data store.
</p><p>External storage can be backed by a variety of technologies ranging from traditional local hard drives or SSD, SAN attached storage (iSCSI, fiber channel), disk clustering software, NFS and others.
</p><p>A choice of storage depends on what customers are using and/or performance and HA requirements for the data in the external storage.
</p><p>CDF/suites currently support only local temporary or NFS storage.
</p><p>The data stored in the external storage can be any of the following (and the list is not exhaustive):
</p>
<ul><li>Configuration files</li>
<li>Add-on binary files</li>
<li>Product customizations (can take many forms)</li>
<li>Database files</li>
<li>Temporary files</li>
<li>Platform and Suite logs</li></ul>
<p>Kubernetes provides external storage to containers via an abstraction called Volumes.
</p><p>Volumes are attached to containers either directly or via a further abstraction called Persistent Volumes and Persistent Volume Claims.
</p><p>Essentially, volumes are just directories that are made accessible to running containers inside a Pod.
</p><p>See <a target="1" rel="nofollow" class="external free" href="https://kubernetes.io/docs/concepts/storage/volumes/#background">https://kubernetes.io/docs/concepts/storage/volumes/#background</a>
</p><p>The CDF uses both directly attached Volumes (for ephemeral data) and Persistent Volumes (for persistent data) attached to containers.
</p><p>Volume mounts are used for:
</p>
<ul><li>exposing a node host path inside a container (hostPath)</li>
<li>providing ephemeral empty directories (emptyDir)</li></ul>
<p>Persistent Volumes are used for:
</p>
<ul><li>container configuration data</li>
<li>log files</li>
<li>database files</li></ul>
<p>Volumes link the container to a specific type of storage and are therefore not ideal when creating portable deployments of Suites. For this reason, their use is kept limited.
</p><p>Persistent Volumes do not link the container to a specific type of storage. Through the creation of persistent volume claims (PVC) that are linked 1:1 with a particular persistent volume (PV) and the use of the PVC as a volume inside the container, we can achieve volume storage backend agnostic deployments.
</p><p>Volumes: <a target="1" rel="nofollow" class="external free" href="https://kubernetes.io/docs/concepts/storage/volumes/">https://kubernetes.io/docs/concepts/storage/volumes/</a>
</p><p>Persistent volumes: <a target="1" rel="nofollow" class="external free" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a>
</p>
<div class="Admonition_Note">
<p><span class="autonumber">Note</span><br>
</p>
<ul><li>Other than hostPath and emptyDir, CDF containers do not use Volumes directly.</li>
<li>CDF does not use StorageClasses in persistent volumes except for StatefulSets. But the storage class is only used as a selector, no provisioner is associated.</li>
<li>CDF only supports NFS persistent volumes</li></ul>
</div>
<h2 class="mw-headline" id="Docker_storage">Docker storage</h2>
<p>Docker provides a RW container filesystem by layering a RW layer on top of the RO image layer.
</p><p>See: <a target="1" rel="nofollow" class="external free" href="https://docs.docker.com/engine/reference/glossary/#union-file-system">https://docs.docker.com/engine/reference/glossary/#union-file-system</a>
</p><p>Excerpt from the above link: “Union file systems implement a union mount and operate by creating layers. Docker uses union file systems in conjunction with copy-on-write techniques to provide the building blocks for containers, making them very lightweight and fast.”
</p><p>Docker hosts use a storage driver to provide a unified view of the layered file system that makes up Docker images.
</p><p>Various storage drivers exist such as AUFS, OverlayFS, OverlayFS2, Btrfs, and others.
</p><p>The Docker hosts inside the Container Deployment Foundation (CDF) use the <b>devicemapper</b> storage driver. This is the default storage driver that Micro Focus uses on the supported Linux host platforms: Red Hat, Oracle Linux and CentOS.
</p>
<h3 class="mw-headline" id="The_devicemapper_storage_driver">The devicemapper storage driver</h3>
<p>The devicemapper storage driver can be configured in two modes:
</p>
<ul><li>loop-lvm or loopback mode, suitable for testing and small installations</li>
<li>direct-lvm, suitable for production use</li></ul>
<p>By default, for easy install, demo or very small Suite installations, the devicemapper configuration for the CDF uses the loopback mode for the devicemapper storage driver.
</p><p>The loopback mode will support demo and small suite installations, but it is <b>not</b> suitable for production use.
</p><p>For production use, we recommend the use of the direct-lvm mode of the devicemapper storage driver.
</p><p>Storage drivers can use node local storage or can use node local storage that is in fact provided by SAN or NAS arrays. Once mounted as a volume inside a directory, it is transparent for the storage driver.
</p><p>For a complete discussion of the devicemapper storage driver including the loop-lvm and direct-lvm modes, refer to <a target="1" rel="nofollow" class="external free" href="https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/">https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/</a>.
</p>
<h3 class="mw-headline" id="Docker_image_cache">Docker image cache</h3>
<p>The image cache is located here: &lt;CDF root&gt;/data/docker/devicemapper/devicemapper
</p><p>The directories contain two files:
</p>
<ul><li>data</li>
<li>metadata</li></ul>
<p>Cleanup of the Docker image cache must be done using Docker commands: docker rmi.
</p><p>DO NOT make manual changes below &lt;CDF root&gt;/data/docker.
</p><p>See also “Docker storage driver”.
</p>
<h2 class="mw-headline" id="Host_storage">Host storage</h2>
<h3 class="mw-headline" id="Docker_storage_driver">Docker storage driver</h3>
<p>See also “Docker storage”.
</p><p>Local node storage, i.e. the local disks, is used extensively when running Docker containers.
</p><p>The Docker storage driver is backed by local storage. Micro Focus recommends fast storage backing such as SSD drives for the storage driver volumes.
</p><p>The directories where the Docker storage driver stores its files are &lt;CDF root&gt;/data/docker.
</p><p>These are sparse files, so the size reported by “ls –l” for the individual files is not the same as the actual size on disk.
</p><p>The devicemapper storage driver uses the Copy-on-Write <a target="1" rel="nofollow" class="external free" href="https://en.wikipedia.org/wiki/Copy-on-write">https://en.wikipedia.org/wiki/Copy-on-write</a> technique to reduce the actual physical size of the container image storage as much as possible.
</p><p>This may throw off typical utilities to check for used disk space such as ‘df’. The ‘df’ utility will report much more storage than a node may physically have as it reports all container devicemapper mount points and their reserved storage.
</p><p>Running the ‘du’ utility on &lt;CDF root&gt;/data/docker, will show a more accurate picture. Try running ‘du -hs .’ from &lt;CDF root&gt;/data/docker.
</p><p>The most accurate information is available when running ‘docker info’.
</p><p>The storage section in the output of the ‘docker info’ command gives the detailed picture - see &gt;&gt;&gt; prefixed lines:
</p>
<pre class="syntaxhighlighter-pre">Storage Driver: devicemapper
Pool Name: docker-253:1-641978369-pool
Pool Blocksize: 65.54 kB
Base Device Size: 10.74 GB
Backing Filesystem: xfs
&gt;&gt;&gt;Data file: /dev/loop0
&gt;&gt;&gt;Metadata file: /dev/loop1
&gt;&gt;&gt;Data Space Used: 25.89 GB
&gt;&gt;&gt;Data Space Total: 107.4 GB
&gt;&gt;&gt;Data Space Available: 81.48 GB
&gt;&gt;&gt;Metadata Space Used: 33.24 MB
&gt;&gt;&gt;Metadata Space Total: 2.147 GB
&gt;&gt;&gt;Metadata Space Available: 2.114 GB
Thin Pool Minimum Free Space: 10.74 GB
Udev Sync Supported: true
Deferred Removal Enabled: false
Deferred Deletion Enabled: false
Deferred Deleted Device Count: 0
Data loop file: &lt;CDF root&gt;/data/docker/devicemapper/devicemapper/data
WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.
Metadata loop file: &lt;CDF root&gt;/data/docker/devicemapper/devicemapper/metadata
Library Version: 1.02.135-RHEL7 (2016-11-16)
</pre>
<p>It is possible to run out of space and needing more capacity on the thin-pool device.
</p><p>Increasing the capacity works for both loopback and direct-lvm modes.
</p><p>See <a target="1" rel="nofollow" class="external free" href="https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#increase-capacity-on-a-running-device">https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#increase-capacity-on-a-running-device</a> for details on how to increase the capacity.
</p>
<h3 class="mw-headline" id="ETCD">ETCD</h3>
<p>Additional local node storage is used by ETCD, but only on the master nodes.
</p><p>The directory where the ETCD storage is located is &lt;CDF root&gt;/data/etcd.
</p><p>A typical CDF and subsequent Suite deployment would require around 500 MB of ETCD storage. This can be more if more nodes and/or Suite capabilities are added.
</p>
<h3 class="mw-headline" id="Other_host_storage">Other host storage</h3>
<p>Additional storage is needed for CDF component and scripts logs.
</p><p>The directory for these logs is &lt;CDF root&gt;/log.
</p><p>Typically, after installation of the CDF and a Suite, the size will be between 512 KB and 1 MB.
</p><p><br>
</p>
<h2 class="mw-headline" id="Container_Deployment_Foundation_storage">Container Deployment Foundation storage</h2>
<p>The storage for the CDF is split between:
</p>
<ul><li>Node local storage for the Docker, Kubernetes binaries and their runtime data</li>
<li>Node local storage for the Docker image cache</li>
<li>Node local storage for hostPath and emptyDir directly attached container volumes</li>
<li>External storage backed by an NFS server attached to containers via Persistent Volumes and persistent volume claims.</li></ul>
<p>For Docker, Kubernetes, hostPath and emptyDir, see the previous chapters.
</p>
<h3 class="mw-headline" id="CDF_external_storage">CDF external storage</h3>
<p>The external storage for CDF is backed by a single NFS volume.
</p><p>NFS is the only external volume backing store that is supported in this release.
</p><p>The top-level path for the CDF external volume will depend on your NFS configuration.
</p><p>Let’s assume that it is exported from the NFS server as /var/vols/itom/core.
</p><p>In this folder, four main data directories exist:
</p>
<ul><li>baseinfra-1.0/
<ul><li>IDM and suite installer PostgreSQL database files</li>
<li>Docker private registry data store</li>
<li>Static local DNS resolution configuration</li></ul></li>
<li>suite-install/
<ul><li>Suite Installer configuration data</li></ul></li>
<li>offline_sync_tools
<ul><li>Data for use by suite update process</li></ul></li>
<li>yaml
<ul><li>Data used by the CDF installer process</li></ul></li></ul>
<div class="Admonition_Note">
<p><span class="autonumber">Note</span><br>
No folders/files can be changed/edited in &lt;CDF NFS root&gt;.
</p>
</div> 
<p>The typical size (minus the Docker private registry store) of the CDF and the configuration data for an installed Suite is around 160 MB. This is not expected to grow significantly when the CDF and an installed Suite are run for extended periods of time.
</p><p>The size of the Docker private registry depends on the number and size of container images that were downloaded prior to the installation of a suite. It may range between 5 and 80 GB.
</p>
<h2 class="mw-headline" id="Suites_storage">Suites storage</h2>
<p>The storage for an installed Suite is split between:
</p>
<ul><li>Node local storage for the Docker image cache</li>
<li>Node local storage for hostPath and emptyDir directly attached container volumes</li>
<li>External storage backed by an NFS server attached to containers via Persistent Volumes and Persistent Volume Claims.</li></ul>
<p>For Docker, Kubernetes, hostPath and emptyDir, see the previous chapters.
</p>
<h3 class="mw-headline" id="Suite_external_storage">Suite external storage</h3>
<p>The external storage for suites is backed by one or more NFS volumes.
</p><p>NFS is the only external volume backing store that is supported.
</p><p>The top-level path for the suite external volume will depend on your NFS configuration.
</p><p>For details of the contents of the external storage for Suites, refer to the Suite installation and administration guides.
</p>
<h2 class="mw-headline" id="How_CDF_and_suites_connect_to_external_storage">How CDF and suites connect to external storage</h2>
<p>Micro Focus uses the Kuberetes persistent volumes and persistent volume claim abstractions.
</p><p>External storage is configured using the Kubernetes persistent volume.
</p><p>Pods use persistent volume claims to request access to persistent volumes
</p><p>Persistent volumes must support ReadWriteMany ie this means external storage must be able to be connected to multiple cluster nodes at the same time (Many) and must support read and write (ReadWrite).
</p><p>CDF supports two modes to define volumes and claims:
</p>
<ol><li>Kubernetes volume and claim objects are created and managed by CDF.
<ol><li>During the installation, CDF will create the necessary Kubernetes objects allowing external storage to be accessed by Pods.</li></ol></li>
<li>Kubernetes volumes objects are managed externally by a cluster administrator. Claim objects are created and managed by CDF.
<ol><li>Before the installation starts, a cluster administrator:
<ol><li>Sets up static persistent volumes connected to external storage with the required Kubernetes storage class and size attributes set.</li>
<li>Or adds a Kubernetes storage class and associated a dynamic provisioner.</li></ol></li>
<li>CDF and suites create persistent volume claims. Kubernetes will do a best-effort matching of claim to volume using storage class and size as the selection criteria.</li></ol></li></ol>
<h2 class="mw-headline" id="Storage_performance">Storage performance</h2>
<p>See Docker storage driver direct-lvm use.
</p><p>Micro Focus recommends that the &lt;CDF ROOT&gt; top level folder such as /opt on each cluster node be located on fast storage such as SSD.
</p>
<!-- 
NewPP limit report
Cached time: 20200217095522
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.020 seconds
Real time usage: 0.059 seconds
Preprocessor visited node count: 30/1000000
Preprocessor generated node count: 80/1000000
Post‐expand include size: 13985/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 1034/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key docops_wiki:pcache:idhash:875853-0!canonical and timestamp 20200217095522 and revision id 1693955
 -->
</div>